{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from typing import Union\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "from dataloaders import NoisyDataset\n",
    "from model import *\n",
    "from preprocessing import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21463/3687639064.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = './data/lr_train.csv'\n",
    "hr_data_path = './data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data],\n",
    "                                        dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 150\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.0005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.4559370149038509 Error:  0.20930187297081518 Topo:  36.53095437809379\n",
      "Epoch:  2 Loss:  0.31040223326511723 Error:  0.1824158550557976 Topo:  20.50272953010605\n",
      "Epoch:  3 Loss:  0.3028820294462992 Error:  0.17900242527088006 Topo:  19.702250640549344\n",
      "Epoch:  4 Loss:  0.29685558762379033 Error:  0.17723631698214365 Topo:  19.075980215015527\n",
      "Epoch:  5 Loss:  0.29153364468477444 Error:  0.17600707700866425 Topo:  18.935270829115087\n",
      "Epoch:  6 Loss:  0.2865848348526184 Error:  0.17504006677758907 Topo:  18.830346689966625\n",
      "Epoch:  7 Loss:  0.2825273882129235 Error:  0.17482053064657543 Topo:  19.06470959486362\n",
      "Epoch:  8 Loss:  0.27701796894658826 Error:  0.17331782643666524 Topo:  18.635903695386325\n",
      "Epoch:  9 Loss:  0.27315063899505637 Error:  0.17260170134598624 Topo:  18.844210367716716\n",
      "Epoch:  10 Loss:  0.2695612660246695 Error:  0.17209978838880619 Topo:  18.84680119246066\n",
      "Epoch:  11 Loss:  0.2653402993065154 Error:  0.1708580909553402 Topo:  18.706497318016556\n",
      "Epoch:  12 Loss:  0.26208044918711315 Error:  0.17014203126915914 Topo:  18.70250700762172\n",
      "Epoch:  13 Loss:  0.25885974664887984 Error:  0.1691939485465695 Topo:  18.76862187870962\n",
      "Epoch:  14 Loss:  0.2558487601087479 Error:  0.16827268996638453 Topo:  18.777568702926178\n",
      "Epoch:  15 Loss:  0.2529199664999625 Error:  0.16706262644893394 Topo:  18.638746564259787\n",
      "Epoch:  16 Loss:  0.25074637096799063 Error:  0.16645277492300478 Topo:  18.854234969544553\n",
      "Epoch:  17 Loss:  0.2479903026969133 Error:  0.16522014266002677 Topo:  18.633728843963073\n",
      "Epoch:  18 Loss:  0.24647353892911694 Error:  0.16464271820233967 Topo:  18.76684034655908\n",
      "Epoch:  19 Loss:  0.2444084768523713 Error:  0.1636302861089478 Topo:  18.623050518378527\n",
      "Epoch:  20 Loss:  0.24250335340014476 Error:  0.16276976840938637 Topo:  18.655277434937254\n",
      "Epoch:  21 Loss:  0.24129966865042726 Error:  0.1623168920506974 Topo:  18.771711806337276\n",
      "Epoch:  22 Loss:  0.24003175748679453 Error:  0.16150225413416675 Topo:  18.761016000530677\n",
      "Epoch:  23 Loss:  0.23863573854198 Error:  0.16072636860573364 Topo:  18.676853750994105\n",
      "Epoch:  24 Loss:  0.23722621263144258 Error:  0.1599395498187242 Topo:  18.656955364935413\n",
      "Epoch:  25 Loss:  0.2364683625048506 Error:  0.15959905319941972 Topo:  18.659570020116018\n",
      "Epoch:  26 Loss:  0.23547297662603642 Error:  0.15906017492274324 Topo:  18.699493676602483\n",
      "Epoch:  27 Loss:  0.23397655365709774 Error:  0.15811150427350026 Topo:  18.57807177697827\n",
      "Epoch:  28 Loss:  0.23353700803782412 Error:  0.15776710337150596 Topo:  18.495385883810993\n",
      "Epoch:  29 Loss:  0.2329168360747263 Error:  0.1575096966977605 Topo:  18.70130958100279\n",
      "Epoch:  30 Loss:  0.23161410910640648 Error:  0.15652283669231895 Topo:  18.524074811421468\n",
      "Epoch:  31 Loss:  0.2310516138276654 Error:  0.15632360424110275 Topo:  18.541888728113232\n",
      "Epoch:  32 Loss:  0.23019814776803205 Error:  0.15585298729157018 Topo:  18.548335320935276\n",
      "Epoch:  33 Loss:  0.2295159769807747 Error:  0.1554451266270198 Topo:  18.49360361498987\n",
      "Epoch:  34 Loss:  0.2291485334406356 Error:  0.15522386883190292 Topo:  18.54419140615863\n",
      "Epoch:  35 Loss:  0.22811232614303065 Error:  0.15451679133369536 Topo:  18.450421904375453\n",
      "Epoch:  36 Loss:  0.22779040199196982 Error:  0.15440138772933068 Topo:  18.526680809295105\n",
      "Epoch:  37 Loss:  0.22701923959626408 Error:  0.1540772866703079 Topo:  18.494225753281643\n",
      "Epoch:  38 Loss:  0.2269703377328233 Error:  0.1540030449807287 Topo:  18.499365766605216\n",
      "Epoch:  39 Loss:  0.22619914856856455 Error:  0.15352475884074937 Topo:  18.548398543260767\n",
      "Epoch:  40 Loss:  0.22561489134848475 Error:  0.15326903269676392 Topo:  18.5620426246506\n",
      "Epoch:  41 Loss:  0.2250067695172247 Error:  0.1530068624251617 Topo:  18.4370027987543\n",
      "Epoch:  42 Loss:  0.2242836893318656 Error:  0.15264804375742724 Topo:  18.416009000675405\n",
      "Epoch:  43 Loss:  0.22402876383530165 Error:  0.15250833957138177 Topo:  18.4168522315111\n",
      "Epoch:  44 Loss:  0.22410063097576896 Error:  0.15262739445099574 Topo:  18.590280766972526\n",
      "Epoch:  45 Loss:  0.2232235533570101 Error:  0.15222447352138108 Topo:  18.498031233599086\n",
      "Epoch:  46 Loss:  0.2228416445369492 Error:  0.1520662939923252 Topo:  18.421731474870693\n",
      "Epoch:  47 Loss:  0.22284902944536267 Error:  0.15219654169625152 Topo:  18.516214010958187\n",
      "Epoch:  48 Loss:  0.22204968190478708 Error:  0.15172525905741902 Topo:  18.384734193721933\n",
      "Epoch:  49 Loss:  0.22171007337684404 Error:  0.15165006106128237 Topo:  18.517089455427524\n",
      "Epoch:  50 Loss:  0.22110683288045985 Error:  0.1512797408385905 Topo:  18.346815463311657\n",
      "Epoch:  51 Loss:  0.22084563006897887 Error:  0.1513191810446585 Topo:  18.376891467385665\n",
      "Epoch:  52 Loss:  0.2202367857544722 Error:  0.15093795027204615 Topo:  18.30908806166963\n",
      "Epoch:  53 Loss:  0.22077730298042297 Error:  0.1514175515510365 Topo:  18.51855351111132\n",
      "Epoch:  54 Loss:  0.21952170954492992 Error:  0.15076768398284912 Topo:  18.319461525557283\n",
      "Epoch:  55 Loss:  0.21960220436849995 Error:  0.1508599896273927 Topo:  18.370952600490547\n",
      "Epoch:  56 Loss:  0.21938653308117462 Error:  0.15088676772788615 Topo:  18.459204713741464\n",
      "Epoch:  57 Loss:  0.21914163936754902 Error:  0.15078916449746685 Topo:  18.38915927110318\n",
      "Epoch:  58 Loss:  0.21863404790798346 Error:  0.15062809200165514 Topo:  18.39501222165045\n",
      "Epoch:  59 Loss:  0.21840785952385314 Error:  0.15051219688204234 Topo:  18.33072613813206\n",
      "Epoch:  60 Loss:  0.2179174772101248 Error:  0.15040528903642814 Topo:  18.381507479502055\n",
      "Epoch:  61 Loss:  0.21801658554705317 Error:  0.15051097681600892 Topo:  18.40800927213566\n",
      "Epoch:  62 Loss:  0.2175698987916558 Error:  0.150379447969134 Topo:  18.44772760882349\n",
      "Epoch:  63 Loss:  0.2169315508918134 Error:  0.15010180623231534 Topo:  18.328691214144587\n",
      "Epoch:  64 Loss:  0.2169023362283935 Error:  0.15018511362775358 Topo:  18.321174381735798\n",
      "Epoch:  65 Loss:  0.2166541523026849 Error:  0.15007773443253455 Topo:  18.359110489576878\n",
      "Epoch:  66 Loss:  0.21645862203158303 Error:  0.15008464352991766 Topo:  18.385007304345777\n",
      "Epoch:  67 Loss:  0.21639623988174392 Error:  0.15017021701721375 Topo:  18.445631249936042\n",
      "Epoch:  68 Loss:  0.2161518569121104 Error:  0.1501086875796318 Topo:  18.421466376253232\n",
      "Epoch:  69 Loss:  0.21555460525486997 Error:  0.149720887848717 Topo:  18.36973667715838\n",
      "Epoch:  70 Loss:  0.21535270388968691 Error:  0.14984523906500755 Topo:  18.327029593690426\n",
      "Epoch:  71 Loss:  0.2151437151039432 Error:  0.14979798498446356 Topo:  18.365035245518484\n",
      "Epoch:  72 Loss:  0.2149230763762297 Error:  0.1497409460162688 Topo:  18.36002492619132\n",
      "Epoch:  73 Loss:  0.2144465234108314 Error:  0.14943231080106634 Topo:  18.257346861376735\n",
      "Epoch:  74 Loss:  0.2143546268790068 Error:  0.1496005387095634 Topo:  18.362424382192646\n",
      "Epoch:  75 Loss:  0.21422296530472304 Error:  0.1495465885647043 Topo:  18.405087916437022\n",
      "Epoch:  76 Loss:  0.21419004596279054 Error:  0.14966254306589058 Topo:  18.37783868298559\n",
      "Epoch:  77 Loss:  0.21386343115818002 Error:  0.14948082327128884 Topo:  18.333217529479615\n",
      "Epoch:  78 Loss:  0.21337952397897572 Error:  0.14924242467937354 Topo:  18.293304900209346\n",
      "Epoch:  79 Loss:  0.2132552912135324 Error:  0.14930337984226422 Topo:  18.318223587767093\n",
      "Epoch:  80 Loss:  0.21296882477706064 Error:  0.14918027251601934 Topo:  18.290524117247074\n",
      "Epoch:  81 Loss:  0.21322574363854116 Error:  0.14950380590326057 Topo:  18.410050089487772\n",
      "Epoch:  82 Loss:  0.2128062253583691 Error:  0.1492742374985518 Topo:  18.311798855216203\n",
      "Epoch:  83 Loss:  0.21294203257846261 Error:  0.14950880849968173 Topo:  18.41652483568934\n",
      "Epoch:  84 Loss:  0.2124446250364452 Error:  0.1492481758137663 Topo:  18.37864311012679\n",
      "Epoch:  85 Loss:  0.2122326207732012 Error:  0.14912951166579824 Topo:  18.273501967241664\n",
      "Epoch:  86 Loss:  0.21230228218489777 Error:  0.1493605017929734 Topo:  18.440767590871115\n",
      "Epoch:  87 Loss:  0.2119388485740045 Error:  0.14908792096340728 Topo:  18.31503473498864\n",
      "Epoch:  88 Loss:  0.2117352834540213 Error:  0.14912856731586113 Topo:  18.420427961977655\n",
      "Epoch:  89 Loss:  0.21157855607435375 Error:  0.1490567661598771 Topo:  18.394348076003755\n",
      "Epoch:  90 Loss:  0.2113689678514789 Error:  0.14901569329514475 Topo:  18.36452870740148\n",
      "Epoch:  91 Loss:  0.21112736372533672 Error:  0.14892196128825227 Topo:  18.28380959333774\n",
      "Epoch:  92 Loss:  0.21115400976763515 Error:  0.14906279881021933 Topo:  18.299599676075097\n",
      "Epoch:  93 Loss:  0.21083527339432767 Error:  0.14891968493511576 Topo:  18.27988128319472\n",
      "Epoch:  94 Loss:  0.21055449643534815 Error:  0.14891252787170295 Topo:  18.213544531496698\n",
      "Epoch:  95 Loss:  0.21071682915002285 Error:  0.14893542961803025 Topo:  18.374982508356698\n",
      "Epoch:  96 Loss:  0.21057722124153982 Error:  0.14892368250621293 Topo:  18.37434802369443\n",
      "Epoch:  97 Loss:  0.21020985809628834 Error:  0.14880713204780738 Topo:  18.31186684020265\n",
      "Epoch:  98 Loss:  0.21029764743979107 Error:  0.14889013932315176 Topo:  18.363434500323084\n",
      "Epoch:  99 Loss:  0.20992937336067954 Error:  0.14877978516017606 Topo:  18.260795404811105\n",
      "Epoch:  100 Loss:  0.20959558904527903 Error:  0.14852439340003237 Topo:  18.266102505301287\n",
      "Epoch:  101 Loss:  0.21008550656769803 Error:  0.1490717154152379 Topo:  18.502349802120005\n",
      "Epoch:  102 Loss:  0.20953374262341481 Error:  0.1487432863719449 Topo:  18.349225838027316\n",
      "Epoch:  103 Loss:  0.20953919600226922 Error:  0.1487246995379111 Topo:  18.323806814090933\n",
      "Epoch:  104 Loss:  0.20895794337381146 Error:  0.1484680299808879 Topo:  18.199624358536955\n",
      "Epoch:  105 Loss:  0.20914205337712866 Error:  0.14867306656823187 Topo:  18.380552931460077\n",
      "Epoch:  106 Loss:  0.20916464657126788 Error:  0.14873732818279436 Topo:  18.341760395529743\n",
      "Epoch:  107 Loss:  0.20890320819652008 Error:  0.14864929506700195 Topo:  18.35096330128744\n",
      "Epoch:  108 Loss:  0.20877649862609224 Error:  0.14857063478160049 Topo:  18.33007107237856\n",
      "Epoch:  109 Loss:  0.20843843330523212 Error:  0.14846409339747743 Topo:  18.20508321887719\n",
      "Epoch:  110 Loss:  0.20869736596495805 Error:  0.1486984891091992 Topo:  18.48125384096614\n",
      "Epoch:  111 Loss:  0.20816598092010635 Error:  0.1484068617849293 Topo:  18.179284775328494\n",
      "Epoch:  112 Loss:  0.2081600300030794 Error:  0.14848215057107503 Topo:  18.308736584143723\n",
      "Epoch:  113 Loss:  0.2081099493417911 Error:  0.14858555338696805 Topo:  18.34787949544941\n",
      "Epoch:  114 Loss:  0.20785373752702496 Error:  0.14841483841220776 Topo:  18.229849415624926\n",
      "Epoch:  115 Loss:  0.20789033852651448 Error:  0.1484464907539105 Topo:  18.293326223681788\n",
      "Epoch:  116 Loss:  0.2076108991921305 Error:  0.14839472394146605 Topo:  18.292986835548263\n",
      "Epoch:  117 Loss:  0.20764641802824899 Error:  0.1484829881144855 Topo:  18.298813094635925\n",
      "Epoch:  118 Loss:  0.2073718874218935 Error:  0.14840862288803397 Topo:  18.214315283084343\n",
      "Epoch:  119 Loss:  0.20714157623445204 Error:  0.14825955354524945 Topo:  18.183078149121677\n",
      "Epoch:  120 Loss:  0.20720767269948284 Error:  0.14830981877571095 Topo:  18.290181445504377\n",
      "Epoch:  121 Loss:  0.2070286432783047 Error:  0.14836165402642268 Topo:  18.30762764365373\n",
      "Epoch:  122 Loss:  0.20690383418591438 Error:  0.14828922550478382 Topo:  18.247783089826207\n",
      "Epoch:  123 Loss:  0.2069787507999443 Error:  0.14834844495008093 Topo:  18.350276661490252\n",
      "Epoch:  124 Loss:  0.20676535002128807 Error:  0.14830487627469138 Topo:  18.279136337919862\n",
      "Epoch:  125 Loss:  0.20674307680058623 Error:  0.14831325862400546 Topo:  18.293716927488408\n",
      "Epoch:  126 Loss:  0.20661720526432562 Error:  0.14825480926536513 Topo:  18.280452945275222\n",
      "Epoch:  127 Loss:  0.20639438775485147 Error:  0.14822687623565067 Topo:  18.28333007218595\n",
      "Epoch:  128 Loss:  0.20639426461950747 Error:  0.14827343718912786 Topo:  18.29799495628494\n",
      "Epoch:  129 Loss:  0.2060871800619685 Error:  0.14816627414997466 Topo:  18.237865921979893\n",
      "Epoch:  130 Loss:  0.20617956877825502 Error:  0.14830624981376225 Topo:  18.300735268050325\n",
      "Epoch:  131 Loss:  0.20607459866358135 Error:  0.1482062990258554 Topo:  18.336708805518235\n",
      "Epoch:  132 Loss:  0.2056615340138624 Error:  0.14801431938024337 Topo:  18.132152111944325\n",
      "Epoch:  133 Loss:  0.20587622503677527 Error:  0.14827133469774337 Topo:  18.306470494070453\n",
      "Epoch:  134 Loss:  0.2057520514298342 Error:  0.14819917850151748 Topo:  18.315902218847217\n",
      "Epoch:  135 Loss:  0.20552241195461707 Error:  0.1480458864641047 Topo:  18.24762392900661\n",
      "Epoch:  136 Loss:  0.205173082158951 Error:  0.1479638051576243 Topo:  18.24313586914611\n",
      "Epoch:  137 Loss:  0.20535771370290995 Error:  0.14806206470834996 Topo:  18.243069380343318\n",
      "Epoch:  138 Loss:  0.20566165072475365 Error:  0.14820267322534572 Topo:  18.318663437209445\n",
      "Epoch:  139 Loss:  0.20525366112500606 Error:  0.14799498864812052 Topo:  18.238530050494713\n",
      "Epoch:  140 Loss:  0.2051621061599183 Error:  0.14800999349284316 Topo:  18.1992659426021\n",
      "Epoch:  141 Loss:  0.20511607156542247 Error:  0.1481733252099174 Topo:  18.330270116200705\n",
      "Epoch:  142 Loss:  0.20488592206004136 Error:  0.14794586215190544 Topo:  18.21600553089987\n",
      "Epoch:  143 Loss:  0.20503578696422234 Error:  0.1480119672274875 Topo:  18.30924814761042\n",
      "Epoch:  144 Loss:  0.20445540663367973 Error:  0.14778273320662047 Topo:  18.161256950058622\n",
      "Epoch:  145 Loss:  0.2047990575700463 Error:  0.1481007532534485 Topo:  18.29108920068798\n",
      "Epoch:  146 Loss:  0.2043218344093083 Error:  0.14774955916190577 Topo:  18.170759275287924\n",
      "Epoch:  147 Loss:  0.2045465762208322 Error:  0.14797191928603692 Topo:  18.14583623052357\n",
      "Epoch:  148 Loss:  0.20435717246846524 Error:  0.14790500950313615 Topo:  18.14693984871139\n",
      "Epoch:  149 Loss:  0.20469684015491052 Error:  0.14822674194674293 Topo:  18.37850354246037\n",
      "Epoch:  150 Loss:  0.204278272426057 Error:  0.147907823413432 Topo:  18.249139037674773\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "criterion = nn.L1Loss()\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "train(final_model, train_data_loader, optimizer, criterion, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final-model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(final_model, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
