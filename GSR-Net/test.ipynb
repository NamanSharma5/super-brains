{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from test import *\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "import networkx as nx\n",
    "from typing import Union\n",
    "\n",
    "import random\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19726/3337449821.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:204.)\n",
      "  lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data],\n",
    "                                        dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, lr_data, hr_data, noise_level=0.01):\n",
    "        \"\"\"\n",
    "        lr_data: Low resolution data (torch.tensor)\n",
    "        hr_data: High resolution data (torch.tensor)\n",
    "        noise_level: Standard deviation of Gaussian noise to be added\n",
    "        \"\"\"\n",
    "        self.lr_data = lr_data\n",
    "        self.hr_data = hr_data\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_sample = self.lr_data[idx]\n",
    "        hr_sample = self.hr_data[idx]\n",
    "\n",
    "        # Adding Gaussian noise\n",
    "        noise = torch.randn(lr_sample.size()) * self.noise_level\n",
    "        noisy_lr_sample = lr_sample + noise\n",
    "\n",
    "        # Clipping to ensure values are between 0 and 1\n",
    "        noisy_lr_sample = torch.clamp(noisy_lr_sample, 0, 1)\n",
    "\n",
    "        return noisy_lr_sample, hr_sample\n",
    "\n",
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 200\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.0005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_csv(model, data_path='../data/lr_test.csv', filename='submission.csv'):\n",
    "    lr_test_data = pd.read_csv(data_path, delimiter=',').to_numpy()\n",
    "    lr_test_data[lr_test_data < 0] = 0\n",
    "    np.nan_to_num(lr_test_data, copy=False)\n",
    "    lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for lr in lr_test_data_vectorized:      \n",
    "        lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "        \n",
    "        model_outputs, _, _, _ = model(lr)\n",
    "        model_outputs  = unpad(model_outputs, args.padding)\n",
    "        preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "    r = np.hstack(preds)\n",
    "    meltedDF = r.flatten()\n",
    "    n = meltedDF.shape[0]\n",
    "    df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                    'Predicted': meltedDF})\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_loader, optimizer, args): \n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "    epoch_topo = []\n",
    "\n",
    "    model.train()\n",
    "    for lr, hr in train_data_loader: \n",
    "      lr.to(device)   \n",
    "      hr.to(device)  \n",
    "      lr = lr.reshape(160, 160)\n",
    "      hr = hr.reshape(268, 268)\n",
    "\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "      topo = compute_topological_MAE_loss(hr, model_outputs)\n",
    "      \n",
    "      loss += args.lamdba_topo * topo\n",
    "\n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      epoch_topo.append(topo.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "      filename = f'{epoch+1}epoch-model.sav'\n",
    "      pickle.dump(model, open(filename, 'wb'))\n",
    "      generate_submission_csv(model, filename=f'{epoch+1}epoch-model.csv')\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error),\n",
    "          \"Topo: \", np.mean(epoch_topo))\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.44269737547743104 Error:  0.2096005307878563 Topo:  38.09577772860042\n",
      "Epoch:  2 Loss:  0.2999330361089307 Error:  0.18171630969304525 Topo:  21.267710702861855\n",
      "Epoch:  3 Loss:  0.2923052475124062 Error:  0.17833045160699035 Topo:  19.844695433885036\n",
      "Epoch:  4 Loss:  0.28625621445878535 Error:  0.1764510716506821 Topo:  19.436927795410156\n",
      "Epoch:  5 Loss:  0.2806439662943343 Error:  0.1753702840048396 Topo:  19.24852789804607\n",
      "Epoch:  6 Loss:  0.27518474199100884 Error:  0.1741512225059692 Topo:  19.01814871348307\n",
      "Epoch:  7 Loss:  0.26976441641053756 Error:  0.1732635708626159 Topo:  19.16435644869319\n",
      "Epoch:  8 Loss:  0.26494179041442756 Error:  0.17233109795404766 Topo:  18.988653708360864\n",
      "Epoch:  9 Loss:  0.2607347944717921 Error:  0.17143990277887106 Topo:  19.010916601398034\n",
      "Epoch:  10 Loss:  0.25620305011729283 Error:  0.1703671977191628 Topo:  19.08328916926584\n",
      "Epoch:  11 Loss:  0.25230781992752394 Error:  0.1693758663838495 Topo:  18.878896810337455\n",
      "Epoch:  12 Loss:  0.24821854074915012 Error:  0.16801426073391282 Topo:  19.013354324295136\n",
      "Epoch:  13 Loss:  0.24483638752006484 Error:  0.1667374163687586 Topo:  18.804298475117026\n",
      "Epoch:  14 Loss:  0.24153998208617022 Error:  0.16570847477027756 Topo:  19.01788232997506\n",
      "Epoch:  15 Loss:  0.23921214740076466 Error:  0.16492562165517294 Topo:  18.90309272126523\n",
      "Epoch:  16 Loss:  0.23639371420095068 Error:  0.1636385459921317 Topo:  19.01874491982831\n",
      "Epoch:  17 Loss:  0.23415038756981582 Error:  0.1627182114623978 Topo:  18.85972060129314\n",
      "Epoch:  18 Loss:  0.23206540674506548 Error:  0.16167611356623873 Topo:  18.922101997329804\n",
      "Epoch:  19 Loss:  0.2300261086867955 Error:  0.16059778684270595 Topo:  18.632949612097825\n",
      "Epoch:  20 Loss:  0.2288399646560589 Error:  0.1600101957778017 Topo:  18.771738240818777\n",
      "Epoch:  21 Loss:  0.22734578953174775 Error:  0.15933873526707382 Topo:  18.823249651286417\n",
      "Epoch:  22 Loss:  0.22602534981187947 Error:  0.15853727997063163 Topo:  18.710300582611634\n",
      "Epoch:  23 Loss:  0.2248704336182086 Error:  0.15802542240676765 Topo:  18.74726167553199\n",
      "Epoch:  24 Loss:  0.22407579047237328 Error:  0.15761783995671186 Topo:  18.7959333979441\n",
      "Epoch:  25 Loss:  0.2227180417247875 Error:  0.15664668229525675 Topo:  18.587304440801017\n",
      "Epoch:  26 Loss:  0.2220670250718465 Error:  0.15642076695036744 Topo:  18.73109746693137\n",
      "Epoch:  27 Loss:  0.22121545547496774 Error:  0.1558238991541777 Topo:  18.66344692892657\n",
      "Epoch:  28 Loss:  0.2204864562271598 Error:  0.155491160240002 Topo:  18.693344618745908\n",
      "Epoch:  29 Loss:  0.2196997308624005 Error:  0.15505061508295778 Topo:  18.684817839525415\n",
      "Epoch:  30 Loss:  0.21881409158963644 Error:  0.154519932980309 Topo:  18.644815256495676\n",
      "Epoch:  31 Loss:  0.21802138693318396 Error:  0.1540868123491367 Topo:  18.57083135593437\n",
      "Epoch:  32 Loss:  0.21746446183341706 Error:  0.15377113695986971 Topo:  18.578266435040685\n",
      "Epoch:  33 Loss:  0.21703282400162635 Error:  0.15364471318835984 Topo:  18.53340196323966\n",
      "Epoch:  34 Loss:  0.2162645874979967 Error:  0.15317932708177737 Topo:  18.539739157625302\n",
      "Epoch:  35 Loss:  0.21579247142026525 Error:  0.15301604546651154 Topo:  18.646879041980128\n",
      "Epoch:  36 Loss:  0.21524036519541712 Error:  0.15275955690952117 Topo:  18.55395496962313\n",
      "Epoch:  37 Loss:  0.21497454516545028 Error:  0.15269080170257363 Topo:  18.732398923999536\n",
      "Epoch:  38 Loss:  0.21412340252699252 Error:  0.152223716328244 Topo:  18.575672486585056\n",
      "Epoch:  39 Loss:  0.2138570520335329 Error:  0.1520596770290843 Topo:  18.546798186387843\n",
      "Epoch:  40 Loss:  0.21326036019596512 Error:  0.15172031976862582 Topo:  18.464193315563087\n",
      "Epoch:  41 Loss:  0.21294057672609112 Error:  0.15177186953271934 Topo:  18.55883439001209\n",
      "Epoch:  42 Loss:  0.21236699980176138 Error:  0.1514871109299317 Topo:  18.506153586381924\n",
      "Epoch:  43 Loss:  0.21228818675715053 Error:  0.15171894431114197 Topo:  18.728695612467693\n",
      "Epoch:  44 Loss:  0.2114426428507902 Error:  0.15115377712927894 Topo:  18.498119417064917\n",
      "Epoch:  45 Loss:  0.21107512043264812 Error:  0.15095087604786822 Topo:  18.444619127376352\n",
      "Epoch:  46 Loss:  0.21091568086318627 Error:  0.15109247045067256 Topo:  18.539057160565953\n",
      "Epoch:  47 Loss:  0.21043121832573486 Error:  0.15080584688279444 Topo:  18.50517472250019\n",
      "Epoch:  48 Loss:  0.21010405119664655 Error:  0.15073364977529663 Topo:  18.518193262065957\n",
      "Epoch:  49 Loss:  0.20980847203089092 Error:  0.15064916906956427 Topo:  18.454763915010556\n",
      "Epoch:  50 Loss:  0.20949166236880296 Error:  0.15054688210080483 Topo:  18.462737289017547\n",
      "Epoch:  51 Loss:  0.2094094666713726 Error:  0.1506566865715438 Topo:  18.56976975652272\n",
      "Epoch:  52 Loss:  0.20882006069857204 Error:  0.1504025247371839 Topo:  18.51016551720168\n",
      "Epoch:  53 Loss:  0.2084846431445219 Error:  0.15033126323522922 Topo:  18.42364170165833\n",
      "Epoch:  54 Loss:  0.20822531386406837 Error:  0.1502919960432424 Topo:  18.521173465751602\n",
      "Epoch:  55 Loss:  0.20791697734130357 Error:  0.15020434714541464 Topo:  18.513826290290513\n",
      "Epoch:  56 Loss:  0.20788273848816305 Error:  0.15028304472833337 Topo:  18.611135751187444\n",
      "Epoch:  57 Loss:  0.20756027182776057 Error:  0.15018196466440212 Topo:  18.523065972470953\n",
      "Epoch:  58 Loss:  0.2069991418344532 Error:  0.1499676552718271 Topo:  18.496550091726338\n",
      "Epoch:  59 Loss:  0.2067738185207287 Error:  0.1499093309223295 Topo:  18.501575001699482\n",
      "Epoch:  60 Loss:  0.2066592892665349 Error:  0.1499585484316249 Topo:  18.54755076105723\n",
      "Epoch:  61 Loss:  0.20619937518756545 Error:  0.1498826289783695 Topo:  18.534905176676677\n",
      "Epoch:  62 Loss:  0.20603637477594935 Error:  0.14977099536778685 Topo:  18.511911534977532\n",
      "Epoch:  63 Loss:  0.2056505788050726 Error:  0.14963139540064122 Topo:  18.45861374403902\n",
      "Epoch:  64 Loss:  0.20528263174845074 Error:  0.1494391552077796 Topo:  18.390536051310466\n",
      "Epoch:  65 Loss:  0.2053039287200231 Error:  0.14967263887028495 Topo:  18.517114627860977\n",
      "Epoch:  66 Loss:  0.20484089405236844 Error:  0.14942090552367135 Topo:  18.422772436084863\n",
      "Epoch:  67 Loss:  0.20465813104264036 Error:  0.1494087748631032 Topo:  18.43287171004061\n",
      "Epoch:  68 Loss:  0.2042834881001604 Error:  0.14920888154092662 Topo:  18.407133473607594\n",
      "Epoch:  69 Loss:  0.2043835293211623 Error:  0.14940507585060098 Topo:  18.463477214653334\n",
      "Epoch:  70 Loss:  0.20415674908432418 Error:  0.14928632617710594 Topo:  18.395627307320783\n",
      "Epoch:  71 Loss:  0.2039049058974146 Error:  0.1492157466932685 Topo:  18.430017237177864\n",
      "Epoch:  72 Loss:  0.20379474384342125 Error:  0.1492278510314262 Topo:  18.490720189260152\n",
      "Epoch:  73 Loss:  0.20358744486720262 Error:  0.1493235605651747 Topo:  18.503891173950926\n",
      "Epoch:  74 Loss:  0.20303880177929015 Error:  0.1488957069591134 Topo:  18.33993521136438\n",
      "Epoch:  75 Loss:  0.20306431256725402 Error:  0.14913043245940866 Topo:  18.429831870301754\n",
      "Epoch:  76 Loss:  0.20285620232542118 Error:  0.1491311151021255 Topo:  18.479432128860566\n",
      "Epoch:  77 Loss:  0.202903938953748 Error:  0.14904939940946546 Topo:  18.401912375124628\n",
      "Epoch:  78 Loss:  0.20255243796074462 Error:  0.14907817877159862 Topo:  18.50519186008476\n",
      "Epoch:  79 Loss:  0.2025869950919808 Error:  0.14911826298443856 Topo:  18.42636763978147\n",
      "Epoch:  80 Loss:  0.20210016291298552 Error:  0.14897657859468175 Topo:  18.484850586531405\n",
      "Epoch:  81 Loss:  0.20204476949697484 Error:  0.14892082148326372 Topo:  18.387001945586974\n",
      "Epoch:  82 Loss:  0.20184095034342325 Error:  0.1488690737211062 Topo:  18.411832900818236\n",
      "Epoch:  83 Loss:  0.20158838511940963 Error:  0.14883294732806213 Topo:  18.39864256853115\n",
      "Epoch:  84 Loss:  0.2018103653085446 Error:  0.14905994372096604 Topo:  18.524247043860885\n",
      "Epoch:  85 Loss:  0.20135107663220275 Error:  0.14882658234613386 Topo:  18.46662062228083\n",
      "Epoch:  86 Loss:  0.20104113390702688 Error:  0.14857092129434654 Topo:  18.362669613546952\n",
      "Epoch:  87 Loss:  0.20119902696795092 Error:  0.14888662470136574 Topo:  18.508191908191066\n",
      "Epoch:  88 Loss:  0.20101724449032082 Error:  0.14886583976759882 Topo:  18.513541849787362\n",
      "Epoch:  89 Loss:  0.20068861267523852 Error:  0.1487391715545854 Topo:  18.479896979417628\n",
      "Epoch:  90 Loss:  0.2005549273091162 Error:  0.1486355456228028 Topo:  18.38164569375044\n",
      "Epoch:  91 Loss:  0.2004927062167379 Error:  0.14872145019248575 Topo:  18.46373518070061\n",
      "Epoch:  92 Loss:  0.2004945720741135 Error:  0.14875465961630474 Topo:  18.43060781855783\n",
      "Epoch:  93 Loss:  0.20024341112839247 Error:  0.14858977068327145 Topo:  18.40607520920074\n",
      "Epoch:  94 Loss:  0.19990682057634798 Error:  0.148598975362535 Topo:  18.353691980510416\n",
      "Epoch:  95 Loss:  0.1997651202593021 Error:  0.14852505855396123 Topo:  18.329392107660897\n",
      "Epoch:  96 Loss:  0.19949005008814577 Error:  0.14831571111422098 Topo:  18.402343470179392\n",
      "Epoch:  97 Loss:  0.19938043455877705 Error:  0.1483119020383515 Topo:  18.25273316777395\n",
      "Epoch:  98 Loss:  0.19935669263679826 Error:  0.14841315935471813 Topo:  18.441993810459525\n",
      "Epoch:  99 Loss:  0.19919529896296426 Error:  0.1482939464514127 Topo:  18.34278392791748\n",
      "Epoch:  100 Loss:  0.19918800346151797 Error:  0.14841390985571695 Topo:  18.362027048350807\n",
      "Epoch:  101 Loss:  0.19913050184349815 Error:  0.14842428164389318 Topo:  18.37184673583436\n",
      "Epoch:  102 Loss:  0.19879348013929266 Error:  0.14839302498601867 Topo:  18.35700741499484\n",
      "Epoch:  103 Loss:  0.1987817750541036 Error:  0.14850186561039108 Topo:  18.489572365126925\n",
      "Epoch:  104 Loss:  0.1985104685415051 Error:  0.1482344279121496 Topo:  18.337687343894366\n",
      "Epoch:  105 Loss:  0.1983910991760071 Error:  0.14820073924200264 Topo:  18.297584213896425\n",
      "Epoch:  106 Loss:  0.1985422468292499 Error:  0.14851323626712412 Topo:  18.43610787534428\n",
      "Epoch:  107 Loss:  0.1981844695920716 Error:  0.14824552927723902 Topo:  18.35582181507956\n",
      "Epoch:  108 Loss:  0.19819203411747596 Error:  0.14825086393756068 Topo:  18.355193001067565\n",
      "Epoch:  109 Loss:  0.1981595040795332 Error:  0.1482682479266635 Topo:  18.41682536873275\n",
      "Epoch:  110 Loss:  0.19807722513190287 Error:  0.14828115358145652 Topo:  18.423789092880522\n",
      "Epoch:  111 Loss:  0.19784295219861106 Error:  0.1482206777987366 Topo:  18.343482856978913\n",
      "Epoch:  112 Loss:  0.1974187300590698 Error:  0.14799414842785474 Topo:  18.299311003999083\n",
      "Epoch:  113 Loss:  0.19780188165381998 Error:  0.14837138073708483 Topo:  18.446969614771312\n",
      "Epoch:  114 Loss:  0.1974478428235311 Error:  0.14811794147520008 Topo:  18.366904292991777\n",
      "Epoch:  115 Loss:  0.1974420041559699 Error:  0.14821110072428595 Topo:  18.39007218298084\n",
      "Epoch:  116 Loss:  0.19710790325781544 Error:  0.14801828951357368 Topo:  18.330307577898402\n",
      "Epoch:  117 Loss:  0.1971224509848806 Error:  0.14813053179643826 Topo:  18.39033494595282\n",
      "Epoch:  118 Loss:  0.19707275855683995 Error:  0.14810631031583169 Topo:  18.346591018631074\n",
      "Epoch:  119 Loss:  0.19686554068933704 Error:  0.14800181485221772 Topo:  18.386528123638588\n",
      "Epoch:  120 Loss:  0.19695450436926173 Error:  0.1482140607016529 Topo:  18.411354835875734\n",
      "Epoch:  121 Loss:  0.1969528635461887 Error:  0.14811711258695512 Topo:  18.364470955854404\n",
      "Epoch:  122 Loss:  0.1965805392957733 Error:  0.14805718560418682 Topo:  18.393987838379637\n",
      "Epoch:  123 Loss:  0.19653692538152912 Error:  0.14802823684172717 Topo:  18.336888855802798\n",
      "Epoch:  124 Loss:  0.19651417005918698 Error:  0.14804105679253618 Topo:  18.378135943841077\n",
      "Epoch:  125 Loss:  0.1964272133069124 Error:  0.14802984509639397 Topo:  18.327048741414874\n",
      "Epoch:  126 Loss:  0.19626890693952936 Error:  0.1479506545259567 Topo:  18.394437789916992\n",
      "Epoch:  127 Loss:  0.19640904468690565 Error:  0.1481271101507598 Topo:  18.353013084320253\n",
      "Epoch:  128 Loss:  0.19581192633705938 Error:  0.1477134791230727 Topo:  18.29189347935294\n",
      "Epoch:  129 Loss:  0.19606519718013124 Error:  0.14799062595395984 Topo:  18.369544868697663\n",
      "Epoch:  130 Loss:  0.19583037442075993 Error:  0.14780983190515085 Topo:  18.25938713050888\n",
      "Epoch:  131 Loss:  0.1958792683428633 Error:  0.14796924680292964 Topo:  18.31836105963427\n",
      "Epoch:  132 Loss:  0.19571500338480144 Error:  0.1478679556243434 Topo:  18.36216325816994\n",
      "Epoch:  133 Loss:  0.19553785186684774 Error:  0.14778070352570025 Topo:  18.206761285930337\n",
      "Epoch:  134 Loss:  0.19553487859443278 Error:  0.14790717008228074 Topo:  18.389845151387288\n",
      "Epoch:  135 Loss:  0.19536922595458117 Error:  0.147819244129929 Topo:  18.30332383138691\n",
      "Epoch:  136 Loss:  0.1954938618544333 Error:  0.14798975347758767 Topo:  18.370307579725804\n",
      "Epoch:  137 Loss:  0.1953719464426269 Error:  0.14789633680425004 Topo:  18.307743083931015\n",
      "Epoch:  138 Loss:  0.1953045517027735 Error:  0.1478927898371291 Topo:  18.396320668523185\n",
      "Epoch:  139 Loss:  0.19517357790184592 Error:  0.14783892495011142 Topo:  18.308463073776153\n",
      "Epoch:  140 Loss:  0.19488280743895892 Error:  0.14761927621271795 Topo:  18.29985367323824\n",
      "Epoch:  141 Loss:  0.1948855935099596 Error:  0.14772285480877598 Topo:  18.32100549572242\n",
      "Epoch:  142 Loss:  0.1947902017189357 Error:  0.14773526805603576 Topo:  18.34477634772569\n",
      "Epoch:  143 Loss:  0.19485052840081518 Error:  0.1478465327513432 Topo:  18.4121305043112\n",
      "Epoch:  144 Loss:  0.19494516853086963 Error:  0.1479806490911695 Topo:  18.411865559880606\n",
      "Epoch:  145 Loss:  0.19486815656373602 Error:  0.1479617685168803 Topo:  18.415214350123605\n",
      "Epoch:  146 Loss:  0.19470163135828372 Error:  0.14778018680339802 Topo:  18.308627768191034\n",
      "Epoch:  147 Loss:  0.1944790490551623 Error:  0.14769546393148913 Topo:  18.255882977011677\n",
      "Epoch:  148 Loss:  0.19457525996390931 Error:  0.14793799201885383 Topo:  18.352978215246143\n",
      "Epoch:  149 Loss:  0.19430423791180113 Error:  0.14770217975991928 Topo:  18.347953362379247\n",
      "Epoch:  150 Loss:  0.19443653283004989 Error:  0.14779542324071873 Topo:  18.332209963998395\n",
      "Epoch:  151 Loss:  0.19448281494443287 Error:  0.14790560479114157 Topo:  18.4053235253888\n",
      "Epoch:  152 Loss:  0.19435740918099523 Error:  0.14783274922185316 Topo:  18.39735316659162\n",
      "Epoch:  153 Loss:  0.1941863001285199 Error:  0.14782006338149487 Topo:  18.40788469485894\n",
      "Epoch:  154 Loss:  0.19396841677719961 Error:  0.14768943382415942 Topo:  18.277124319247857\n",
      "Epoch:  155 Loss:  0.19385755204868887 Error:  0.14763125393561974 Topo:  18.215505577133086\n",
      "Epoch:  156 Loss:  0.19394843869223566 Error:  0.1477536924049526 Topo:  18.33598083221984\n",
      "Epoch:  157 Loss:  0.19377363415178425 Error:  0.14758284523815451 Topo:  18.312362819374677\n",
      "Epoch:  158 Loss:  0.19396674240420678 Error:  0.14780810207663897 Topo:  18.356501208094066\n",
      "Epoch:  159 Loss:  0.19382911804550423 Error:  0.1477065064503761 Topo:  18.322068928244587\n",
      "Epoch:  160 Loss:  0.19397327741106113 Error:  0.14786352221658844 Topo:  18.380991730147493\n",
      "Epoch:  161 Loss:  0.1936302055677254 Error:  0.1477762380402959 Topo:  18.39595768408861\n",
      "Epoch:  162 Loss:  0.19356978760508006 Error:  0.14764943562760324 Topo:  18.245031459602767\n",
      "Epoch:  163 Loss:  0.19342353957855773 Error:  0.14767591030654792 Topo:  18.3689420551597\n",
      "Epoch:  164 Loss:  0.19349299763847969 Error:  0.14768425803520008 Topo:  18.389895667572937\n",
      "Epoch:  165 Loss:  0.19328381255001365 Error:  0.14746061551892115 Topo:  18.206173531309574\n",
      "Epoch:  166 Loss:  0.19329819788119038 Error:  0.14762146717417027 Topo:  18.39960025170606\n",
      "Epoch:  167 Loss:  0.1934024716565709 Error:  0.14789052196070104 Topo:  18.430336095615775\n",
      "Epoch:  168 Loss:  0.1931602434662288 Error:  0.14767484630117872 Topo:  18.398194164573077\n",
      "Epoch:  169 Loss:  0.19319667850068944 Error:  0.14771141620453246 Topo:  18.36946304686769\n",
      "Epoch:  170 Loss:  0.1933131561486307 Error:  0.1478117673161501 Topo:  18.45194022812529\n",
      "Epoch:  171 Loss:  0.19291785252308416 Error:  0.14753687140827407 Topo:  18.307135410651476\n",
      "Epoch:  172 Loss:  0.1929838555658649 Error:  0.14760742117901762 Topo:  18.24365029363575\n",
      "Epoch:  173 Loss:  0.19289226267866033 Error:  0.14760470207401377 Topo:  18.367189287425514\n",
      "Epoch:  174 Loss:  0.19257263419870846 Error:  0.14729766297840072 Topo:  18.209581243777702\n",
      "Epoch:  175 Loss:  0.19287549271555005 Error:  0.1476727608524397 Topo:  18.348738144971655\n",
      "Epoch:  176 Loss:  0.19277387072226246 Error:  0.14756462905935186 Topo:  18.299169894464\n",
      "Epoch:  177 Loss:  0.19273474839276183 Error:  0.14754507043761408 Topo:  18.296724662095485\n",
      "Epoch:  178 Loss:  0.19259414531870517 Error:  0.14747027285441666 Topo:  18.260067357274586\n",
      "Epoch:  179 Loss:  0.19255101136461703 Error:  0.14753185689984682 Topo:  18.375766074586057\n",
      "Epoch:  180 Loss:  0.1926049768567799 Error:  0.14755480370657173 Topo:  18.270146786809683\n",
      "Epoch:  181 Loss:  0.19245954170198498 Error:  0.147473792249928 Topo:  18.368357087323766\n",
      "Epoch:  182 Loss:  0.19261833302632064 Error:  0.14770415554681937 Topo:  18.335234562080064\n",
      "Epoch:  183 Loss:  0.19226769317766865 Error:  0.1473771405077266 Topo:  18.20164300724418\n",
      "Epoch:  184 Loss:  0.19234091873297435 Error:  0.14756447572015716 Topo:  18.279057234347224\n",
      "Epoch:  185 Loss:  0.19251578546569734 Error:  0.14772837345828554 Topo:  18.45986383974909\n",
      "Epoch:  186 Loss:  0.19244145240612373 Error:  0.14770159343937914 Topo:  18.373628319380526\n",
      "Epoch:  187 Loss:  0.1923920425647747 Error:  0.1475845385275915 Topo:  18.368890151292263\n",
      "Epoch:  188 Loss:  0.19237376669209874 Error:  0.14767410998751304 Topo:  18.387516290127873\n",
      "Epoch:  189 Loss:  0.1921153699388047 Error:  0.14751249221627583 Topo:  18.327422616010654\n",
      "Epoch:  190 Loss:  0.19231544095955924 Error:  0.14762666228110205 Topo:  18.32971450668609\n",
      "Epoch:  191 Loss:  0.19202862679958344 Error:  0.14740024386587258 Topo:  18.271344550355465\n",
      "Epoch:  192 Loss:  0.19221519933132356 Error:  0.14762087215385036 Topo:  18.38500960572751\n",
      "Epoch:  193 Loss:  0.1917778931691975 Error:  0.1473553379228015 Topo:  18.176730418633557\n",
      "Epoch:  194 Loss:  0.19205825423409126 Error:  0.14752171166285782 Topo:  18.409173480051006\n",
      "Epoch:  195 Loss:  0.19209046840310812 Error:  0.14754852612396915 Topo:  18.39073691111125\n",
      "Epoch:  196 Loss:  0.19200312662981228 Error:  0.14751946328285925 Topo:  18.381428718566895\n",
      "Epoch:  197 Loss:  0.1918223253088797 Error:  0.14749975422185338 Topo:  18.33635074364211\n",
      "Epoch:  198 Loss:  0.19185219892484698 Error:  0.14744054039795243 Topo:  18.27031225215889\n",
      "Epoch:  199 Loss:  0.1916025012374638 Error:  0.1473542672699083 Topo:  18.299362171195938\n",
      "Epoch:  200 Loss:  0.19178633463239955 Error:  0.1474062368095278 Topo:  18.300775887723454\n",
      "Epoch:  201 Loss:  0.1918559887273583 Error:  0.14756263984356097 Topo:  18.344981136436235\n",
      "Epoch:  202 Loss:  0.1918733660689371 Error:  0.14765264379407117 Topo:  18.438193503968016\n",
      "Epoch:  203 Loss:  0.19155031389105107 Error:  0.14735226034225818 Topo:  18.31542304033291\n",
      "Epoch:  204 Loss:  0.19147482202081623 Error:  0.14740304929945997 Topo:  18.311920251674994\n",
      "Epoch:  205 Loss:  0.19147641546355038 Error:  0.1473328482783483 Topo:  18.410506305580366\n",
      "Epoch:  206 Loss:  0.1917172784576873 Error:  0.14752811742221525 Topo:  18.32394579356302\n",
      "Epoch:  207 Loss:  0.19161587096973806 Error:  0.14757432788610458 Topo:  18.324414932799197\n",
      "Epoch:  208 Loss:  0.19128199733659892 Error:  0.1473298308378208 Topo:  18.264256854257184\n",
      "Epoch:  209 Loss:  0.19138966482913422 Error:  0.14739083043650952 Topo:  18.336084822694698\n",
      "Epoch:  210 Loss:  0.19130761252191966 Error:  0.14740637114304983 Topo:  18.27431750725843\n",
      "Epoch:  211 Loss:  0.19135259689685113 Error:  0.14744259604436907 Topo:  18.369139180211963\n",
      "Epoch:  212 Loss:  0.19107926632472855 Error:  0.14723211883784768 Topo:  18.24163436318586\n",
      "Epoch:  213 Loss:  0.19109614193439484 Error:  0.14728344811829264 Topo:  18.325958645986226\n",
      "Epoch:  214 Loss:  0.19130288831845016 Error:  0.14735553029946938 Topo:  18.290497037464988\n",
      "Epoch:  215 Loss:  0.19121321705644 Error:  0.14744448291505882 Topo:  18.348949700772405\n",
      "Epoch:  216 Loss:  0.1911351352394698 Error:  0.1473350258555241 Topo:  18.239095487994348\n",
      "Epoch:  217 Loss:  0.1909595445066155 Error:  0.14723237519135732 Topo:  18.289765929033656\n",
      "Epoch:  218 Loss:  0.19107422482467698 Error:  0.1473476031493998 Topo:  18.272041532093894\n",
      "Epoch:  219 Loss:  0.19103513515281106 Error:  0.1473345528998061 Topo:  18.341518373546485\n",
      "Epoch:  220 Loss:  0.1909807878518533 Error:  0.1472982556341651 Topo:  18.348305725051972\n",
      "Epoch:  221 Loss:  0.1910562532212206 Error:  0.14741767130925984 Topo:  18.363695213180815\n",
      "Epoch:  222 Loss:  0.19115633985953417 Error:  0.1474374019189509 Topo:  18.35256785546948\n",
      "Epoch:  223 Loss:  0.19109275598012046 Error:  0.1475671624352118 Topo:  18.450790473800932\n",
      "Epoch:  224 Loss:  0.19088885357636892 Error:  0.14727058252710068 Topo:  18.222707514277474\n",
      "Epoch:  225 Loss:  0.19096314478777127 Error:  0.14751513953694326 Topo:  18.385279895302777\n",
      "Epoch:  226 Loss:  0.19108797315351977 Error:  0.1475856483071864 Topo:  18.401298368762355\n",
      "Epoch:  227 Loss:  0.19063243260997498 Error:  0.14727020424283194 Topo:  18.31214999010463\n",
      "Epoch:  228 Loss:  0.19061630527059475 Error:  0.1471234103608988 Topo:  18.242762348609055\n",
      "Epoch:  229 Loss:  0.19070930047306472 Error:  0.14730269426178788 Topo:  18.314314431059145\n",
      "Epoch:  230 Loss:  0.19098923395493786 Error:  0.1474322861807789 Topo:  18.399977866760985\n",
      "Epoch:  231 Loss:  0.19049697971629526 Error:  0.14715275248724544 Topo:  18.288238673866864\n",
      "Epoch:  232 Loss:  0.1908643029764027 Error:  0.14743607219107852 Topo:  18.307261198580623\n",
      "Epoch:  233 Loss:  0.19047767655578202 Error:  0.1470948634836488 Topo:  18.27816457234457\n",
      "Epoch:  234 Loss:  0.190563585676119 Error:  0.1472325822461151 Topo:  18.33694393477754\n",
      "Epoch:  235 Loss:  0.1903856952568728 Error:  0.147117199030465 Topo:  18.18014852443855\n",
      "Epoch:  236 Loss:  0.19065727819939574 Error:  0.1473377963115355 Topo:  18.320774398164122\n",
      "Epoch:  237 Loss:  0.19056768162164858 Error:  0.1472084619060248 Topo:  18.293799211878977\n",
      "Epoch:  238 Loss:  0.1906344402514532 Error:  0.14729950153185223 Topo:  18.282361693011072\n",
      "Epoch:  239 Loss:  0.19053880790036595 Error:  0.14731781171289032 Topo:  18.301388169477086\n",
      "Epoch:  240 Loss:  0.1904805628303996 Error:  0.14729263428264036 Topo:  18.315023867669932\n",
      "Epoch:  241 Loss:  0.19053182095110774 Error:  0.1473151942837738 Topo:  18.357482007877554\n",
      "Epoch:  242 Loss:  0.19057423929254452 Error:  0.1474088917146186 Topo:  18.34457961670653\n",
      "Epoch:  243 Loss:  0.19035892224240447 Error:  0.14718542832457376 Topo:  18.299426347195745\n",
      "Epoch:  244 Loss:  0.19036139173065117 Error:  0.14726335198400978 Topo:  18.274418859424706\n",
      "Epoch:  245 Loss:  0.1905783914520355 Error:  0.147428904509473 Topo:  18.35331820870588\n",
      "Epoch:  246 Loss:  0.19013642069108472 Error:  0.1471021474746173 Topo:  18.317625051486992\n",
      "Epoch:  247 Loss:  0.19040668688848347 Error:  0.14726228664021293 Topo:  18.30214278283947\n",
      "Epoch:  248 Loss:  0.19008709927518924 Error:  0.14716315037476088 Topo:  18.33352044956413\n",
      "Epoch:  249 Loss:  0.1902992869922501 Error:  0.14727234073027878 Topo:  18.338144445133782\n",
      "Epoch:  250 Loss:  0.19015054056744377 Error:  0.14722725316257534 Topo:  18.273459737172384\n",
      "Epoch:  251 Loss:  0.190368564364439 Error:  0.14725878575961746 Topo:  18.26583647585201\n",
      "Epoch:  252 Loss:  0.19007544585330757 Error:  0.14725569796240973 Topo:  18.308935953471476\n",
      "Epoch:  253 Loss:  0.19022057432971315 Error:  0.14728417260918075 Topo:  18.33082895792887\n",
      "Epoch:  254 Loss:  0.190291557929473 Error:  0.14742402629759496 Topo:  18.35559430950416\n",
      "Epoch:  255 Loss:  0.1898868042908743 Error:  0.14706668088179148 Topo:  18.263372101469667\n",
      "Epoch:  256 Loss:  0.19020355452677448 Error:  0.1472453764991132 Topo:  18.35642078536713\n",
      "Epoch:  257 Loss:  0.18982883761385957 Error:  0.14696146257801684 Topo:  18.11341868760343\n",
      "Epoch:  258 Loss:  0.1900659320418706 Error:  0.14720514033012047 Topo:  18.306003085153545\n",
      "Epoch:  259 Loss:  0.19013362070043643 Error:  0.14728066414416194 Topo:  18.3113879300877\n",
      "Epoch:  260 Loss:  0.19009036387868983 Error:  0.1472736587246021 Topo:  18.34598830217373\n",
      "Epoch:  261 Loss:  0.1902620563071645 Error:  0.14734754796156627 Topo:  18.379329767055854\n",
      "Epoch:  262 Loss:  0.1899853907302468 Error:  0.14715154585010276 Topo:  18.2568241222176\n",
      "Epoch:  263 Loss:  0.19001518430824052 Error:  0.1471250010375491 Topo:  18.268606391495574\n",
      "Epoch:  264 Loss:  0.18994033961238976 Error:  0.14724564766455553 Topo:  18.324231176319238\n",
      "Epoch:  265 Loss:  0.1898217147695804 Error:  0.14709678194123113 Topo:  18.203377980671956\n",
      "Epoch:  266 Loss:  0.1898566036524173 Error:  0.14716341551727877 Topo:  18.324364068265446\n",
      "Epoch:  267 Loss:  0.18979085375091986 Error:  0.14705803258690292 Topo:  18.261853823404827\n",
      "Epoch:  268 Loss:  0.18986497364358274 Error:  0.14714002292491718 Topo:  18.273288624015397\n",
      "Epoch:  269 Loss:  0.19011908243159334 Error:  0.1473504635031352 Topo:  18.357842719483518\n",
      "Epoch:  270 Loss:  0.18983148065155853 Error:  0.1470909433182842 Topo:  18.243880460362234\n",
      "Epoch:  271 Loss:  0.1898382834331718 Error:  0.14716677461377162 Topo:  18.326004673620897\n",
      "Epoch:  272 Loss:  0.1899006600508433 Error:  0.1472814737590487 Topo:  18.36025892474694\n",
      "Epoch:  273 Loss:  0.1900181875614349 Error:  0.1474097326933267 Topo:  18.400936075313364\n",
      "Epoch:  274 Loss:  0.18968513126144865 Error:  0.14701545845248742 Topo:  18.23942860015138\n",
      "Epoch:  275 Loss:  0.1896659378520029 Error:  0.14697564107750705 Topo:  18.255820982470485\n",
      "Epoch:  276 Loss:  0.18994496396915642 Error:  0.1472919714040385 Topo:  18.353347881111556\n",
      "Epoch:  277 Loss:  0.18956837147295832 Error:  0.14696474016426567 Topo:  18.300353295788792\n",
      "Epoch:  278 Loss:  0.1896808507913601 Error:  0.14706821061536937 Topo:  18.267476538698116\n",
      "Epoch:  279 Loss:  0.18970983818976464 Error:  0.1471278801738859 Topo:  18.32259559060285\n",
      "Epoch:  280 Loss:  0.18981258517610813 Error:  0.14716502939334172 Topo:  18.355633764209863\n",
      "Epoch:  281 Loss:  0.18955784821938612 Error:  0.14694423491726377 Topo:  18.2594022636642\n",
      "Epoch:  282 Loss:  0.1895527430994068 Error:  0.14709047464553468 Topo:  18.229221869371607\n",
      "Epoch:  283 Loss:  0.1898064938134062 Error:  0.14724178558695103 Topo:  18.387360424338702\n",
      "Epoch:  284 Loss:  0.18995907920563293 Error:  0.14735151889795314 Topo:  18.32563877105713\n",
      "Epoch:  285 Loss:  0.18954361725353194 Error:  0.14705970254308448 Topo:  18.25629720859185\n",
      "Epoch:  286 Loss:  0.18989818770728426 Error:  0.14730447454902226 Topo:  18.364032277090107\n",
      "Epoch:  287 Loss:  0.18946841020070151 Error:  0.14702564344077768 Topo:  18.21470487902978\n",
      "Epoch:  288 Loss:  0.1893763851798223 Error:  0.14700641523221294 Topo:  18.215357671954674\n",
      "Epoch:  289 Loss:  0.18965838171407848 Error:  0.14722374518831333 Topo:  18.34510934567023\n",
      "Epoch:  290 Loss:  0.18964005096586878 Error:  0.1471611813246133 Topo:  18.27229523801518\n",
      "Epoch:  291 Loss:  0.18963309944032908 Error:  0.14722301918054057 Topo:  18.339841260167653\n",
      "Epoch:  292 Loss:  0.18950412328728658 Error:  0.1470867303941778 Topo:  18.284220912499343\n",
      "Epoch:  293 Loss:  0.18935200045565645 Error:  0.1470694429949372 Topo:  18.29855024029395\n",
      "Epoch:  294 Loss:  0.18941463958360477 Error:  0.14703391804666577 Topo:  18.276214662426245\n",
      "Epoch:  295 Loss:  0.18947787413340128 Error:  0.14706300502408765 Topo:  18.32227455641695\n",
      "Epoch:  296 Loss:  0.18950401389313315 Error:  0.14721099291732925 Topo:  18.33045817849165\n",
      "Epoch:  297 Loss:  0.18945495177528815 Error:  0.14725997678177086 Topo:  18.366650312960505\n",
      "Epoch:  298 Loss:  0.1892434404103342 Error:  0.14695844446827552 Topo:  18.205219777044423\n",
      "Epoch:  299 Loss:  0.18946604136221423 Error:  0.147131206434287 Topo:  18.2767476236035\n",
      "Epoch:  300 Loss:  0.1895706470676525 Error:  0.1472304145286897 Topo:  18.395824243922434\n",
      "Epoch:  301 Loss:  0.18943072578864184 Error:  0.1470331235113972 Topo:  18.267894019623718\n",
      "Epoch:  302 Loss:  0.18920810940022953 Error:  0.147003723743433 Topo:  18.237692193356818\n",
      "Epoch:  303 Loss:  0.18930467481384733 Error:  0.14698078496727401 Topo:  18.140705919551277\n",
      "Epoch:  304 Loss:  0.18910116564014 Error:  0.14693743557094813 Topo:  18.261022984624624\n",
      "Epoch:  305 Loss:  0.18915865646151012 Error:  0.1468771417251604 Topo:  18.212572040672075\n",
      "Epoch:  306 Loss:  0.18951004645424688 Error:  0.14712856112126105 Topo:  18.328592231887544\n",
      "Epoch:  307 Loss:  0.18948042883487518 Error:  0.14716616558457563 Topo:  18.344591266380814\n",
      "Epoch:  308 Loss:  0.18892082518446232 Error:  0.14677471837062323 Topo:  18.162222930771147\n",
      "Epoch:  309 Loss:  0.18935630992501082 Error:  0.14711410297962005 Topo:  18.299932976682744\n",
      "Epoch:  310 Loss:  0.18917053973603393 Error:  0.1469681463287976 Topo:  18.256162609168868\n",
      "Epoch:  311 Loss:  0.18933025091708064 Error:  0.14694968026555227 Topo:  18.294518510738534\n",
      "Epoch:  312 Loss:  0.18943139027335687 Error:  0.14705315531192426 Topo:  18.30323119363385\n",
      "Epoch:  313 Loss:  0.18931416756735592 Error:  0.14706027918233128 Topo:  18.34886324191522\n",
      "Epoch:  314 Loss:  0.18922378622486205 Error:  0.14696996137053667 Topo:  18.254689159507524\n",
      "Epoch:  315 Loss:  0.18903602766776514 Error:  0.1468979496352687 Topo:  18.202208456165064\n",
      "Epoch:  316 Loss:  0.18933299594296665 Error:  0.1471043611893397 Topo:  18.341994125686007\n",
      "Epoch:  317 Loss:  0.1894022769735245 Error:  0.14711851747093085 Topo:  18.358851335719674\n",
      "Epoch:  318 Loss:  0.18929993686918728 Error:  0.14698235596903783 Topo:  18.27192168321438\n",
      "Epoch:  319 Loss:  0.18918188871023897 Error:  0.14695694592006192 Topo:  18.208253883316132\n",
      "Epoch:  320 Loss:  0.18939083930618034 Error:  0.1470299983274437 Topo:  18.236037254333496\n",
      "Epoch:  321 Loss:  0.1889678470924229 Error:  0.1467032425864014 Topo:  18.15054308725688\n",
      "Epoch:  322 Loss:  0.1890152342126755 Error:  0.14686121965596777 Topo:  18.250015247367813\n",
      "Epoch:  323 Loss:  0.18906053641956008 Error:  0.1469770570447345 Topo:  18.24470203079863\n",
      "Epoch:  324 Loss:  0.18897590772834366 Error:  0.1467930278824475 Topo:  18.200588991542062\n",
      "Epoch:  325 Loss:  0.1889479952658008 Error:  0.14687284055405747 Topo:  18.24259234331325\n",
      "Epoch:  326 Loss:  0.18888609343303178 Error:  0.14680397189305927 Topo:  18.253078632012098\n",
      "Epoch:  327 Loss:  0.18909228981255058 Error:  0.1470013498635349 Topo:  18.249291328612916\n",
      "Epoch:  328 Loss:  0.18922928555639917 Error:  0.14687453794800592 Topo:  18.29220755799802\n",
      "Epoch:  329 Loss:  0.18898946671428796 Error:  0.14680804410380519 Topo:  18.216767316806816\n",
      "Epoch:  330 Loss:  0.18911901366210984 Error:  0.14700551598728773 Topo:  18.286394182079565\n",
      "Epoch:  331 Loss:  0.18932812094331503 Error:  0.14709326816711596 Topo:  18.326018464779427\n",
      "Epoch:  332 Loss:  0.18894800481325139 Error:  0.14685355568538883 Topo:  18.2449877733242\n",
      "Epoch:  333 Loss:  0.18909534562133742 Error:  0.146991249240801 Topo:  18.27478874800448\n",
      "Epoch:  334 Loss:  0.18911076814471606 Error:  0.14696051983419292 Topo:  18.242254422810262\n",
      "Epoch:  335 Loss:  0.18898931092130924 Error:  0.1469150398841161 Topo:  18.226682109033277\n",
      "Epoch:  336 Loss:  0.1887746459531213 Error:  0.1465387275208256 Topo:  18.069540680525545\n",
      "Epoch:  337 Loss:  0.18883575722129045 Error:  0.14689288038515044 Topo:  18.288298229971332\n",
      "Epoch:  338 Loss:  0.18926481099542744 Error:  0.14703121361975185 Topo:  18.279136286524242\n",
      "Epoch:  339 Loss:  0.1890442597116539 Error:  0.14696095874923432 Topo:  18.22334532252329\n",
      "Epoch:  340 Loss:  0.18885813046715216 Error:  0.14687442543085463 Topo:  18.349265326996765\n",
      "Epoch:  341 Loss:  0.18898608771983735 Error:  0.14682495848325913 Topo:  18.17801390436595\n",
      "Epoch:  342 Loss:  0.18884443282010313 Error:  0.1468073112254371 Topo:  18.208348913820917\n",
      "Epoch:  343 Loss:  0.18907769595434565 Error:  0.1469696044564961 Topo:  18.298813391588403\n",
      "Epoch:  344 Loss:  0.18890873882584944 Error:  0.1468966469882491 Topo:  18.303622171550455\n",
      "Epoch:  345 Loss:  0.18905325724693114 Error:  0.14689055156564998 Topo:  18.200857750669925\n",
      "Epoch:  346 Loss:  0.18876421549719966 Error:  0.14683725274430065 Topo:  18.19963255899395\n",
      "Epoch:  347 Loss:  0.18903865607198841 Error:  0.14700437769918384 Topo:  18.296098686263946\n",
      "Epoch:  348 Loss:  0.1889026842252937 Error:  0.1467541916195504 Topo:  18.19573246504732\n",
      "Epoch:  349 Loss:  0.18898460956033833 Error:  0.14695229408091415 Topo:  18.30786064285004\n",
      "Epoch:  350 Loss:  0.18901980654922074 Error:  0.14693854686742772 Topo:  18.31227489859758\n",
      "Epoch:  351 Loss:  0.18913103631156647 Error:  0.14699170198626146 Topo:  18.324610801514037\n",
      "Epoch:  352 Loss:  0.18908184186784094 Error:  0.1469225309298424 Topo:  18.217103478437412\n",
      "Epoch:  353 Loss:  0.18898547445228714 Error:  0.14684230013343388 Topo:  18.264286886432213\n",
      "Epoch:  354 Loss:  0.1890112824604183 Error:  0.14678963987591737 Topo:  18.173230999244186\n",
      "Epoch:  355 Loss:  0.18915888711720882 Error:  0.1470380201935768 Topo:  18.363509760645336\n",
      "Epoch:  356 Loss:  0.189139380783378 Error:  0.14694885512490471 Topo:  18.296093449621143\n",
      "Epoch:  357 Loss:  0.18887256594475157 Error:  0.14681428479041883 Topo:  18.2234529849298\n",
      "Epoch:  358 Loss:  0.188689006838256 Error:  0.14666599485867038 Topo:  18.146526816362393\n",
      "Epoch:  359 Loss:  0.18905014103044293 Error:  0.14677295954284553 Topo:  18.1833162479058\n",
      "Epoch:  360 Loss:  0.18900930444280545 Error:  0.1467946824020968 Topo:  18.219869408064973\n",
      "Epoch:  361 Loss:  0.18878815416804332 Error:  0.14677684107226527 Topo:  18.252553186016883\n",
      "Epoch:  362 Loss:  0.18906099514333075 Error:  0.14689545882140806 Topo:  18.239062509137\n",
      "Epoch:  363 Loss:  0.18886707642835057 Error:  0.14687683116533085 Topo:  18.22146481953695\n",
      "Epoch:  364 Loss:  0.18893469700556315 Error:  0.14686010777950287 Topo:  18.304058788779255\n",
      "Epoch:  365 Loss:  0.18866646111368418 Error:  0.14672519061379805 Topo:  18.17104042076065\n",
      "Epoch:  366 Loss:  0.1887977879918264 Error:  0.14682500314212846 Topo:  18.20191095546334\n",
      "Epoch:  367 Loss:  0.18877020745934125 Error:  0.146736567382684 Topo:  18.2347400059957\n",
      "Epoch:  368 Loss:  0.1886651222398895 Error:  0.14672666185808753 Topo:  18.251185342937173\n",
      "Epoch:  369 Loss:  0.1888772884528794 Error:  0.14684970270909234 Topo:  18.231718948501314\n",
      "Epoch:  370 Loss:  0.18877004711570855 Error:  0.14676482025199308 Topo:  18.289333092238376\n",
      "Epoch:  371 Loss:  0.1885491116139703 Error:  0.1466422076532227 Topo:  18.264189223329463\n",
      "Epoch:  372 Loss:  0.18884366491954482 Error:  0.14657845575652437 Topo:  18.11183681031187\n",
      "Epoch:  373 Loss:  0.18893970950634895 Error:  0.1468300528065887 Topo:  18.165958393119766\n",
      "Epoch:  374 Loss:  0.18891115807844494 Error:  0.14684593369682392 Topo:  18.256791428891486\n",
      "Epoch:  375 Loss:  0.1888143398804579 Error:  0.14668976073850415 Topo:  18.13246789806617\n",
      "Epoch:  376 Loss:  0.18895343891874758 Error:  0.14678889325635877 Topo:  18.158457070767522\n",
      "Epoch:  377 Loss:  0.18916954457046029 Error:  0.14681542446156462 Topo:  18.23554696294362\n",
      "Epoch:  378 Loss:  0.18863935393844536 Error:  0.1466567259348795 Topo:  18.177357348139413\n",
      "Epoch:  379 Loss:  0.1891113244844768 Error:  0.14677202219734647 Topo:  18.195992269915735\n",
      "Epoch:  380 Loss:  0.1888902715580192 Error:  0.14659877741586663 Topo:  18.09620653940532\n",
      "Epoch:  381 Loss:  0.18897595394871192 Error:  0.1467289018059919 Topo:  18.132207447897173\n",
      "Epoch:  382 Loss:  0.1887495007343635 Error:  0.14681306508129943 Topo:  18.230165167483026\n",
      "Epoch:  383 Loss:  0.1882728467622917 Error:  0.14628859859205293 Topo:  18.023449755000495\n",
      "Epoch:  384 Loss:  0.18882100953313405 Error:  0.1467764908949772 Topo:  18.26109659457635\n",
      "Epoch:  385 Loss:  0.18866820579874302 Error:  0.14660896334105622 Topo:  18.182740736864282\n",
      "Epoch:  386 Loss:  0.1888772101102475 Error:  0.14660283552850792 Topo:  18.090149656741204\n",
      "Epoch:  387 Loss:  0.18855186145819589 Error:  0.14665357826534145 Topo:  18.180970163402442\n",
      "Epoch:  388 Loss:  0.18865992708834345 Error:  0.14662651891658407 Topo:  18.15473676441672\n",
      "Epoch:  389 Loss:  0.18850288044906663 Error:  0.14653517122932536 Topo:  18.123181617188596\n",
      "Epoch:  390 Loss:  0.18845285707248186 Error:  0.1463844458053926 Topo:  18.15120389789878\n",
      "Epoch:  391 Loss:  0.18877927557437005 Error:  0.14649803816022988 Topo:  18.12773665696561\n",
      "Epoch:  392 Loss:  0.18828990877031565 Error:  0.14632077006522767 Topo:  18.11130912575179\n",
      "Epoch:  393 Loss:  0.1886658435631655 Error:  0.1464908333149499 Topo:  18.10773982544859\n",
      "Epoch:  394 Loss:  0.18837495041107702 Error:  0.14622406175215089 Topo:  18.05496012522075\n",
      "Epoch:  395 Loss:  0.18896826923250437 Error:  0.14668564226277575 Topo:  18.108295389278208\n",
      "Epoch:  396 Loss:  0.18862426066826918 Error:  0.14649691776244225 Topo:  18.143644681233845\n",
      "Epoch:  397 Loss:  0.1883459350841488 Error:  0.14640014574020924 Topo:  18.107074440596346\n",
      "Epoch:  398 Loss:  0.18859984319724007 Error:  0.14638320249533224 Topo:  18.101516283914716\n",
      "Epoch:  399 Loss:  0.18881772628087484 Error:  0.14645652133904533 Topo:  18.12044953443333\n",
      "Epoch:  400 Loss:  0.18876838443165053 Error:  0.14637146287870978 Topo:  17.98454082511856\n",
      "Epoch:  401 Loss:  0.18871528632983475 Error:  0.1465868052697467 Topo:  18.151756526467327\n",
      "Epoch:  402 Loss:  0.1882597125932842 Error:  0.1462625209353641 Topo:  18.024881562786902\n",
      "Epoch:  403 Loss:  0.1886424526661456 Error:  0.14634573981612028 Topo:  18.08463952641287\n",
      "Epoch:  404 Loss:  0.18881926154662035 Error:  0.14656727096277797 Topo:  18.127521954610675\n",
      "Epoch:  405 Loss:  0.18836479147751176 Error:  0.14627918766108816 Topo:  18.077513729026933\n",
      "Epoch:  406 Loss:  0.18884625790004958 Error:  0.14657979605797522 Topo:  18.158361977445864\n",
      "Epoch:  407 Loss:  0.18836984752181046 Error:  0.14620160886984385 Topo:  18.019483709049798\n",
      "Epoch:  408 Loss:  0.18874447315395948 Error:  0.14631123742657506 Topo:  18.08828099759039\n",
      "Epoch:  409 Loss:  0.18817246736523635 Error:  0.14609355064566265 Topo:  17.91780404416387\n",
      "Epoch:  410 Loss:  0.18864900614330155 Error:  0.1465279612980203 Topo:  18.153307897602012\n",
      "Epoch:  411 Loss:  0.1884679650653622 Error:  0.14633501472765814 Topo:  18.11190195711787\n",
      "Epoch:  412 Loss:  0.18826071748476542 Error:  0.14617240656457262 Topo:  17.934257678642958\n",
      "Epoch:  413 Loss:  0.1888174173717727 Error:  0.14654978699312954 Topo:  18.19625306272221\n",
      "Epoch:  414 Loss:  0.18846487311902874 Error:  0.1463576077879546 Topo:  18.076117909597066\n",
      "Epoch:  415 Loss:  0.18874231391324253 Error:  0.14638462305782798 Topo:  18.14262089757862\n",
      "Epoch:  416 Loss:  0.18890452527714346 Error:  0.14622820547954765 Topo:  18.005185207207045\n",
      "Epoch:  417 Loss:  0.1883620217353284 Error:  0.14621617653948105 Topo:  18.077830931383694\n",
      "Epoch:  418 Loss:  0.18866534415119424 Error:  0.146385080800085 Topo:  18.130761517736012\n",
      "Epoch:  419 Loss:  0.18831427252578165 Error:  0.14610063806622328 Topo:  17.923073797168847\n",
      "Epoch:  420 Loss:  0.1884520295672788 Error:  0.14610119239834254 Topo:  17.965291320206877\n",
      "Epoch:  421 Loss:  0.18848086902481354 Error:  0.1462181349714359 Topo:  18.06313812233017\n",
      "Epoch:  422 Loss:  0.18865219624099616 Error:  0.1461399812362865 Topo:  17.982825730375186\n",
      "Epoch:  423 Loss:  0.1884257857671041 Error:  0.14624721989660205 Topo:  18.05942112243104\n",
      "Epoch:  424 Loss:  0.1883675286512889 Error:  0.14609542203520587 Topo:  17.959921237237438\n",
      "Epoch:  425 Loss:  0.18860123373434215 Error:  0.14616578121385174 Topo:  18.059272063706448\n",
      "Epoch:  426 Loss:  0.1883915426309951 Error:  0.14608751973527634 Topo:  18.033468657624937\n",
      "Epoch:  427 Loss:  0.18889388959564848 Error:  0.1463014191674615 Topo:  18.00345451103713\n",
      "Epoch:  428 Loss:  0.18849131643415212 Error:  0.14609927608224446 Topo:  18.00146962354283\n",
      "Epoch:  429 Loss:  0.18860837344280973 Error:  0.14611151417393883 Topo:  17.992612124917034\n",
      "Epoch:  430 Loss:  0.1883945424042776 Error:  0.1460040921936492 Topo:  17.926574786979995\n",
      "Epoch:  431 Loss:  0.18821973813151172 Error:  0.14588484997877818 Topo:  17.920627314173533\n",
      "Epoch:  432 Loss:  0.18859838440032775 Error:  0.14613012920418186 Topo:  18.00314833018594\n",
      "Epoch:  433 Loss:  0.18848954703279597 Error:  0.14582916701625206 Topo:  17.9638932279484\n",
      "Epoch:  434 Loss:  0.18857659970571894 Error:  0.14602303112338402 Topo:  17.8895202967935\n",
      "Epoch:  435 Loss:  0.18923497503389142 Error:  0.14639455035418095 Topo:  18.093419086433457\n",
      "Epoch:  436 Loss:  0.18826794276337425 Error:  0.14584984652653427 Topo:  17.842902794569554\n",
      "Epoch:  437 Loss:  0.18853135353433872 Error:  0.14587738422933452 Topo:  17.931240064655235\n",
      "Epoch:  438 Loss:  0.18804717670657678 Error:  0.14585577357493476 Topo:  17.951891505075785\n",
      "Epoch:  439 Loss:  0.18829211672979915 Error:  0.14599545139395548 Topo:  17.96192168047328\n",
      "Epoch:  440 Loss:  0.18845998080904613 Error:  0.1458451376703685 Topo:  17.986494172832924\n",
      "Epoch:  441 Loss:  0.18848445386943702 Error:  0.14606464511441614 Topo:  18.006489902199384\n",
      "Epoch:  442 Loss:  0.18876067812214353 Error:  0.14596463646182042 Topo:  17.94876797042207\n",
      "Epoch:  443 Loss:  0.18844750425415838 Error:  0.14600232819061792 Topo:  17.98705604119215\n",
      "Epoch:  444 Loss:  0.18861236659709565 Error:  0.14589983077641733 Topo:  17.902008273644363\n",
      "Epoch:  445 Loss:  0.18823493231913288 Error:  0.14581430096647696 Topo:  17.84243869781494\n",
      "Epoch:  446 Loss:  0.18963952558840105 Error:  0.1463345324475608 Topo:  18.146085013886413\n",
      "Epoch:  447 Loss:  0.1884808782153501 Error:  0.14577215450431058 Topo:  17.878865384770013\n",
      "Epoch:  448 Loss:  0.1880310780095483 Error:  0.14568195884634635 Topo:  17.836550387079843\n",
      "Epoch:  449 Loss:  0.1881312899246901 Error:  0.14574491357553504 Topo:  17.896527004813006\n",
      "Epoch:  450 Loss:  0.18857157292837154 Error:  0.14583769538802302 Topo:  17.88166381356245\n",
      "Epoch:  451 Loss:  0.18828191041589498 Error:  0.1455314133963185 Topo:  17.736690664005852\n",
      "Epoch:  452 Loss:  0.1883249599598125 Error:  0.14570030846638593 Topo:  17.84971206345244\n",
      "Epoch:  453 Loss:  0.18832343941677115 Error:  0.1457729478796085 Topo:  17.863761536375492\n",
      "Epoch:  454 Loss:  0.1890569120467066 Error:  0.14570362049483968 Topo:  17.777500438119123\n",
      "Epoch:  455 Loss:  0.18834242331767512 Error:  0.1457016154856025 Topo:  17.90532767130229\n",
      "Epoch:  456 Loss:  0.1885400484957381 Error:  0.14581192008214083 Topo:  17.859014779507756\n",
      "Epoch:  457 Loss:  0.1884582350532452 Error:  0.14577655445137425 Topo:  17.9302172518062\n",
      "Epoch:  458 Loss:  0.1883857382271818 Error:  0.14566694842484182 Topo:  17.810517819341786\n",
      "Epoch:  459 Loss:  0.188498049706756 Error:  0.14572557713278753 Topo:  17.939082785280878\n",
      "Epoch:  460 Loss:  0.1883669981877961 Error:  0.1457234396192128 Topo:  17.814118648003674\n",
      "Epoch:  461 Loss:  0.188868456704174 Error:  0.14589932025549654 Topo:  17.955378629490287\n",
      "Epoch:  462 Loss:  0.1878756997649541 Error:  0.14528861231432705 Topo:  17.759961213894233\n",
      "Epoch:  463 Loss:  0.18884276951144555 Error:  0.14571612921660532 Topo:  17.823314838066786\n",
      "Epoch:  464 Loss:  0.18797065540702043 Error:  0.14549236026352752 Topo:  17.803909907084027\n",
      "Epoch:  465 Loss:  0.1879652685926346 Error:  0.14536659263386698 Topo:  17.686872248164192\n",
      "Epoch:  466 Loss:  0.18862634174481124 Error:  0.14555724835145972 Topo:  17.863633538434605\n",
      "Epoch:  467 Loss:  0.1878768125337041 Error:  0.1452166685711838 Topo:  17.65634444231045\n",
      "Epoch:  468 Loss:  0.1882900182536976 Error:  0.14538703363634156 Topo:  17.755277353846385\n",
      "Epoch:  469 Loss:  0.18855164980816985 Error:  0.14567859564534205 Topo:  17.792791092467166\n",
      "Epoch:  470 Loss:  0.1880026839807362 Error:  0.1451915813955718 Topo:  17.721352114648877\n",
      "Epoch:  471 Loss:  0.1883332460761784 Error:  0.1454728951086541 Topo:  17.852342302927713\n",
      "Epoch:  472 Loss:  0.18832308651801355 Error:  0.14539053594459317 Topo:  17.723445743857745\n",
      "Epoch:  473 Loss:  0.18846732788457127 Error:  0.14529740373174588 Topo:  17.595544135499143\n",
      "Epoch:  474 Loss:  0.18843907546140476 Error:  0.1454176171989498 Topo:  17.815488889545737\n",
      "Epoch:  475 Loss:  0.18816389045315587 Error:  0.1455863508546424 Topo:  17.89104770043653\n",
      "Epoch:  476 Loss:  0.18799953314358603 Error:  0.14518942081642722 Topo:  17.64452467707103\n",
      "Epoch:  477 Loss:  0.18784789607196511 Error:  0.145270549296262 Topo:  17.792232576244604\n",
      "Epoch:  478 Loss:  0.188539313163586 Error:  0.145405272166886 Topo:  17.669045071401996\n",
      "Epoch:  479 Loss:  0.18753036776345647 Error:  0.14501736487100225 Topo:  17.615764754974915\n",
      "Epoch:  480 Loss:  0.18812419340282144 Error:  0.14504657708063812 Topo:  17.659938817966484\n",
      "Epoch:  481 Loss:  0.18832295437059002 Error:  0.1451464844588748 Topo:  17.69533489135925\n",
      "Epoch:  482 Loss:  0.18772798856932246 Error:  0.14506366054811878 Topo:  17.590884117309205\n",
      "Epoch:  483 Loss:  0.18763822076206435 Error:  0.14512230371108312 Topo:  17.543994046970756\n",
      "Epoch:  484 Loss:  0.18842171239638758 Error:  0.1453024466594536 Topo:  17.742293774724722\n",
      "Epoch:  485 Loss:  0.1887121977741847 Error:  0.1451693428847604 Topo:  17.575889633087343\n",
      "Epoch:  486 Loss:  0.18816343940303712 Error:  0.14533703756368088 Topo:  17.766839398595387\n",
      "Epoch:  487 Loss:  0.18772025468820583 Error:  0.14448173116602583 Topo:  17.33799166879254\n",
      "Epoch:  488 Loss:  0.1877659772506017 Error:  0.1449583113015055 Topo:  17.61248017499547\n",
      "Epoch:  489 Loss:  0.18744285699136243 Error:  0.14467683171262286 Topo:  17.447700934495753\n",
      "Epoch:  490 Loss:  0.18860411295990745 Error:  0.1448644845785495 Topo:  17.547599980931082\n",
      "Epoch:  491 Loss:  0.18743816678395528 Error:  0.1448607438517188 Topo:  17.5703759793036\n",
      "Epoch:  492 Loss:  0.18775828093468785 Error:  0.14479874364451734 Topo:  17.454504321435255\n",
      "Epoch:  493 Loss:  0.18850035760217085 Error:  0.14524513911344333 Topo:  17.781721332116042\n",
      "Epoch:  494 Loss:  0.188073742086302 Error:  0.14492388845917709 Topo:  17.57570894892344\n",
      "Epoch:  495 Loss:  0.1871819871806813 Error:  0.1442803227883613 Topo:  17.358794332264427\n",
      "Epoch:  496 Loss:  0.18743923342156554 Error:  0.14449917101217602 Topo:  17.350114051453367\n",
      "Epoch:  497 Loss:  0.1883066141319846 Error:  0.1452025311614225 Topo:  17.678220086468908\n",
      "Epoch:  498 Loss:  0.1878036107131821 Error:  0.144950862371636 Topo:  17.691019800608743\n",
      "Epoch:  499 Loss:  0.18747983319673708 Error:  0.14456813670917898 Topo:  17.4221814378293\n",
      "Epoch:  500 Loss:  0.18780704913382046 Error:  0.14455489751821507 Topo:  17.402436616178043\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args) #pickle.load(open('300epoch-model.sav', 'rb'))#\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "train(final_model, train_data_loader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final-model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(final_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
