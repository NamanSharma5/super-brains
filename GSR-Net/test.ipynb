{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "import networkx as nx\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(167, 160, 160)\n",
      "(167, 268, 268)\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = \"../data/lr_train.csv\"\n",
    "hr_data_path = \"../data/hr_train.csv\"\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=\",\").to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=\",\").to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array(\n",
    "    [MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data]\n",
    ")\n",
    "hr_train_data_vectorized = np.array(\n",
    "    [MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data]\n",
    ")\n",
    "\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "num_samples_list = range(num_samples)\n",
    "sample_to_index = dict(zip(num_samples_list, hr_train_data_vectorized))\n",
    "\n",
    "print(lr_train_data_vectorized.shape)\n",
    "print(hr_train_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 160]) torch.Size([1, 268, 268])\n"
     ]
    }
   ],
   "source": [
    "lr_train_data_vectorized = torch.tensor(lr_train_data_vectorized, dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor(hr_train_data_vectorized, dtype=torch.float32)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, lr_data, hr_data, noise_level=0.01):\n",
    "        \"\"\"\n",
    "        lr_data: Low resolution data (torch.tensor)\n",
    "        hr_data: High resolution data (torch.tensor)\n",
    "        noise_level: Standard deviation of Gaussian noise to be added\n",
    "        \"\"\"\n",
    "        self.lr_data = lr_data\n",
    "        self.hr_data = hr_data\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_sample = self.lr_data[idx]\n",
    "        hr_sample = self.hr_data[idx]\n",
    "\n",
    "        # Adding Gaussian noise\n",
    "        noise = torch.randn(lr_sample.size()) * self.noise_level\n",
    "        noisy_lr_sample = lr_sample + noise\n",
    "\n",
    "        # Clipping to ensure values are between 0 and 1\n",
    "        noisy_lr_sample = torch.clamp(noisy_lr_sample, 0, 1)\n",
    "\n",
    "        return noisy_lr_sample, hr_sample\n",
    "\n",
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "sample = next(iter(train_data_loader))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(num_samples * 0.8)\n",
    "\n",
    "subjects_adj, subjects_labels = (\n",
    "    lr_train_data_vectorized[:split],\n",
    "    hr_train_data_vectorized[:split],\n",
    ")\n",
    "\n",
    "held_out_subjects_adj, held_out_subjects_labels = (\n",
    "    lr_train_data_vectorized[split:],\n",
    "    hr_train_data_vectorized[split:],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 30\n",
    "lr = 0.00005\n",
    "lmbda = 25\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320\n",
    "padding = 26\n",
    "dropout = 0.1\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.7, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologicalMeasures:\n",
    "    def __init__(self, graph: Union[np.ndarray, torch.Tensor]):\n",
    "        if isinstance(graph, np.ndarray):\n",
    "            self.graph = nx.Graph(graph)\n",
    "        elif isinstance(graph, torch.Tensor):\n",
    "            graph_numpy = graph.cpu().detach().numpy()\n",
    "            self.graph = nx.Graph(graph_numpy)\n",
    "\n",
    "    def compute_measures(self, number=0):\n",
    "        measures = {}\n",
    "        measures[\"degree\"] = torch.FloatTensor(list(dict(self.graph.degree()).values()))\n",
    "        # measures['clustering'] = torch.FloatTensor(list(nx.clustering(self.graph).values())) # removed due to slow computation\n",
    "        measures[\"closeness\"] = torch.FloatTensor(\n",
    "            list(nx.closeness_centrality(self.graph).values())\n",
    "        )\n",
    "        # measures['betweenness'] = torch.FloatTensor(list(nx.betweenness_centrality(self.graph).values())) # removed due to slow computation\n",
    "        measures[\"pagerank\"] = torch.FloatTensor(list(nx.pagerank(self.graph).values()))\n",
    "        measures[\"eigenvector\"] = torch.FloatTensor(\n",
    "            list(nx.eigenvector_centrality(self.graph).values())\n",
    "        )\n",
    "        return measures\n",
    "\n",
    "\n",
    "def precompute_topological_measures(hr_train_data_vectorized: np.ndarray):\n",
    "    index_to_measure = {}\n",
    "    for index, graph in enumerate(hr_train_data_vectorized):\n",
    "        index_to_measure[index] = TopologicalMeasures(graph).compute_measures()\n",
    "        if index % 10 == 0:\n",
    "            print(f\"Computed measures for {index} graphs\")\n",
    "    return index_to_measure\n",
    "\n",
    "\n",
    "def compute_topological_MAE_loss(\n",
    "    graph1, graph2: Union[np.ndarray, torch.Tensor], precomputed_g1: False\n",
    "):\n",
    "    if precomputed_g1:\n",
    "        measures1 = graph1\n",
    "    else:\n",
    "        measures1 = TopologicalMeasures(graph1).compute_measures()\n",
    "    measures2 = TopologicalMeasures(graph2).compute_measures()\n",
    "    loss = 0\n",
    "    # compute MAE for each measure\n",
    "\n",
    "    for measure in measures1:\n",
    "        loss += F.l1_loss(measures1[measure], measures2[measure])\n",
    "    loss = loss / len(measures1)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute topological measures for hr_train_data_vectorized\n",
    "# precomputed_measures = precompute_topological_measures(hr_train_data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "\n",
    "def train(model, optimizer, subjects_adj, subjects_labels, args):\n",
    "    # , subjects_adj_test, subjects_ground_truth_test):\n",
    "\n",
    "    all_epochs_loss = []\n",
    "    no_epochs = args.epochs\n",
    "\n",
    "    for epoch in range(no_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_error = []\n",
    "        epoch_topo = []\n",
    "\n",
    "        model.train()\n",
    "        for index, (lr, hr) in enumerate(train_data_loader):\n",
    "            \n",
    "            lr = lr.reshape(160, 160)\n",
    "            hr = hr.reshape(268, 268)\n",
    "\n",
    "            model_outputs, net_outs, start_gcn_outs, layer_outs = model(lr)\n",
    "            model_outputs = unpad(model_outputs, args.padding)\n",
    "\n",
    "            # weights = unpad(model.layer.weights, args.padding)\n",
    "\n",
    "            padded_hr = pad_HR_adj(hr, args.padding)\n",
    "            eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO=\"U\")\n",
    "\n",
    "            # loss = criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + args.lmbda * criterion(model_outputs, hr)\n",
    "            # loss = criterion(model_outputs, hr)\n",
    "            loss = (\n",
    "                args.lmbda * criterion(net_outs, start_gcn_outs)\n",
    "                + criterion(model.layer.weights, U_hr)\n",
    "                + criterion(model_outputs, hr)\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "            # topo = compute_topological_MAE_loss(hr, model_outputs, precomputed_g1 = False)\n",
    "            # topo = compute_topological_MAE_loss(\n",
    "            #     precomputed_measures[index], model_outputs, precomputed_g1=True\n",
    "            # )\n",
    "            # print(\"Time to compute topo: \", time.time() - start_time)\n",
    "            # print(topo.item())\n",
    "\n",
    "            error = criterion(model_outputs, hr)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            epoch_error.append(error.item())\n",
    "            # epoch_topo.append(topo.item())\n",
    "\n",
    "        model.eval()\n",
    "        # print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error))\n",
    "        print(\n",
    "            \"Epoch: \",\n",
    "            epoch + 1,\n",
    "            \"Loss: \",\n",
    "            np.mean(epoch_loss),\n",
    "            \"Error: \",\n",
    "            np.mean(epoch_error),\n",
    "            # \"Topo: \",\n",
    "            # np.mean(epoch_topo),\n",
    "        )\n",
    "        # test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "        # test(model, subjects_adj_test, subjects_ground_truth_test, args)\n",
    "        all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# for train_index, test_index in cv.split(subjects_adj):\n",
    "#     subjects_adj_train = subjects_adj[train_index]  # Get training data\n",
    "#     subjects_adj_test = subjects_adj[test_index]   # Get testing data\n",
    "#     subjects_ground_truth_train = subjects_labels[train_index]\n",
    "#     subjects_ground_truth_test = subjects_labels[test_index]\n",
    "\n",
    "#     train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "\n",
    "#     print('Held out test score:')\n",
    "#     test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "#     print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shivbhatia/AllFiles/university/year_4/computing/deep_graph_based_learning/super-brains/.venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/shivbhatia/AllFiles/university/year_4/computing/deep_graph_based_learning/super-brains/.venv/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.5707595127071449 Error:  0.2406230556393812 Topo:  nan\n",
      "Epoch:  2 Loss:  0.38345538570495424 Error:  0.20803140344733964 Topo:  nan\n",
      "Epoch:  3 Loss:  0.36614711645120634 Error:  0.1969838216454683 Topo:  nan\n",
      "Epoch:  4 Loss:  0.3543242904954328 Error:  0.19150986482283314 Topo:  nan\n",
      "Epoch:  5 Loss:  0.3433955650843546 Error:  0.1876029204465672 Topo:  nan\n",
      "Epoch:  6 Loss:  0.3334669738472579 Error:  0.18493208735288974 Topo:  nan\n",
      "Epoch:  7 Loss:  0.32464526620453704 Error:  0.18312408440484257 Topo:  nan\n",
      "Epoch:  8 Loss:  0.3165310728335809 Error:  0.18154095211428797 Topo:  nan\n",
      "Epoch:  9 Loss:  0.3085047434903904 Error:  0.1803627714603961 Topo:  nan\n",
      "Epoch:  10 Loss:  0.3014208462066993 Error:  0.17958564529875795 Topo:  nan\n",
      "Epoch:  11 Loss:  0.294827392893637 Error:  0.17898524226899631 Topo:  nan\n",
      "Epoch:  12 Loss:  0.28837572653850396 Error:  0.17817109546618548 Topo:  nan\n",
      "Epoch:  13 Loss:  0.28255740896670406 Error:  0.1774993182834751 Topo:  nan\n",
      "Epoch:  14 Loss:  0.27724802797426007 Error:  0.17714134394051786 Topo:  nan\n",
      "Epoch:  15 Loss:  0.27318432605909015 Error:  0.17742039557702527 Topo:  nan\n",
      "Epoch:  16 Loss:  0.26843294215773394 Error:  0.1767958673888338 Topo:  nan\n",
      "Epoch:  17 Loss:  0.2643635812812223 Error:  0.17660814479082645 Topo:  nan\n",
      "Epoch:  18 Loss:  0.2609261188678399 Error:  0.1764289055041924 Topo:  nan\n",
      "Epoch:  19 Loss:  0.2576340283462387 Error:  0.1762819564092659 Topo:  nan\n",
      "Epoch:  20 Loss:  0.2546071526176201 Error:  0.17592990942700895 Topo:  nan\n",
      "Epoch:  21 Loss:  0.2521001767969417 Error:  0.17592684612302723 Topo:  nan\n",
      "Epoch:  22 Loss:  0.2496199713138763 Error:  0.17569712428989526 Topo:  nan\n",
      "Epoch:  23 Loss:  0.2474404436028646 Error:  0.17550927882422945 Topo:  nan\n",
      "Epoch:  24 Loss:  0.2463012728505506 Error:  0.17591223388374921 Topo:  nan\n",
      "Epoch:  25 Loss:  0.24441545166655215 Error:  0.17543020175245708 Topo:  nan\n",
      "Epoch:  26 Loss:  0.24310395407105634 Error:  0.17554682198755756 Topo:  nan\n",
      "Epoch:  27 Loss:  0.24179343914914275 Error:  0.17531123828745174 Topo:  nan\n",
      "Epoch:  28 Loss:  0.24114752948997978 Error:  0.17544384782542727 Topo:  nan\n",
      "Epoch:  29 Loss:  0.24009889078711322 Error:  0.17528422881743153 Topo:  nan\n",
      "Epoch:  30 Loss:  0.23927235139344266 Error:  0.17523072972269116 Topo:  nan\n"
     ]
    }
   ],
   "source": [
    "# final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "train(final_model, optimizer, lr_train_data_vectorized, hr_train_data_vectorized, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 12720)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "# Generate submission\n",
    "\n",
    "# load csvs as numpy\n",
    "test_lr_data_path = \"../data/lr_test.csv\"\n",
    "\n",
    "# lr_test_data = np.loadtxt(test_lr_data_path, delimiter=',')\n",
    "lr_test_data = pd.read_csv(test_lr_data_path, delimiter=\",\").to_numpy()\n",
    "print(lr_test_data.shape)\n",
    "lr_test_data[lr_test_data < 0] = 0\n",
    "np.nan_to_num(lr_test_data, copy=False)\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_test_data_vectorized = np.array(\n",
    "    [MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data]\n",
    ")\n",
    "print(lr_test_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 (35778,)\n",
      "(4007136,)\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "preds = []\n",
    "for lr in lr_test_data_vectorized:\n",
    "    lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "\n",
    "    model_outputs, _, _, _ = final_model(lr)\n",
    "    model_outputs = unpad(model_outputs, args.padding)\n",
    "    preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "print(len(preds), preds[0].shape)\n",
    "r = np.hstack(preds)\n",
    "print(r.shape)\n",
    "meltedDF = r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = meltedDF.shape[0]\n",
    "df = pd.DataFrame({\"ID\": np.arange(1, n + 1), \"Predicted\": meltedDF})\n",
    "df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
