{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "import networkx as nx\n",
    "from typing import Union\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "# random_seed = 42\n",
    "# random.seed(random_seed)\n",
    "# np.random.seed(random_seed)\n",
    "# torch.manual_seed(random_seed)\n",
    "\n",
    "# # Check for CUDA (GPU support) and set device accordingly\n",
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     print(\"CUDA is available. Using GPU.\")\n",
    "#     torch.cuda.manual_seed(random_seed)\n",
    "#     torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "#     # Additional settings for ensuring reproducibility on CUDA\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "# else:\n",
    "device = torch.device(\"cpu\")\n",
    "    # print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data])\n",
    "hr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data])\n",
    "\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "num_samples_list = range(num_samples)\n",
    "sample_to_index = dict(zip(num_samples_list, hr_train_data_vectorized))\n",
    "\n",
    "split = int(num_samples * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 160]) torch.Size([1, 268, 268])\n"
     ]
    }
   ],
   "source": [
    "lr_train_data_vectorized = torch.tensor(lr_train_data_vectorized, dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor(hr_train_data_vectorized, dtype=torch.float32)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, lr_data, hr_data, noise_level=0.01):\n",
    "        \"\"\"\n",
    "        lr_data: Low resolution data (torch.tensor)\n",
    "        hr_data: High resolution data (torch.tensor)\n",
    "        noise_level: Standard deviation of Gaussian noise to be added\n",
    "        \"\"\"\n",
    "        self.lr_data = lr_data\n",
    "        self.hr_data = hr_data\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_sample = self.lr_data[idx]\n",
    "        hr_sample = self.hr_data[idx]\n",
    "\n",
    "        # Adding Gaussian noise\n",
    "        noise = torch.randn(lr_sample.size()) * self.noise_level\n",
    "        noisy_lr_sample = lr_sample + noise\n",
    "\n",
    "        # Clipping to ensure values are between 0 and 1\n",
    "        noisy_lr_sample = torch.clamp(noisy_lr_sample, 0, 1)\n",
    "\n",
    "        return noisy_lr_sample, hr_sample\n",
    "\n",
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.25)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "sample = next(iter(train_data_loader))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# subjects_adj,subjects_labels = lr_train_data_vectorized[:split], hr_train_data_vectorized[:split]\n",
    "\n",
    "# held_out_subjects_adj,held_out_subjects_labels = lr_train_data_vectorized[split:], hr_train_data_vectorized[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 250\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, train_data_loader, optimizer, args): \n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "    epoch_topo = []\n",
    "\n",
    "    model.train()\n",
    "    for lr, hr in train_data_loader: \n",
    "      lr.to(device)   \n",
    "      hr.to(device)  \n",
    "      lr = lr.reshape(160, 160)\n",
    "      hr = hr.reshape(268, 268)\n",
    "\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "\n",
    "      topo = compute_topological_MAE_loss(hr, model_outputs)\n",
    "      \n",
    "      loss += args.lamdba_topo * topo\n",
    "\n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      epoch_topo.append(topo.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error),\n",
    "          \"Topo: \", np.mean(epoch_topo))\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# for train_index, test_index in cv.split(subjects_adj):\n",
    "#     subjects_adj_train = subjects_adj[train_index]  # Get training data \n",
    "#     subjects_adj_test = subjects_adj[test_index]   # Get testing data \n",
    "#     subjects_ground_truth_train = subjects_labels[train_index]\n",
    "#     subjects_ground_truth_test = subjects_labels[test_index]\n",
    "\n",
    "#     train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "    \n",
    "#     print('Held out test score:')\n",
    "#     test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "#     print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.43957221632946036 Error:  0.19513740484229106 Topo:  25.994731166405593\n",
      "Epoch:  2 Loss:  0.3224733923366684 Error:  0.17929550996440613 Topo:  19.299362725126528\n",
      "Epoch:  3 Loss:  0.31739621533605156 Error:  0.17731826069826137 Topo:  19.094766319868807\n",
      "Epoch:  4 Loss:  0.31281009816124056 Error:  0.1758801123339259 Topo:  18.85132145453356\n",
      "Epoch:  5 Loss:  0.3092679688316619 Error:  0.17531977228061882 Topo:  19.0426359233742\n",
      "Epoch:  6 Loss:  0.30437449227550073 Error:  0.17363158876667478 Topo:  18.654082446755048\n",
      "Epoch:  7 Loss:  0.30184652062947165 Error:  0.17312008829530842 Topo:  18.98274203546033\n",
      "Epoch:  8 Loss:  0.2976593819563974 Error:  0.1717436775832833 Topo:  19.149108966667495\n",
      "Epoch:  9 Loss:  0.2927160609268143 Error:  0.16938561040484262 Topo:  18.901615045741647\n",
      "Epoch:  10 Loss:  0.2892707214205565 Error:  0.16787429617907473 Topo:  19.237397085406823\n",
      "Epoch:  11 Loss:  0.2834456574595617 Error:  0.16490871356632894 Topo:  18.794494154924404\n",
      "Epoch:  12 Loss:  0.2803251792392331 Error:  0.16355928662651315 Topo:  18.837710985880413\n",
      "Epoch:  13 Loss:  0.2768289411139345 Error:  0.1624840066104592 Topo:  18.80701780604745\n",
      "Epoch:  14 Loss:  0.27391456790313035 Error:  0.1612320593195761 Topo:  18.72687638448384\n",
      "Epoch:  15 Loss:  0.2714454498655068 Error:  0.16028290612255028 Topo:  18.708602362764097\n",
      "Epoch:  16 Loss:  0.2693167580458932 Error:  0.1595677919908912 Topo:  18.735699699310487\n",
      "Epoch:  17 Loss:  0.26758132045140526 Error:  0.15897819970896143 Topo:  18.86246234071469\n",
      "Epoch:  18 Loss:  0.2659291197618325 Error:  0.15841565321305554 Topo:  18.839779162835217\n",
      "Epoch:  19 Loss:  0.2643714330332008 Error:  0.15785159001093424 Topo:  18.9417773263897\n",
      "Epoch:  20 Loss:  0.2621951639473795 Error:  0.15675602193007213 Topo:  18.636166269907694\n",
      "Epoch:  21 Loss:  0.2606797695695283 Error:  0.15626552825916312 Topo:  18.592669949560108\n",
      "Epoch:  22 Loss:  0.25966489118730235 Error:  0.156054411284224 Topo:  18.597355380029736\n",
      "Epoch:  23 Loss:  0.25867198309498635 Error:  0.15558709605725227 Topo:  18.719539499568366\n",
      "Epoch:  24 Loss:  0.25743258820322457 Error:  0.15516002404832555 Topo:  18.649717353775117\n",
      "Epoch:  25 Loss:  0.2569047459763681 Error:  0.1549215494337196 Topo:  18.741309428643323\n",
      "Epoch:  26 Loss:  0.256080963922118 Error:  0.15453510753765792 Topo:  18.76047751146876\n",
      "Epoch:  27 Loss:  0.2556554531444333 Error:  0.15437043355610555 Topo:  18.840032154928423\n",
      "Epoch:  28 Loss:  0.2540638001557596 Error:  0.15370825575497335 Topo:  18.573141594846806\n",
      "Epoch:  29 Loss:  0.25408945710002306 Error:  0.15371581858503605 Topo:  18.778746559234435\n",
      "Epoch:  30 Loss:  0.25304778375311526 Error:  0.1532128297104807 Topo:  18.657121201475224\n",
      "Epoch:  31 Loss:  0.2527045517446038 Error:  0.15327366669020967 Topo:  18.688523418175247\n",
      "Epoch:  32 Loss:  0.2518968736518643 Error:  0.15278509134304025 Topo:  18.604497441274678\n",
      "Epoch:  33 Loss:  0.25146055596317357 Error:  0.1526511892408668 Topo:  18.716386001267118\n",
      "Epoch:  34 Loss:  0.25103450071311995 Error:  0.1524632885427532 Topo:  18.59678073129254\n",
      "Epoch:  35 Loss:  0.25051897889125846 Error:  0.1523604154765249 Topo:  18.573857004770975\n",
      "Epoch:  36 Loss:  0.2500388064605747 Error:  0.1521621989811252 Topo:  18.530606869452015\n",
      "Epoch:  37 Loss:  0.24992014717556046 Error:  0.15224889795223395 Topo:  18.76596723773522\n",
      "Epoch:  38 Loss:  0.24989044889361559 Error:  0.1523728722226834 Topo:  18.800130564295603\n",
      "Epoch:  39 Loss:  0.2493081193662689 Error:  0.15217288784281222 Topo:  18.726257980940584\n",
      "Epoch:  40 Loss:  0.24866416265150745 Error:  0.15181785812991822 Topo:  18.58468681609559\n",
      "Epoch:  41 Loss:  0.24839928442846515 Error:  0.1518290600109243 Topo:  18.66743596585211\n",
      "Epoch:  42 Loss:  0.24803906188396638 Error:  0.15161854135775996 Topo:  18.605273081157023\n",
      "Epoch:  43 Loss:  0.24724338083210107 Error:  0.15150533621004242 Topo:  18.635945365814393\n",
      "Epoch:  44 Loss:  0.24731255014856418 Error:  0.151531532942178 Topo:  18.656009965314123\n",
      "Epoch:  45 Loss:  0.24686391955007336 Error:  0.1512010337082212 Topo:  18.50042443646642\n",
      "Epoch:  46 Loss:  0.24729278317825523 Error:  0.15173495788417177 Topo:  18.734896614166075\n",
      "Epoch:  47 Loss:  0.24589183899813782 Error:  0.15084503947974678 Topo:  18.536933790423912\n",
      "Epoch:  48 Loss:  0.24525372907073198 Error:  0.1507519134236667 Topo:  18.323116519493972\n",
      "Epoch:  49 Loss:  0.24595347252077684 Error:  0.15125606052889795 Topo:  18.59817343272135\n",
      "Epoch:  50 Loss:  0.24525516011757764 Error:  0.15097860264742446 Topo:  18.562970761053577\n",
      "Epoch:  51 Loss:  0.24484084403800394 Error:  0.15091247374783018 Topo:  18.596576536486964\n",
      "Epoch:  52 Loss:  0.24443049657487584 Error:  0.15069206804037094 Topo:  18.495187873611908\n",
      "Epoch:  53 Loss:  0.24450282654362523 Error:  0.150852380443119 Topo:  18.61150615943406\n",
      "Epoch:  54 Loss:  0.24360262294729312 Error:  0.15049156493055607 Topo:  18.491285946554765\n",
      "Epoch:  55 Loss:  0.24377322589565895 Error:  0.15070888355463566 Topo:  18.5663163590574\n",
      "Epoch:  56 Loss:  0.24353577839043325 Error:  0.1506512793684434 Topo:  18.49513762582562\n",
      "Epoch:  57 Loss:  0.2435491390213995 Error:  0.15072885967657237 Topo:  18.648067845555836\n",
      "Epoch:  58 Loss:  0.24228651714539098 Error:  0.1502623206037961 Topo:  18.363865024315384\n",
      "Epoch:  59 Loss:  0.2420449322033785 Error:  0.15017241971221512 Topo:  18.41679896280437\n",
      "Epoch:  60 Loss:  0.24239643890700655 Error:  0.1504623722619639 Topo:  18.654577266670273\n",
      "Epoch:  61 Loss:  0.24182098684553616 Error:  0.15022466893859965 Topo:  18.50196420361182\n",
      "Epoch:  62 Loss:  0.24190184411531437 Error:  0.15045363679082094 Topo:  18.597299341670052\n",
      "Epoch:  63 Loss:  0.24161625631198197 Error:  0.15029111111949303 Topo:  18.62962408122902\n",
      "Epoch:  64 Loss:  0.24137651750784433 Error:  0.15022217821396752 Topo:  18.686110250964138\n",
      "Epoch:  65 Loss:  0.24082639322666352 Error:  0.15021523288980929 Topo:  18.55614125371693\n",
      "Epoch:  66 Loss:  0.2401247905221528 Error:  0.14979828505994316 Topo:  18.416639716325406\n",
      "Epoch:  67 Loss:  0.24165374986425844 Error:  0.15077484840761401 Topo:  18.902579330398652\n",
      "Epoch:  68 Loss:  0.24003331568426714 Error:  0.1499900396712526 Topo:  18.516894437595756\n",
      "Epoch:  69 Loss:  0.2395639601581825 Error:  0.14997384554433252 Topo:  18.52076132426005\n",
      "Epoch:  70 Loss:  0.23951342452072097 Error:  0.15010806150779038 Topo:  18.603105847707052\n",
      "Epoch:  71 Loss:  0.2390927977547674 Error:  0.14982997340534976 Topo:  18.50282862086496\n",
      "Epoch:  72 Loss:  0.23919632557980314 Error:  0.1498196446253154 Topo:  18.580993852215613\n",
      "Epoch:  73 Loss:  0.2384231791881744 Error:  0.14972994458711075 Topo:  18.468421770427042\n",
      "Epoch:  74 Loss:  0.23837703698409532 Error:  0.14980792356822306 Topo:  18.526351351937848\n",
      "Epoch:  75 Loss:  0.23893929866259683 Error:  0.15025794064391873 Topo:  18.84538297596092\n",
      "Epoch:  76 Loss:  0.2382219785344815 Error:  0.1498594283165332 Topo:  18.52795378176752\n",
      "Epoch:  77 Loss:  0.23755776132652145 Error:  0.14959480091483293 Topo:  18.515618986712244\n",
      "Epoch:  78 Loss:  0.23715886500424255 Error:  0.1495515866104714 Topo:  18.42884979704897\n",
      "Epoch:  79 Loss:  0.23745392048787214 Error:  0.1498081505209386 Topo:  18.57459826669293\n",
      "Epoch:  80 Loss:  0.23682837714692076 Error:  0.14946793666678274 Topo:  18.496190219582196\n",
      "Epoch:  81 Loss:  0.23719174651328676 Error:  0.14974481768593817 Topo:  18.59933324916634\n",
      "Epoch:  82 Loss:  0.23607864824240793 Error:  0.1492948298771938 Topo:  18.385864326339995\n",
      "Epoch:  83 Loss:  0.23602840125917673 Error:  0.14944884109639836 Topo:  18.473635947632932\n",
      "Epoch:  84 Loss:  0.235839218971972 Error:  0.1493688417141309 Topo:  18.45887505651234\n",
      "Epoch:  85 Loss:  0.23575794295279565 Error:  0.1494573663897857 Topo:  18.534744810915278\n",
      "Epoch:  86 Loss:  0.23498294042969892 Error:  0.14911105550691753 Topo:  18.397345177427738\n",
      "Epoch:  87 Loss:  0.23553493966956338 Error:  0.1495961470607512 Topo:  18.48481647125975\n",
      "Epoch:  88 Loss:  0.23451024543739366 Error:  0.1491973080231758 Topo:  18.31688173374016\n",
      "Epoch:  89 Loss:  0.2350269712195425 Error:  0.14955518782852653 Topo:  18.673584195668113\n",
      "Epoch:  90 Loss:  0.23437500829825145 Error:  0.1491058353267744 Topo:  18.37102393190304\n",
      "Epoch:  91 Loss:  0.23425815116145654 Error:  0.14926521493467743 Topo:  18.457357852044932\n",
      "Epoch:  92 Loss:  0.23422248938126478 Error:  0.14947187681933363 Topo:  18.53070509219598\n",
      "Epoch:  93 Loss:  0.23421340395590504 Error:  0.14937682039366512 Topo:  18.49953518941731\n",
      "Epoch:  94 Loss:  0.2342577698880327 Error:  0.14961505413590792 Topo:  18.565849424122337\n",
      "Epoch:  95 Loss:  0.2336955319443149 Error:  0.14940608841573408 Topo:  18.554436203962315\n",
      "Epoch:  96 Loss:  0.23313870972502018 Error:  0.1490973593321389 Topo:  18.42513327684231\n",
      "Epoch:  97 Loss:  0.23317333342072494 Error:  0.14925367478838938 Topo:  18.498705310022046\n",
      "Epoch:  98 Loss:  0.23292019178053577 Error:  0.14915580159711267 Topo:  18.490628607972653\n",
      "Epoch:  99 Loss:  0.23306122582829641 Error:  0.14942849235620329 Topo:  18.671982907963372\n",
      "Epoch:  100 Loss:  0.2324938530515054 Error:  0.14917924705736652 Topo:  18.479817082068163\n",
      "Epoch:  101 Loss:  0.23282366607360497 Error:  0.1494617962730145 Topo:  18.5880232371256\n",
      "Epoch:  102 Loss:  0.23256313791888916 Error:  0.1492878405187658 Topo:  18.56753465800942\n",
      "Epoch:  103 Loss:  0.23217812974652843 Error:  0.1492682273337941 Topo:  18.586513125254008\n",
      "Epoch:  104 Loss:  0.2317066474589045 Error:  0.14904054022299315 Topo:  18.461847864939067\n",
      "Epoch:  105 Loss:  0.23151230276701693 Error:  0.1492116336604792 Topo:  18.485083488647096\n",
      "Epoch:  106 Loss:  0.23137021609052213 Error:  0.14897100506963845 Topo:  18.43409413206363\n",
      "Epoch:  107 Loss:  0.23129794608333154 Error:  0.149074205782956 Topo:  18.54660405941352\n",
      "Epoch:  108 Loss:  0.23115244382869699 Error:  0.14912869330651746 Topo:  18.473276743631878\n",
      "Epoch:  109 Loss:  0.23066209560025952 Error:  0.14899517439618082 Topo:  18.416280163976246\n",
      "Epoch:  110 Loss:  0.23027596216715737 Error:  0.1488497639576832 Topo:  18.39537479492005\n",
      "Epoch:  111 Loss:  0.23056082463193084 Error:  0.1490278722283369 Topo:  18.430748574034183\n",
      "Epoch:  112 Loss:  0.23024300017399701 Error:  0.14904172397302295 Topo:  18.443335390376472\n",
      "Epoch:  113 Loss:  0.22993285186633378 Error:  0.14890914053438667 Topo:  18.437557483147717\n",
      "Epoch:  114 Loss:  0.22993305405813777 Error:  0.14888904386473273 Topo:  18.39502561306525\n",
      "Epoch:  115 Loss:  0.2293959548194965 Error:  0.1487456998693015 Topo:  18.436263506997847\n",
      "Epoch:  116 Loss:  0.2293398714886454 Error:  0.14879671783147458 Topo:  18.361057915373475\n",
      "Epoch:  117 Loss:  0.22966526136426868 Error:  0.14906876583299236 Topo:  18.47284562002399\n",
      "Epoch:  118 Loss:  0.22909892792116382 Error:  0.14878787595533324 Topo:  18.508385121465444\n",
      "Epoch:  119 Loss:  0.22912427791935241 Error:  0.1488661381031225 Topo:  18.488507476395476\n",
      "Epoch:  120 Loss:  0.22871160310899427 Error:  0.1487594401675784 Topo:  18.367894138404708\n",
      "Epoch:  121 Loss:  0.2291729291399082 Error:  0.14896335840939048 Topo:  18.60947422210328\n",
      "Epoch:  122 Loss:  0.22816408242651087 Error:  0.148521900712373 Topo:  18.34736619595282\n",
      "Epoch:  123 Loss:  0.22843701361182206 Error:  0.14874187522305699 Topo:  18.471192639744924\n",
      "Epoch:  124 Loss:  0.22830237126992847 Error:  0.14881851278736205 Topo:  18.45063516194235\n",
      "Epoch:  125 Loss:  0.22771607553530596 Error:  0.14859725008467714 Topo:  18.389022507353456\n",
      "Epoch:  126 Loss:  0.22823920069697373 Error:  0.14890123417455994 Topo:  18.537198717722635\n",
      "Epoch:  127 Loss:  0.2278700906359507 Error:  0.1488511310276871 Topo:  18.47900663592858\n",
      "Epoch:  128 Loss:  0.22752629898622365 Error:  0.14878344709823232 Topo:  18.530146336127185\n",
      "Epoch:  129 Loss:  0.22756024775747768 Error:  0.1487399587285019 Topo:  18.559434159787116\n",
      "Epoch:  130 Loss:  0.22796319585717367 Error:  0.14909990497691902 Topo:  18.584081883915886\n",
      "Epoch:  131 Loss:  0.2271424650610564 Error:  0.14871735687919718 Topo:  18.45453947032997\n",
      "Epoch:  132 Loss:  0.22738594825039365 Error:  0.14876880889345787 Topo:  18.60737706372838\n",
      "Epoch:  133 Loss:  0.22675678009044625 Error:  0.1485436451203095 Topo:  18.417755481011852\n",
      "Epoch:  134 Loss:  0.22641431633940715 Error:  0.14856899904455254 Topo:  18.31843319624484\n",
      "Epoch:  135 Loss:  0.22644033581910733 Error:  0.14854151465578708 Topo:  18.359164940382907\n",
      "Epoch:  136 Loss:  0.22646322382424405 Error:  0.14861409160905256 Topo:  18.482329876836904\n",
      "Epoch:  137 Loss:  0.22636571124999108 Error:  0.14873203786904227 Topo:  18.466755061806317\n",
      "Epoch:  138 Loss:  0.2265586176675237 Error:  0.14882162298092586 Topo:  18.495111796670333\n",
      "Epoch:  139 Loss:  0.22668964670089906 Error:  0.14889657876627174 Topo:  18.615878670515414\n",
      "Epoch:  140 Loss:  0.22572063587739796 Error:  0.14844599656180707 Topo:  18.329091928676217\n",
      "Epoch:  141 Loss:  0.22572868411055583 Error:  0.1485714167356491 Topo:  18.44015736494236\n",
      "Epoch:  142 Loss:  0.22536643759576147 Error:  0.148331260342084 Topo:  18.300161715753063\n",
      "Epoch:  143 Loss:  0.22583743769251657 Error:  0.14872614954581517 Topo:  18.5007731272075\n",
      "Epoch:  144 Loss:  0.2248833259779536 Error:  0.14816120677365513 Topo:  18.244725301594077\n",
      "Epoch:  145 Loss:  0.2255084833163701 Error:  0.14863417614362912 Topo:  18.50567224354087\n",
      "Epoch:  146 Loss:  0.22547497256787238 Error:  0.14863764989875747 Topo:  18.529838116582997\n",
      "Epoch:  147 Loss:  0.22440070368929538 Error:  0.1481291410362649 Topo:  18.30671723302967\n",
      "Epoch:  148 Loss:  0.22455267278020252 Error:  0.14836344872406143 Topo:  18.27956204785558\n",
      "Epoch:  149 Loss:  0.2252370995318818 Error:  0.1486795878785099 Topo:  18.583985294410567\n",
      "Epoch:  150 Loss:  0.2247598514763895 Error:  0.1484894674427495 Topo:  18.468936885902266\n",
      "Epoch:  151 Loss:  0.22474297881126404 Error:  0.14849990079859773 Topo:  18.431083502169855\n",
      "Epoch:  152 Loss:  0.2238858481367191 Error:  0.14815722628981767 Topo:  18.216900848342988\n",
      "Epoch:  153 Loss:  0.22448019103375738 Error:  0.14855700700047486 Topo:  18.46750811045755\n",
      "Epoch:  154 Loss:  0.22485677978235805 Error:  0.14874621511933334 Topo:  18.65048003625013\n",
      "Epoch:  155 Loss:  0.22414485726527825 Error:  0.14831394358666358 Topo:  18.395283664771895\n",
      "Epoch:  156 Loss:  0.22395761114751508 Error:  0.1483806703172758 Topo:  18.443383393887274\n",
      "Epoch:  157 Loss:  0.22380593981214625 Error:  0.14832671994934538 Topo:  18.40275589149155\n",
      "Epoch:  158 Loss:  0.22350678088779222 Error:  0.148203425733986 Topo:  18.40046448050859\n",
      "Epoch:  159 Loss:  0.22408028268171643 Error:  0.14859349824889692 Topo:  18.538945198059082\n",
      "Epoch:  160 Loss:  0.22390601262004076 Error:  0.1485747234550065 Topo:  18.486663047425047\n",
      "Epoch:  161 Loss:  0.2237621054142535 Error:  0.14842297154629303 Topo:  18.399193004219832\n",
      "Epoch:  162 Loss:  0.22281400207987803 Error:  0.14810201454305363 Topo:  18.142016148138904\n",
      "Epoch:  163 Loss:  0.22325316928104014 Error:  0.1482775943900297 Topo:  18.409352011309412\n",
      "Epoch:  164 Loss:  0.22273267546813644 Error:  0.14809931181148142 Topo:  18.23589202172742\n",
      "Epoch:  165 Loss:  0.22314399409436894 Error:  0.1484312724745916 Topo:  18.366529144926698\n",
      "Epoch:  166 Loss:  0.22271020460628463 Error:  0.14825211117367545 Topo:  18.321410413273796\n",
      "Epoch:  167 Loss:  0.22265685380932815 Error:  0.148078903020499 Topo:  18.33674309210863\n",
      "Epoch:  168 Loss:  0.22310470919052283 Error:  0.1485420369548712 Topo:  18.421954526158864\n",
      "Epoch:  169 Loss:  0.222865667885649 Error:  0.14833460302053098 Topo:  18.399382745434423\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "train(final_model, train_data_loader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 12720)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "#Generate submission \n",
    "\n",
    "# load csvs as numpy\n",
    "test_lr_data_path = '../data/lr_test.csv'\n",
    "\n",
    "lr_test_data = pd.read_csv(test_lr_data_path, delimiter=',').to_numpy()\n",
    "print(lr_test_data.shape)\n",
    "lr_test_data[lr_test_data < 0] = 0\n",
    "np.nan_to_num(lr_test_data, copy=False)\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "print(lr_test_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 (35778,)\n",
      "(4007136,)\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "preds = []\n",
    "for lr in lr_test_data_vectorized:      \n",
    "  lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "  \n",
    "  model_outputs, _, _, _ = final_model(lr)\n",
    "  model_outputs  = unpad(model_outputs, args.padding)\n",
    "  preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "print(len(preds), preds[0].shape)\n",
    "r = np.hstack(preds)\n",
    "print(r.shape)\n",
    "meltedDF = r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = meltedDF.shape[0]\n",
    "df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                   'Predicted': meltedDF})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
