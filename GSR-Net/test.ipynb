{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "\n",
    "from MatrixVectorizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = np.loadtxt(lr_data_path, delimiter=',')\n",
    "hr_train_data = np.loadtxt(hr_data_path, delimiter=',')\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data])\n",
    "hr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 260) for row in hr_train_data])\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "split = int(num_samples * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_adj,subjects_labels = lr_train_data_vectorized[:split], hr_train_data_vectorized[:split]\n",
    "\n",
    "held_out_subjects_adj,held_out_subjects_labels = lr_train_data_vectorized[split:], hr_train_data_vectorized[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_train_data_vectorized.shape=(168, 160, 160)\n",
      "lr_train.shape=(134, 160, 160)\n",
      "lr_test.shape=(34, 160, 160)\n",
      "hr_train_data_vectorized.shape=(168, 260, 260)\n",
      "hr_train.shape=(134, 260, 260)\n",
      "hr_test.shape=(34, 260, 260)\n"
     ]
    }
   ],
   "source": [
    "num_splt = 3\n",
    "epochs = 15\n",
    "lr = 0.0001\n",
    "lmbda = 500\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 1024\n",
    "padding = 30\n",
    "dropout = 0.15\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ks = [0]\n",
    "ks = [0.7, 0.3]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj,subjects_labels, args, subjects_adj_test, subjects_ground_truth_test):\n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "\n",
    "    model.train()\n",
    "    for lr,hr in zip(subjects_adj,subjects_labels):      \n",
    "      lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "      hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "      \n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      # model_outputs  = unpad(model_outputs, args.padding)\n",
    "      # weights = unpad(model.layer.weights, args.padding)\n",
    "      \n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      # print the shapes of the outputs\n",
    "      # print(f\"{net_outs.shape} ; {start_gcn_outs.shape}\")\n",
    "      # print(f\"{model.layer.weights.shape} ; {U_hr.shape}\")\n",
    "      # print(f\"{model_outputs.shape} ; {hr.shape}\")\n",
    "      \n",
    "      # loss = criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + args.lmbda * criterion(model_outputs, hr) \n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, padded_hr) \n",
    "\n",
    "      \n",
    "      error = criterion(model_outputs, padded_hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error))\n",
    "    test(model, subjects_adj_test, subjects_ground_truth_test, args)\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  3.6596158515201527 Error:  0.15330434565463763\n",
      "Test error MAE:  372.8449371493525\n",
      "Epoch:  2 Loss:  1.9087520810995209 Error:  0.14469659378689326\n",
      "Test error MAE:  372.84525650342306\n",
      "Epoch:  3 Loss:  1.5236091921838482 Error:  0.14320210626955782\n",
      "Test error MAE:  372.853780942493\n",
      "Epoch:  4 Loss:  1.2878693797615137 Error:  0.1424808210871193\n",
      "Test error MAE:  372.8468992928664\n",
      "Epoch:  5 Loss:  1.1263392005073891 Error:  0.1414220974351583\n",
      "Test error MAE:  372.8462562004725\n",
      "Epoch:  6 Loss:  1.0410796604799422 Error:  0.14068403078264066\n",
      "Test error MAE:  372.8432337535752\n",
      "Epoch:  7 Loss:  0.9679718091246787 Error:  0.13984059575903282\n",
      "Test error MAE:  372.83922985692817\n",
      "Epoch:  8 Loss:  0.917169061269653 Error:  0.1387727681672975\n",
      "Test error MAE:  372.8376896282037\n",
      "Epoch:  9 Loss:  0.8739230063524139 Error:  0.1381598791379607\n",
      "Test error MAE:  372.83738103674517\n",
      "Epoch:  10 Loss:  0.8450608414210631 Error:  0.13723236732603458\n",
      "Test error MAE:  372.83630076845486\n",
      "Epoch:  11 Loss:  0.8203199519200272 Error:  0.1369910239336196\n",
      "Test error MAE:  372.836732068989\n",
      "Epoch:  12 Loss:  0.7849813793482405 Error:  0.1364645567000582\n",
      "Test error MAE:  372.8365072733826\n",
      "Epoch:  13 Loss:  0.7635080653629945 Error:  0.1361711836430464\n",
      "Test error MAE:  372.8349521958166\n",
      "Epoch:  14 Loss:  0.7429837591192695 Error:  0.13507984059580255\n",
      "Test error MAE:  372.83607939845984\n",
      "Epoch:  15 Loss:  0.7251070737838745 Error:  0.13465699762775657\n",
      "Test error MAE:  372.8349287039704\n",
      "Held out test score:\n",
      "Test error MAE:  0.1827182138667387\n",
      "------------------------------\n",
      "Epoch:  1 Loss:  125.10412945372335 Error:  124.51506208092644\n",
      "Test error MAE:  0.19247846934530471\n",
      "Epoch:  2 Loss:  125.08859642502966 Error:  124.51513311250156\n",
      "Test error MAE:  0.19257907569408417\n",
      "Epoch:  3 Loss:  125.0844716322556 Error:  124.51510346228822\n",
      "Test error MAE:  0.19188371433152093\n",
      "Epoch:  4 Loss:  125.07033991478802 Error:  124.51487995692518\n",
      "Test error MAE:  0.19266804489824507\n",
      "Epoch:  5 Loss:  125.0700266649214 Error:  124.51481202001987\n",
      "Test error MAE:  0.19265950587060715\n",
      "Epoch:  6 Loss:  125.06995362512181 Error:  124.51472525424167\n",
      "Test error MAE:  0.19234944217734867\n",
      "Epoch:  7 Loss:  125.05677190791356 Error:  124.51494460845932\n",
      "Test error MAE:  0.19277941783269245\n",
      "Epoch:  8 Loss:  125.05147099227048 Error:  124.51466706838835\n",
      "Test error MAE:  0.1925088945362303\n",
      "Epoch:  9 Loss:  125.06289888700742 Error:  124.51449266854632\n",
      "Test error MAE:  0.1922853085729811\n",
      "Epoch:  10 Loss:  125.04926411958223 Error:  124.51472862625725\n",
      "Test error MAE:  0.19296611448129017\n",
      "Epoch:  11 Loss:  125.0604834308785 Error:  124.51479655788856\n",
      "Test error MAE:  0.19303681684864893\n",
      "Epoch:  12 Loss:  125.06922382451175 Error:  124.51418681559937\n",
      "Test error MAE:  0.19311078157689837\n",
      "Epoch:  13 Loss:  125.06306584430544 Error:  124.51404232682472\n",
      "Test error MAE:  0.19230797092119853\n",
      "Epoch:  14 Loss:  125.05773321698221 Error:  124.51357388019227\n",
      "Test error MAE:  0.19144811133543652\n",
      "Epoch:  15 Loss:  125.06122455101335 Error:  124.51396719106798\n",
      "Test error MAE:  0.1911076198021571\n",
      "Held out test score:\n",
      "Test error MAE:  0.1815195035408525\n",
      "------------------------------\n",
      "Epoch:  1 Loss:  123.68603488935365 Error:  123.1350374020636\n",
      "Test error MAE:  0.18179964985359798\n",
      "Epoch:  2 Loss:  123.6935371922122 Error:  123.13482104697161\n",
      "Test error MAE:  0.1816241629421711\n",
      "Epoch:  3 Loss:  123.69563148617745 Error:  123.13469109079904\n",
      "Test error MAE:  0.1823949268595739\n",
      "Epoch:  4 Loss:  123.7012663245201 Error:  123.1349152005381\n",
      "Test error MAE:  0.18324260210449045\n",
      "Epoch:  5 Loss:  123.6873921016852 Error:  123.13438204808368\n",
      "Test error MAE:  0.1808942464942282\n",
      "Epoch:  6 Loss:  123.69595665600565 Error:  123.13351856867472\n",
      "Test error MAE:  0.18253868411887775\n",
      "Epoch:  7 Loss:  123.69385237230195 Error:  123.13328120062748\n"
     ]
    }
   ],
   "source": [
    "# Define sets of hyperparameters for tuning\n",
    "learning_rates = [0.0001, 0.0002]\n",
    "lambdas = [1000, 5000, 10000]\n",
    "hidden_dims = [256, 512, 1024]\n",
    "\n",
    "num_splt = 3\n",
    "epochs = 10\n",
    "# lr = 0.00005\n",
    "# lmbda = 5000\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "# hidden_dim = 512\n",
    "padding = 30\n",
    "ks = [0.7, 0.3]\n",
    "\n",
    "hyperparam_grid = {\n",
    "    'lr': learning_rates,\n",
    "    'lmbda': lambdas,\n",
    "    'hidden_dim': hidden_dims\n",
    "}\n",
    "\n",
    "best_hyperparams = None\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for hyperparams in ParameterGrid(hyperparam_grid):\n",
    "    print(f\"Training with hyperparams: {hyperparams}\")\n",
    "\n",
    "    train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "    \n",
    "    print('Held out test score:')\n",
    "    test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "    print('------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
