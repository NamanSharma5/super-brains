{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "import networkx as nx\n",
    "from typing import Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data])\n",
    "hr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data])\n",
    "\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "num_samples_list = range(num_samples)\n",
    "sample_to_index = dict(zip(num_samples_list, hr_train_data_vectorized))\n",
    "\n",
    "split = int(num_samples * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_adj,subjects_labels = lr_train_data_vectorized[:split], hr_train_data_vectorized[:split]\n",
    "\n",
    "held_out_subjects_adj,held_out_subjects_labels = lr_train_data_vectorized[split:], hr_train_data_vectorized[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 10\n",
    "lr = 0.00005\n",
    "lmbda = 25\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320\n",
    "padding = 26\n",
    "dropout = 0.1\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.7, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopologicalMeasures:\n",
    "    def __init__(self,graph:Union[np.ndarray,torch.Tensor]):\n",
    "        if isinstance(graph,np.ndarray):\n",
    "            self.graph = nx.Graph(graph)\n",
    "        elif isinstance(graph,torch.Tensor):\n",
    "            graph_numpy = graph.cpu().detach().numpy()\n",
    "            self.graph = nx.Graph(graph_numpy)\n",
    "\n",
    "    def compute_measures(self, number = 0):\n",
    "        measures = {}\n",
    "        measures['degree'] = torch.FloatTensor(list(dict(self.graph.degree()).values()))\n",
    "        # measures['clustering'] = torch.FloatTensor(list(nx.clustering(self.graph).values())) # removed due to slow computation\n",
    "        measures['closeness'] = torch.FloatTensor(list(nx.closeness_centrality(self.graph).values()))\n",
    "        # measures['betweenness'] = torch.FloatTensor(list(nx.betweenness_centrality(self.graph).values())) # removed due to slow computation\n",
    "        measures['pagerank'] = torch.FloatTensor(list(nx.pagerank(self.graph).values()))\n",
    "        measures['eigenvector'] = torch.FloatTensor(list(nx.eigenvector_centrality(self.graph).values()))\n",
    "        return measures\n",
    "\n",
    "def precompute_topological_measures(hr_train_data_vectorized:np.ndarray):\n",
    "    index_to_measure = {}\n",
    "    for index, graph in enumerate(hr_train_data_vectorized):\n",
    "        index_to_measure[index] = TopologicalMeasures(graph).compute_measures()\n",
    "        if index % 10 == 0:\n",
    "            print(f'Computed measures for {index} graphs')\n",
    "    return index_to_measure\n",
    "\n",
    "\n",
    "def compute_topological_MAE_loss(graph1,graph2:Union[np.ndarray,torch.Tensor], precomputed_g1: False):\n",
    "    if precomputed_g1:\n",
    "        measures1 = graph1\n",
    "    else:\n",
    "        measures1 = TopologicalMeasures(graph1).compute_measures()\n",
    "    measures2 = TopologicalMeasures(graph2).compute_measures()\n",
    "    loss = 0\n",
    "    # compute MAE for each measure\n",
    "\n",
    "    for measure in measures1:\n",
    "        loss += F.l1_loss(measures1[measure], measures2[measure])\n",
    "    loss = loss/len(measures1)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed measures for 0 graphs\n",
      "Computed measures for 10 graphs\n",
      "Computed measures for 20 graphs\n",
      "Computed measures for 30 graphs\n",
      "Computed measures for 40 graphs\n",
      "Computed measures for 50 graphs\n",
      "Computed measures for 60 graphs\n",
      "Computed measures for 70 graphs\n",
      "Computed measures for 80 graphs\n",
      "Computed measures for 90 graphs\n",
      "Computed measures for 100 graphs\n",
      "Computed measures for 110 graphs\n",
      "Computed measures for 120 graphs\n",
      "Computed measures for 130 graphs\n",
      "Computed measures for 140 graphs\n",
      "Computed measures for 150 graphs\n",
      "Computed measures for 160 graphs\n"
     ]
    }
   ],
   "source": [
    "# precompute topological measures for hr_train_data_vectorized\n",
    "precomputed_measures = precompute_topological_measures(hr_train_data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj,subjects_labels, args): \n",
    "  #, subjects_adj_test, subjects_ground_truth_test):\n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "    epoch_topo = []\n",
    "\n",
    "    model.train()\n",
    "    for index,(lr,hr) in enumerate(zip(subjects_adj,subjects_labels)):      \n",
    "      lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "      hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "      \n",
    "      # weights = unpad(model.layer.weights, args.padding)\n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      # loss = criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + args.lmbda * criterion(model_outputs, hr) \n",
    "      # loss = criterion(model_outputs, hr) \n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "      start_time = time.time()\n",
    "\n",
    "      # topo = compute_topological_MAE_loss(hr, model_outputs, precomputed_g1 = False)\n",
    "      topo = compute_topological_MAE_loss(precomputed_measures[index], model_outputs, precomputed_g1 = True)\n",
    "      # print(\"Time to compute topo: \", time.time() - start_time)\n",
    "      # print(topo.item())\n",
    "\n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      epoch_topo.append(topo.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    # print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error))\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error), \"Topo: \", np.mean(epoch_topo))\n",
    "    # test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "    # test(model, subjects_adj_test, subjects_ground_truth_test, args)\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# for train_index, test_index in cv.split(subjects_adj):\n",
    "#     subjects_adj_train = subjects_adj[train_index]  # Get training data \n",
    "#     subjects_adj_test = subjects_adj[test_index]   # Get testing data \n",
    "#     subjects_ground_truth_train = subjects_labels[train_index]\n",
    "#     subjects_ground_truth_test = subjects_labels[test_index]\n",
    "\n",
    "#     train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "    \n",
    "#     print('Held out test score:')\n",
    "#     test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "#     print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.5797495531464765 Error:  0.23944988998467337 Topo:  13.371520245146609\n",
      "Epoch:  2 Loss:  0.381369464947078 Error:  0.21050367169751377 Topo:  12.77267477755061\n",
      "Epoch:  3 Loss:  0.3578568504242126 Error:  0.1977207723135006 Topo:  12.793938816664461\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m final_model \u001b[38;5;241m=\u001b[39m GSRNet(ks, args)\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(final_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_train_data_vectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhr_train_data_vectorized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[147], line 34\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, subjects_adj, subjects_labels, args)\u001b[0m\n\u001b[1;32m     31\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# topo = compute_topological_MAE_loss(hr, model_outputs, precomputed_g1 = False)\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m topo \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_topological_MAE_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecomputed_measures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecomputed_g1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(\"Time to compute topo: \", time.time() - start_time)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# print(topo.item())\u001b[39;00m\n\u001b[1;32m     38\u001b[0m error \u001b[38;5;241m=\u001b[39m criterion(model_outputs, hr)\n",
      "Cell \u001b[0;32mIn[136], line 33\u001b[0m, in \u001b[0;36mcompute_topological_MAE_loss\u001b[0;34m(graph1, graph2, precomputed_g1)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     measures1 \u001b[38;5;241m=\u001b[39m TopologicalMeasures(graph1)\u001b[38;5;241m.\u001b[39mcompute_measures()\n\u001b[0;32m---> 33\u001b[0m measures2 \u001b[38;5;241m=\u001b[39m \u001b[43mTopologicalMeasures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcompute_measures()\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# compute MAE for each measure\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[136], line 7\u001b[0m, in \u001b[0;36mTopologicalMeasures.__init__\u001b[0;34m(self, graph)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(graph,torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m      6\u001b[0m     graph_numpy \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph_numpy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/y4/DGL/super-brains/.venv/lib/python3.10/site-packages/networkx/classes/graph.py:370\u001b[0m, in \u001b[0;36mGraph.__init__\u001b[0;34m(self, incoming_graph_data, **attr)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m# attempt to load graph with data\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incoming_graph_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[43mconvert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_networkx_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mincoming_graph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# load graph attributes (must be after convert)\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(attr)\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/y4/DGL/super-brains/.venv/lib/python3.10/site-packages/networkx/convert.py:144\u001b[0m, in \u001b[0;36mto_networkx_graph\u001b[0;34m(data, create_using, multigraph_input)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_using\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_using\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mNetworkXError(\n\u001b[1;32m    147\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to interpret array as an adjacency matrix.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/y4/DGL/super-brains/.venv/lib/python3.10/site-packages/networkx/utils/backends.py:412\u001b[0m, in \u001b[0;36m_dispatch.__call__\u001b[0;34m(self, backend, *args, **kwargs)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m/\u001b[39m, \u001b[38;5;241m*\u001b[39margs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m backends:\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;66;03m# Fast path if no backends are installed\u001b[39;00m\n\u001b[0;32m--> 412\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morig_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001b[39;00m\n\u001b[1;32m    415\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m backend\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/y4/DGL/super-brains/.venv/lib/python3.10/site-packages/networkx/convert_matrix.py:1199\u001b[0m, in \u001b[0;36mfrom_numpy_array\u001b[0;34m(A, parallel_edges, create_using, edge_attr)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_multigraph() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m G\u001b[38;5;241m.\u001b[39mis_directed():\n\u001b[1;32m   1198\u001b[0m     triples \u001b[38;5;241m=\u001b[39m ((u, v, d) \u001b[38;5;28;01mfor\u001b[39;00m u, v, d \u001b[38;5;129;01min\u001b[39;00m triples \u001b[38;5;28;01mif\u001b[39;00m u \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m v)\n\u001b[0;32m-> 1199\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_edges_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtriples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m/vol/biomedic3/bglocker/ugproj2324/nns20/y4/DGL/super-brains/.venv/lib/python3.10/site-packages/networkx/classes/graph.py:1039\u001b[0m, in \u001b[0;36mGraph.add_edges_from\u001b[0;34m(self, ebunch_to_add, **attr)\u001b[0m\n\u001b[1;32m   1037\u001b[0m datadict\u001b[38;5;241m.\u001b[39mupdate(dd)\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[u][v] \u001b[38;5;241m=\u001b[39m datadict\n\u001b[0;32m-> 1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj[v][u] \u001b[38;5;241m=\u001b[39m datadict\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "train(final_model, optimizer, lr_train_data_vectorized, hr_train_data_vectorized, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 12720)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "#Generate submission \n",
    "\n",
    "# load csvs as numpy\n",
    "test_lr_data_path = '../data/lr_test.csv'\n",
    "\n",
    "# lr_test_data = np.loadtxt(test_lr_data_path, delimiter=',')\n",
    "lr_test_data = pd.read_csv(test_lr_data_path, delimiter=',').to_numpy()\n",
    "print(lr_test_data.shape)\n",
    "lr_test_data[lr_test_data < 0] = 0\n",
    "np.nan_to_num(lr_test_data, copy=False)\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "print(lr_test_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 (35778,)\n",
      "(4007136,)\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "preds = []\n",
    "for lr in lr_test_data_vectorized:      \n",
    "  lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "  \n",
    "  model_outputs, _, _, _ = final_model(lr)\n",
    "  model_outputs  = unpad(model_outputs, args.padding)\n",
    "  preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "print(len(preds), preds[0].shape)\n",
    "r = np.hstack(preds)\n",
    "print(r.shape)\n",
    "meltedDF = r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = meltedDF.shape[0]\n",
    "df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                   'Predicted': meltedDF})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
