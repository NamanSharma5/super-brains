{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "import networkx as nx\n",
    "from typing import Union\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA (GPU support) and set device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    torch.cuda.manual_seed_all(random_seed)  # For multi-GPU setups\n",
    "    # Additional settings for ensuring reproducibility on CUDA\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data])\n",
    "hr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data])\n",
    "\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "num_samples_list = range(num_samples)\n",
    "sample_to_index = dict(zip(num_samples_list, hr_train_data_vectorized))\n",
    "\n",
    "split = int(num_samples * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 160, 160]) torch.Size([1, 268, 268])\n"
     ]
    }
   ],
   "source": [
    "lr_train_data_vectorized = torch.tensor(lr_train_data_vectorized, dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor(hr_train_data_vectorized, dtype=torch.float32)\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NoisyDataset(Dataset):\n",
    "    def __init__(self, lr_data, hr_data, noise_level=0.01):\n",
    "        \"\"\"\n",
    "        lr_data: Low resolution data (torch.tensor)\n",
    "        hr_data: High resolution data (torch.tensor)\n",
    "        noise_level: Standard deviation of Gaussian noise to be added\n",
    "        \"\"\"\n",
    "        self.lr_data = lr_data\n",
    "        self.hr_data = hr_data\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lr_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lr_sample = self.lr_data[idx]\n",
    "        hr_sample = self.hr_data[idx]\n",
    "\n",
    "        # Adding Gaussian noise\n",
    "        noise = torch.randn(lr_sample.size()) * self.noise_level\n",
    "        noisy_lr_sample = lr_sample + noise\n",
    "\n",
    "        # Clipping to ensure values are between 0 and 1\n",
    "        noisy_lr_sample = torch.clamp(noisy_lr_sample, 0, 1)\n",
    "\n",
    "        return noisy_lr_sample, hr_sample\n",
    "\n",
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)\n",
    "sample = next(iter(train_data_loader))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_adj,subjects_labels = lr_train_data_vectorized[:split], hr_train_data_vectorized[:split]\n",
    "\n",
    "held_out_subjects_adj,held_out_subjects_labels = lr_train_data_vectorized[split:], hr_train_data_vectorized[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 200\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 1 # should be around 0.5-1.5\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precompute topological measures for hr_train_data_vectorized\n",
    "# precomputed_measures = precompute_topological_measures(hr_train_data_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj,subjects_labels, args): \n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "    epoch_topo = []\n",
    "\n",
    "    model.train()\n",
    "    for lr, hr in train_data_loader:      \n",
    "      lr = lr.reshape(160, 160)\n",
    "      hr = hr.reshape(268, 268)\n",
    "\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "      start_time = time.time()\n",
    "\n",
    "      topo = args.lamdba_topo * compute_topological_MAE_loss(hr, model_outputs)\n",
    "\n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      epoch_topo.append(topo.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error),\n",
    "          \"Topo: \", np.mean(epoch_topo))\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# for train_index, test_index in cv.split(subjects_adj):\n",
    "#     subjects_adj_train = subjects_adj[train_index]  # Get training data \n",
    "#     subjects_adj_test = subjects_adj[test_index]   # Get testing data \n",
    "#     subjects_ground_truth_train = subjects_labels[train_index]\n",
    "#     subjects_ground_truth_test = subjects_labels[test_index]\n",
    "\n",
    "#     train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "    \n",
    "#     print('Held out test score:')\n",
    "#     test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "#     print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.43777106746941985 Error:  0.21379898550981533 Topo:  39.7049633757083\n",
      "Epoch:  2 Loss:  0.3004957930056635 Error:  0.1823972540344307 Topo:  21.88477148410089\n",
      "Epoch:  3 Loss:  0.29328333242924626 Error:  0.1786811695305887 Topo:  20.170655073519953\n",
      "Epoch:  4 Loss:  0.2865609928162512 Error:  0.1766832286190844 Topo:  19.415003867920287\n",
      "Epoch:  5 Loss:  0.28116676139974306 Error:  0.17528427307477254 Topo:  19.27946554972026\n",
      "Epoch:  6 Loss:  0.27575906495491187 Error:  0.17429556371922977 Topo:  19.075980597627378\n",
      "Epoch:  7 Loss:  0.2711637485883907 Error:  0.17383947932791566 Topo:  19.383889797918812\n",
      "Epoch:  8 Loss:  0.2664204063172826 Error:  0.17273907202803446 Topo:  19.05367251641736\n",
      "Epoch:  9 Loss:  0.2615383422660257 Error:  0.17154153048278328 Topo:  18.853358879774632\n",
      "Epoch:  10 Loss:  0.2570333658399696 Error:  0.17061601757646322 Topo:  18.913820906313592\n",
      "Epoch:  11 Loss:  0.2534014636171078 Error:  0.1698098043481747 Topo:  19.18066168116952\n",
      "Epoch:  12 Loss:  0.2497615837408397 Error:  0.16869330513263178 Topo:  18.995811439559844\n",
      "Epoch:  13 Loss:  0.24597818069829197 Error:  0.1673912799465442 Topo:  18.995263031142915\n",
      "Epoch:  14 Loss:  0.24292618875018138 Error:  0.16637844837711244 Topo:  19.027357118572304\n",
      "Epoch:  15 Loss:  0.24001471591209936 Error:  0.16517842413779504 Topo:  18.94682152685291\n",
      "Epoch:  16 Loss:  0.23726832991588614 Error:  0.16401802119380698 Topo:  18.944116186952876\n",
      "Epoch:  17 Loss:  0.23499706896122344 Error:  0.16303208184813311 Topo:  18.882192817276824\n",
      "Epoch:  18 Loss:  0.23292285516233502 Error:  0.16196817579026707 Topo:  18.79452898402414\n",
      "Epoch:  19 Loss:  0.23091633956946298 Error:  0.1610118091820243 Topo:  18.798014161115635\n",
      "Epoch:  20 Loss:  0.22921996041686235 Error:  0.16011384147369934 Topo:  18.819404162332685\n",
      "Epoch:  21 Loss:  0.2281379435590641 Error:  0.15962121399219878 Topo:  18.990284228753186\n",
      "Epoch:  22 Loss:  0.2263202956336701 Error:  0.15851968401920297 Topo:  18.742446802333443\n",
      "Epoch:  23 Loss:  0.2249584066832137 Error:  0.15779838340725014 Topo:  18.727160864961363\n",
      "Epoch:  24 Loss:  0.2239336819348935 Error:  0.157139760291505 Topo:  18.734674716424085\n",
      "Epoch:  25 Loss:  0.22303113814242587 Error:  0.1567486689654653 Topo:  18.72570357065715\n",
      "Epoch:  26 Loss:  0.22164489025484302 Error:  0.15588810165485223 Topo:  18.599028147623212\n",
      "Epoch:  27 Loss:  0.22107598301536308 Error:  0.1556741529239152 Topo:  18.687909246204857\n",
      "Epoch:  28 Loss:  0.22035948129114277 Error:  0.15527514836745349 Topo:  18.762281143736697\n",
      "Epoch:  29 Loss:  0.219756701749242 Error:  0.15482070435306983 Topo:  18.69772731187101\n",
      "Epoch:  30 Loss:  0.21891981568522081 Error:  0.1544268532784399 Topo:  18.694673361178644\n",
      "Epoch:  31 Loss:  0.21808487275046504 Error:  0.15375484401237466 Topo:  18.52115998867743\n",
      "Epoch:  32 Loss:  0.2175787365186714 Error:  0.15369695380419315 Topo:  18.722108366960537\n",
      "Epoch:  33 Loss:  0.2173344337654685 Error:  0.15362693485385642 Topo:  18.713441037846184\n",
      "Epoch:  34 Loss:  0.21639950127301816 Error:  0.15314240869647727 Topo:  18.612500304947357\n",
      "Epoch:  35 Loss:  0.21585652794309718 Error:  0.15281283007767385 Topo:  18.66602122712278\n",
      "Epoch:  36 Loss:  0.21531430538186055 Error:  0.15240359569559553 Topo:  18.435679298674987\n",
      "Epoch:  37 Loss:  0.21511258586438117 Error:  0.15257700626364726 Topo:  18.625607262114563\n",
      "Epoch:  38 Loss:  0.21434399922807773 Error:  0.15203955591081858 Topo:  18.51072242302809\n",
      "Epoch:  39 Loss:  0.2135886065438836 Error:  0.15177203433777758 Topo:  18.564152928883445\n",
      "Epoch:  40 Loss:  0.2134519655725913 Error:  0.15177706343506625 Topo:  18.54801738761856\n",
      "Epoch:  41 Loss:  0.2132834186275562 Error:  0.1517920102812573 Topo:  18.589911506561464\n",
      "Epoch:  42 Loss:  0.21272706298414104 Error:  0.1514393037664676 Topo:  18.62595660386685\n",
      "Epoch:  43 Loss:  0.2123655468047022 Error:  0.15131957365010312 Topo:  18.529647838569687\n",
      "Epoch:  44 Loss:  0.21183151354689797 Error:  0.15112426881483215 Topo:  18.56554987901699\n",
      "Epoch:  45 Loss:  0.21158218169640639 Error:  0.15110521522646178 Topo:  18.485320753680018\n",
      "Epoch:  46 Loss:  0.21079546596832618 Error:  0.1507905165324668 Topo:  18.44341548188718\n",
      "Epoch:  47 Loss:  0.2107989797870556 Error:  0.15082438829951658 Topo:  18.515519953059577\n",
      "Epoch:  48 Loss:  0.2107259742692559 Error:  0.15090525355524645 Topo:  18.527992174297037\n",
      "Epoch:  49 Loss:  0.21016330669026176 Error:  0.150641368088608 Topo:  18.537908811055257\n",
      "Epoch:  50 Loss:  0.2100585046821012 Error:  0.15063012409174514 Topo:  18.59213329931933\n",
      "Epoch:  51 Loss:  0.20948774746792045 Error:  0.15033061191171943 Topo:  18.4674670681982\n",
      "Epoch:  52 Loss:  0.20919711557690968 Error:  0.15044672894263697 Topo:  18.622385127815658\n",
      "Epoch:  53 Loss:  0.20908446404748335 Error:  0.15035169491332448 Topo:  18.538180328414825\n",
      "Epoch:  54 Loss:  0.2089443697543915 Error:  0.15041611771919056 Topo:  18.623200987627406\n",
      "Epoch:  55 Loss:  0.2084103956550895 Error:  0.1501374616326686 Topo:  18.52496035798581\n",
      "Epoch:  56 Loss:  0.20795950231080998 Error:  0.14990545925266013 Topo:  18.4644454282201\n",
      "Epoch:  57 Loss:  0.20798728226901528 Error:  0.1501419388962363 Topo:  18.466688338867918\n",
      "Epoch:  58 Loss:  0.2075770061886953 Error:  0.15006240136073734 Topo:  18.57205187631938\n",
      "Epoch:  59 Loss:  0.20700368615324627 Error:  0.1496004639360719 Topo:  18.275546702082284\n",
      "Epoch:  60 Loss:  0.20695200580322815 Error:  0.14976767160578403 Topo:  18.449849288620634\n",
      "Epoch:  61 Loss:  0.2065421913019911 Error:  0.14954792694774216 Topo:  18.396255510295937\n",
      "Epoch:  62 Loss:  0.2068013458730218 Error:  0.1499444697342233 Topo:  18.569762812403148\n",
      "Epoch:  63 Loss:  0.20620828768807256 Error:  0.14953923162942875 Topo:  18.42337306816421\n",
      "Epoch:  64 Loss:  0.20623733576186404 Error:  0.14970484367030823 Topo:  18.38277735681591\n",
      "Epoch:  65 Loss:  0.20601821427573702 Error:  0.14974839927372105 Topo:  18.554515747253053\n",
      "Epoch:  66 Loss:  0.20558527834758072 Error:  0.14936617494164828 Topo:  18.435132472101085\n",
      "Epoch:  67 Loss:  0.2053228549793095 Error:  0.14941143820028818 Topo:  18.470167291378548\n",
      "Epoch:  68 Loss:  0.20501263279043985 Error:  0.14916068717034278 Topo:  18.3544812288113\n",
      "Epoch:  69 Loss:  0.2047643110959116 Error:  0.1490734479206051 Topo:  18.354087104340515\n",
      "Epoch:  70 Loss:  0.20491129293770133 Error:  0.14933266802997647 Topo:  18.50096328529769\n",
      "Epoch:  71 Loss:  0.2043935591410734 Error:  0.14913051191739693 Topo:  18.45880532407475\n",
      "Epoch:  72 Loss:  0.20423894460329753 Error:  0.1491939404499745 Topo:  18.4202034544802\n",
      "Epoch:  73 Loss:  0.20397945819143765 Error:  0.14888114905999805 Topo:  18.27120742683639\n",
      "Epoch:  74 Loss:  0.20364957253733082 Error:  0.14897879217556137 Topo:  18.36350499798438\n",
      "Epoch:  75 Loss:  0.20364590970698945 Error:  0.14908067507301262 Topo:  18.4773022428958\n",
      "Epoch:  76 Loss:  0.20343591121142496 Error:  0.14896644883883928 Topo:  18.426632344365835\n",
      "Epoch:  77 Loss:  0.20320416201731403 Error:  0.14906415976807028 Topo:  18.44867773684199\n",
      "Epoch:  78 Loss:  0.2031566067905483 Error:  0.14890425893182527 Topo:  18.421230093447747\n",
      "Epoch:  79 Loss:  0.2029330154914342 Error:  0.14899400809032473 Topo:  18.477887736109203\n",
      "Epoch:  80 Loss:  0.20277516026339845 Error:  0.14886323844422839 Topo:  18.37532507730815\n",
      "Epoch:  81 Loss:  0.20287490676263134 Error:  0.14899443982247107 Topo:  18.438190003355107\n",
      "Epoch:  82 Loss:  0.2023455403343646 Error:  0.148709939134692 Topo:  18.351148708137924\n",
      "Epoch:  83 Loss:  0.2022433129256357 Error:  0.148769008140721 Topo:  18.468579954729822\n",
      "Epoch:  84 Loss:  0.20205666035592199 Error:  0.14874284848303138 Topo:  18.381506485853368\n",
      "Epoch:  85 Loss:  0.20193045305277774 Error:  0.14859529241116462 Topo:  18.36763707463613\n",
      "Epoch:  86 Loss:  0.20194264175649174 Error:  0.1487976549985166 Topo:  18.3940735263025\n",
      "Epoch:  87 Loss:  0.2018627828645135 Error:  0.14890470074679324 Topo:  18.43819643351846\n",
      "Epoch:  88 Loss:  0.20152064863436236 Error:  0.14861824809612628 Topo:  18.367493663719316\n",
      "Epoch:  89 Loss:  0.2015627497684456 Error:  0.148744252582867 Topo:  18.490391188752866\n",
      "Epoch:  90 Loss:  0.20139599096275376 Error:  0.14873863033905715 Topo:  18.411767320004767\n",
      "Epoch:  91 Loss:  0.2008796038920294 Error:  0.14846674727644035 Topo:  18.33608309237543\n",
      "Epoch:  92 Loss:  0.20079723857120127 Error:  0.14856626039850498 Topo:  18.39523658638229\n",
      "Epoch:  93 Loss:  0.2007269588951579 Error:  0.14857671396461075 Topo:  18.41292165996072\n",
      "Epoch:  94 Loss:  0.20074688424607237 Error:  0.14867311992688093 Topo:  18.481990517256502\n",
      "Epoch:  95 Loss:  0.20047006278694746 Error:  0.14841127061022968 Topo:  18.334301143349286\n",
      "Epoch:  96 Loss:  0.20068732216329632 Error:  0.14866390257716894 Topo:  18.487297457849195\n",
      "Epoch:  97 Loss:  0.20027607251070217 Error:  0.14846657595770088 Topo:  18.401048557487076\n",
      "Epoch:  98 Loss:  0.19990811608508677 Error:  0.1483155754422713 Topo:  18.4109252998215\n",
      "Epoch:  99 Loss:  0.1997335487140153 Error:  0.14824705383556333 Topo:  18.358347190354397\n",
      "Epoch:  100 Loss:  0.19968984806965925 Error:  0.14830180525244352 Topo:  18.344726294100642\n",
      "Epoch:  101 Loss:  0.19961403238916112 Error:  0.14833042114794612 Topo:  18.414077050671604\n",
      "Epoch:  102 Loss:  0.19949713153039625 Error:  0.14830849462462042 Topo:  18.389569385322982\n",
      "Epoch:  103 Loss:  0.199704717376275 Error:  0.14853054396585078 Topo:  18.4410725667805\n",
      "Epoch:  104 Loss:  0.19930289435886336 Error:  0.1482648173135198 Topo:  18.376495532646864\n",
      "Epoch:  105 Loss:  0.1990087258958531 Error:  0.14806296564861685 Topo:  18.31354130956227\n",
      "Epoch:  106 Loss:  0.19901119770404108 Error:  0.14839143591548154 Topo:  18.44975858676933\n",
      "Epoch:  107 Loss:  0.1988098127042462 Error:  0.14801308985598788 Topo:  18.256699190882152\n",
      "Epoch:  108 Loss:  0.19894411901157058 Error:  0.14832917993475578 Topo:  18.41159266054987\n",
      "Epoch:  109 Loss:  0.19868126440190984 Error:  0.1483117390178635 Topo:  18.412805962705328\n",
      "Epoch:  110 Loss:  0.19858224118898016 Error:  0.14815754626325506 Topo:  18.38120170981584\n",
      "Epoch:  111 Loss:  0.1983174970763886 Error:  0.14806504989277103 Topo:  18.31867022143153\n",
      "Epoch:  112 Loss:  0.1984920693550281 Error:  0.14829907245086338 Topo:  18.462527726224796\n",
      "Epoch:  113 Loss:  0.19814516773480856 Error:  0.1480911572625537 Topo:  18.336486148263166\n",
      "Epoch:  114 Loss:  0.1980622333145427 Error:  0.14808949315083955 Topo:  18.401413552061527\n",
      "Epoch:  115 Loss:  0.19789069284222083 Error:  0.14806343302755298 Topo:  18.32124300060158\n",
      "Epoch:  116 Loss:  0.19776523389859113 Error:  0.1479685673278249 Topo:  18.364443036610496\n",
      "Epoch:  117 Loss:  0.19800440155103535 Error:  0.14823847014211608 Topo:  18.386662414687837\n",
      "Epoch:  118 Loss:  0.19778333363418807 Error:  0.14805083672800465 Topo:  18.369275498532964\n",
      "Epoch:  119 Loss:  0.19737801999745969 Error:  0.14796489039937893 Topo:  18.345807526639835\n",
      "Epoch:  120 Loss:  0.1972632179717104 Error:  0.1479575387285855 Topo:  18.314253481562265\n",
      "Epoch:  121 Loss:  0.1974279329983774 Error:  0.14801207697855498 Topo:  18.288553797556254\n",
      "Epoch:  122 Loss:  0.19716450991387852 Error:  0.14790886680701534 Topo:  18.325814007285114\n",
      "Epoch:  123 Loss:  0.19730659052283464 Error:  0.14821975990505276 Topo:  18.46476271623623\n",
      "Epoch:  124 Loss:  0.1970759084124765 Error:  0.14789342902556152 Topo:  18.35496270870734\n",
      "Epoch:  125 Loss:  0.19680011852415735 Error:  0.1480139267301845 Topo:  18.375045033985984\n",
      "Epoch:  126 Loss:  0.19710817005106074 Error:  0.14815524746914824 Topo:  18.490846119954913\n",
      "Epoch:  127 Loss:  0.1966593887456163 Error:  0.14782364099860906 Topo:  18.32979944651712\n",
      "Epoch:  128 Loss:  0.19676361326685923 Error:  0.1481063249046931 Topo:  18.361972077877937\n",
      "Epoch:  129 Loss:  0.19669300398069942 Error:  0.14804818311672724 Topo:  18.347991652117518\n",
      "Epoch:  130 Loss:  0.19646859115469242 Error:  0.1478961830189128 Topo:  18.330419900174626\n",
      "Epoch:  131 Loss:  0.1964474627357757 Error:  0.1480023394712431 Topo:  18.359592420612266\n",
      "Epoch:  132 Loss:  0.19647929866513805 Error:  0.14793234456798987 Topo:  18.37429417250399\n",
      "Epoch:  133 Loss:  0.19617318651990262 Error:  0.14786602543321198 Topo:  18.33494481069599\n",
      "Epoch:  134 Loss:  0.1961735346895492 Error:  0.14800274474713618 Topo:  18.424278915999178\n",
      "Epoch:  135 Loss:  0.19609172080091375 Error:  0.14789252505152525 Topo:  18.346533375585864\n",
      "Epoch:  136 Loss:  0.19575572665223104 Error:  0.14766085170164794 Topo:  18.32085787607524\n",
      "Epoch:  137 Loss:  0.19600612576493245 Error:  0.14791110889640396 Topo:  18.40601584725751\n",
      "Epoch:  138 Loss:  0.19583621290986408 Error:  0.14788894774671085 Topo:  18.273734304005515\n",
      "Epoch:  139 Loss:  0.19578969808752666 Error:  0.14791390531791185 Topo:  18.333178411700768\n",
      "Epoch:  140 Loss:  0.19566681909703923 Error:  0.14772548165150032 Topo:  18.360967310602792\n",
      "Epoch:  141 Loss:  0.1953847024612084 Error:  0.14771330503824942 Topo:  18.310489414694782\n",
      "Epoch:  142 Loss:  0.19530936495629614 Error:  0.14757332005008253 Topo:  18.25095227521337\n",
      "Epoch:  143 Loss:  0.19548220263269847 Error:  0.1478422422876615 Topo:  18.372792974917473\n",
      "Epoch:  144 Loss:  0.19516760209006465 Error:  0.14764473899574337 Topo:  18.259646324340455\n",
      "Epoch:  145 Loss:  0.19537984432574518 Error:  0.1479047898671584 Topo:  18.45846012823596\n",
      "Epoch:  146 Loss:  0.19505925895925053 Error:  0.14764566469692184 Topo:  18.286975420877607\n",
      "Epoch:  147 Loss:  0.1951702372756547 Error:  0.1478204312706422 Topo:  18.326431594209044\n",
      "Epoch:  148 Loss:  0.19518400443171313 Error:  0.14787628291966673 Topo:  18.33447330726121\n",
      "Epoch:  149 Loss:  0.19491805198663723 Error:  0.14771604827956525 Topo:  18.375410034271056\n",
      "Epoch:  150 Loss:  0.1946735716686991 Error:  0.14755706721080278 Topo:  18.283692160052453\n",
      "Epoch:  151 Loss:  0.194931549137224 Error:  0.14771937538763721 Topo:  18.329524245804656\n",
      "Epoch:  152 Loss:  0.19496845217522032 Error:  0.1479690598692009 Topo:  18.430824131308917\n",
      "Epoch:  153 Loss:  0.19470224748114626 Error:  0.147689887640362 Topo:  18.315710387544005\n",
      "Epoch:  154 Loss:  0.1948450409366699 Error:  0.14787929456034107 Topo:  18.39539860251421\n",
      "Epoch:  155 Loss:  0.19448659751943487 Error:  0.14777897735555728 Topo:  18.340112497706613\n",
      "Epoch:  156 Loss:  0.19464428312407284 Error:  0.1476960037639755 Topo:  18.371473135348566\n",
      "Epoch:  157 Loss:  0.19445781757731637 Error:  0.14775864284730958 Topo:  18.424466315857664\n",
      "Epoch:  158 Loss:  0.1943426804985115 Error:  0.14767708201072888 Topo:  18.297012346233437\n",
      "Epoch:  159 Loss:  0.19431024209824865 Error:  0.1476509479170074 Topo:  18.335777653905446\n",
      "Epoch:  160 Loss:  0.19421255427920175 Error:  0.1477380351213638 Topo:  18.33322566711974\n",
      "Epoch:  161 Loss:  0.19422273525220904 Error:  0.14767955877109915 Topo:  18.34604771551258\n",
      "Epoch:  162 Loss:  0.19435111259271998 Error:  0.14786239633124745 Topo:  18.44793192926281\n",
      "Epoch:  163 Loss:  0.1938157830231204 Error:  0.14749092751455878 Topo:  18.31267646972291\n",
      "Epoch:  164 Loss:  0.19384296532876477 Error:  0.14740202076242356 Topo:  18.30439266615999\n",
      "Epoch:  165 Loss:  0.1939527057780477 Error:  0.1475970977526939 Topo:  18.308917582391977\n",
      "Epoch:  166 Loss:  0.1938717024411984 Error:  0.1476595008712329 Topo:  18.342805182862424\n",
      "Epoch:  167 Loss:  0.19386877190923976 Error:  0.14763594048465797 Topo:  18.335575138023515\n",
      "Epoch:  168 Loss:  0.19382075563876214 Error:  0.14763630605386402 Topo:  18.364825128795143\n",
      "Epoch:  169 Loss:  0.19373908415882887 Error:  0.14762725368766727 Topo:  18.29403084052537\n",
      "Epoch:  170 Loss:  0.1938232035337094 Error:  0.1478082150399328 Topo:  18.489989674733785\n",
      "Epoch:  171 Loss:  0.19358758146534422 Error:  0.1475863231424086 Topo:  18.3109529803613\n",
      "Epoch:  172 Loss:  0.19348503042480902 Error:  0.14753964797643845 Topo:  18.275373744393537\n",
      "Epoch:  173 Loss:  0.19356216766877088 Error:  0.14763744527886727 Topo:  18.359831490202577\n",
      "Epoch:  174 Loss:  0.19347173600139733 Error:  0.1475658984687514 Topo:  18.330004440810153\n",
      "Epoch:  175 Loss:  0.19315297937321807 Error:  0.14742680746102763 Topo:  18.25879119827362\n",
      "Epoch:  176 Loss:  0.19333776314101533 Error:  0.14757379737799753 Topo:  18.30698355943143\n",
      "Epoch:  177 Loss:  0.19340383033909483 Error:  0.14768532320053993 Topo:  18.37519296771752\n",
      "Epoch:  178 Loss:  0.19322024981775685 Error:  0.14746074601561723 Topo:  18.3394881037181\n",
      "Epoch:  179 Loss:  0.19326794843473835 Error:  0.14763664793290063 Topo:  18.375233918606877\n",
      "Epoch:  180 Loss:  0.19312728718369307 Error:  0.1476191833258389 Topo:  18.368092759640632\n",
      "Epoch:  181 Loss:  0.19290698698894707 Error:  0.14738871295473532 Topo:  18.315427780151367\n",
      "Epoch:  182 Loss:  0.1929291870422706 Error:  0.14751389042702978 Topo:  18.268130074004212\n",
      "Epoch:  183 Loss:  0.1927545287473473 Error:  0.14725289431339253 Topo:  18.206321990418576\n",
      "Epoch:  184 Loss:  0.19310040896881125 Error:  0.14760861135350017 Topo:  18.346498740647366\n",
      "Epoch:  185 Loss:  0.19293567805946943 Error:  0.14759177464746429 Topo:  18.375815979734867\n",
      "Epoch:  186 Loss:  0.19286269993482236 Error:  0.1475438059357826 Topo:  18.321598447011617\n",
      "Epoch:  187 Loss:  0.19291246553024133 Error:  0.14765356057239865 Topo:  18.342158940024003\n",
      "Epoch:  188 Loss:  0.19308721242907517 Error:  0.14768760749501383 Topo:  18.420201261600333\n",
      "Epoch:  189 Loss:  0.1926123531100279 Error:  0.14739092693714326 Topo:  18.245076031028155\n",
      "Epoch:  190 Loss:  0.19250414872954705 Error:  0.1473191375147083 Topo:  18.24577298992408\n",
      "Epoch:  191 Loss:  0.19268611305488084 Error:  0.1475236542121379 Topo:  18.325585028368558\n",
      "Epoch:  192 Loss:  0.1923778260539392 Error:  0.1473721189056328 Topo:  18.264819436444494\n",
      "Epoch:  193 Loss:  0.19238627153242419 Error:  0.1472653852340704 Topo:  18.241679237274354\n",
      "Epoch:  194 Loss:  0.192486492370417 Error:  0.14750917130958535 Topo:  18.33747901459654\n",
      "Epoch:  195 Loss:  0.19219873435126095 Error:  0.14724929099846742 Topo:  18.244817345442172\n",
      "Epoch:  196 Loss:  0.19242344903731776 Error:  0.14755716384527925 Topo:  18.38489534754953\n",
      "Epoch:  197 Loss:  0.19212593120372223 Error:  0.1472837018841755 Topo:  18.252688676297307\n",
      "Epoch:  198 Loss:  0.19226577351550142 Error:  0.14741596320789016 Topo:  18.310409346026574\n",
      "Epoch:  199 Loss:  0.19226568776690317 Error:  0.14737540764544538 Topo:  18.381159542563434\n",
      "Epoch:  200 Loss:  0.1925077409801369 Error:  0.14755731040310716 Topo:  18.366864872549822\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "final_model.to(device)\n",
    "lr_train_data_vectorized.to(device)\n",
    "hr_train_data_vectorized.to(device)\n",
    "\n",
    "train(final_model, optimizer, lr_train_data_vectorized, hr_train_data_vectorized, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 12720)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "#Generate submission \n",
    "\n",
    "# load csvs as numpy\n",
    "test_lr_data_path = '../data/lr_test.csv'\n",
    "\n",
    "lr_test_data = pd.read_csv(test_lr_data_path, delimiter=',').to_numpy()\n",
    "print(lr_test_data.shape)\n",
    "lr_test_data[lr_test_data < 0] = 0\n",
    "np.nan_to_num(lr_test_data, copy=False)\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "print(lr_test_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 (35778,)\n",
      "(4007136,)\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "preds = []\n",
    "for lr in lr_test_data_vectorized:      \n",
    "  lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "  \n",
    "  model_outputs, _, _, _ = final_model(lr)\n",
    "  model_outputs  = unpad(model_outputs, args.padding)\n",
    "  preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "print(len(preds), preds[0].shape)\n",
    "r = np.hstack(preds)\n",
    "print(r.shape)\n",
    "meltedDF = r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = meltedDF.shape[0]\n",
    "df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                   'Predicted': meltedDF})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
