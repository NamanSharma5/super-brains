{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from sklearn.model_selection import KFold\n",
    "import argparse\n",
    "from model import *\n",
    "from train import test\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from MatrixVectorizer import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33881717 0.20248584 0.03983874 ... 0.42020538 0.25529165 0.18337431]\n",
      " [0.63832894 0.11630679 0.89276219 ... 0.73026434 0.44675317 0.48986114]\n",
      " [0.62471606 0.02028873 0.2971744  ... 0.52366473 0.60326226 0.43792296]\n",
      " ...\n",
      " [0.29367774 0.54314711 0.17428709 ... 0.22557486 0.71399684 0.36689618]\n",
      " [0.60139502 0.77059429 0.60485866 ... 0.68879933 0.74031453 0.41991915]\n",
      " [0.50114943 0.27204258 0.3557178  ... 0.43947877 0.60193006 0.31666221]]\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = '../data/lr_train.csv'\n",
    "hr_data_path = '../data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data])\n",
    "hr_train_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data])\n",
    "num_samples = hr_train_data_vectorized.shape[0]\n",
    "split = int(num_samples * 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subjects_adj,subjects_labels = lr_train_data_vectorized[:split], hr_train_data_vectorized[:split]\n",
    "\n",
    "held_out_subjects_adj,held_out_subjects_labels = lr_train_data_vectorized[split:], hr_train_data_vectorized[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 200\n",
    "lr = 0.00005\n",
    "lmbda = 35\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320\n",
    "padding = 26\n",
    "dropout = 0.3\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()\n",
    "\n",
    "def train(model, optimizer, subjects_adj,subjects_labels, args): \n",
    "  #, subjects_adj_test, subjects_ground_truth_test):\n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "\n",
    "    model.train()\n",
    "    for lr,hr in zip(subjects_adj,subjects_labels):      \n",
    "      lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "      hr = torch.from_numpy(hr).type(torch.FloatTensor)\n",
    "      \n",
    "      \n",
    "      # net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "      # weights = unpad(model.layer.weights, args.padding)\n",
    "      \n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      eig_val_hr, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      # print the shapes of the outputs\n",
    "      # print(f\"{net_outs.shape} ; {start_gcn_outs.shape}\")\n",
    "      # print(f\"{model.layer.weights.shape} ; {U_hr.shape}\")\n",
    "      # print(f\"{model_outputs.shape} ; {hr.shape}\")\n",
    "      \n",
    "      # loss = criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + args.lmbda * criterion(model_outputs, hr) \n",
    "      # loss = criterion(model_outputs, hr) \n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "\n",
    "      \n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error))\n",
    "    # test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "    # test(model, subjects_adj_test, subjects_ground_truth_test, args)\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# train(model, optimizer, subjects_adj, subjects_labels, args)\n",
    "\n",
    "# print('Held out test score:')\n",
    "# test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "# print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(model)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# # optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "# for train_index, test_index in cv.split(subjects_adj):\n",
    "#     subjects_adj_train = subjects_adj[train_index]  # Get training data \n",
    "#     subjects_adj_test = subjects_adj[test_index]   # Get testing data \n",
    "#     subjects_ground_truth_train = subjects_labels[train_index]\n",
    "#     subjects_ground_truth_test = subjects_labels[test_index]\n",
    "\n",
    "#     train(model, optimizer, subjects_adj_train, subjects_ground_truth_train, args, subjects_adj_test, subjects_ground_truth_test)\n",
    "    \n",
    "#     print('Held out test score:')\n",
    "#     test(model, held_out_subjects_adj, held_out_subjects_labels, args)\n",
    "#     print('------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.6943827694404625 Error:  0.24107033380134377\n",
      "Epoch:  2 Loss:  0.42194930557719246 Error:  0.2097839743434312\n",
      "Epoch:  3 Loss:  0.39719308725374186 Error:  0.19953125242344633\n",
      "Epoch:  4 Loss:  0.3804442973550922 Error:  0.19597093252364747\n",
      "Epoch:  5 Loss:  0.36628957708438714 Error:  0.19365328202347556\n",
      "Epoch:  6 Loss:  0.3536843803828348 Error:  0.19148478268863198\n",
      "Epoch:  7 Loss:  0.34289177467009263 Error:  0.18970103044352846\n",
      "Epoch:  8 Loss:  0.3340673610835732 Error:  0.18852050798738787\n",
      "Epoch:  9 Loss:  0.32568974409274715 Error:  0.18675150084281397\n",
      "Epoch:  10 Loss:  0.31787273912372704 Error:  0.1846015876281761\n",
      "Epoch:  11 Loss:  0.3112945330000209 Error:  0.183010894113672\n",
      "Epoch:  12 Loss:  0.305807844250502 Error:  0.18195410688480218\n",
      "Epoch:  13 Loss:  0.30113224230126706 Error:  0.18125191107838454\n",
      "Epoch:  14 Loss:  0.2970472222673679 Error:  0.18076733290078398\n",
      "Epoch:  15 Loss:  0.2934673557024516 Error:  0.18043872598045602\n",
      "Epoch:  16 Loss:  0.28968554360423976 Error:  0.17962911823195613\n",
      "Epoch:  17 Loss:  0.2863673071661395 Error:  0.17905647633318417\n",
      "Epoch:  18 Loss:  0.2835287614139968 Error:  0.17874978661180257\n",
      "Epoch:  19 Loss:  0.2810028646877426 Error:  0.17857181097932917\n",
      "Epoch:  20 Loss:  0.2786646199797442 Error:  0.1784074612541827\n",
      "Epoch:  21 Loss:  0.27648385324164065 Error:  0.1782726402768118\n",
      "Epoch:  22 Loss:  0.27445274823440047 Error:  0.17815474800007072\n",
      "Epoch:  23 Loss:  0.27254137655575117 Error:  0.1780550206849675\n",
      "Epoch:  24 Loss:  0.27072983480499174 Error:  0.1779385911311932\n",
      "Epoch:  25 Loss:  0.2689900895257196 Error:  0.17783120390540824\n",
      "Epoch:  26 Loss:  0.2673271722243932 Error:  0.17773651639501492\n",
      "Epoch:  27 Loss:  0.2657388362163555 Error:  0.17765186878735434\n",
      "Epoch:  28 Loss:  0.2641886232677334 Error:  0.1775567025660041\n",
      "Epoch:  29 Loss:  0.26268169837083644 Error:  0.17746576006540996\n",
      "Epoch:  30 Loss:  0.2612169378710364 Error:  0.17738606000017976\n",
      "Epoch:  31 Loss:  0.2598085388273536 Error:  0.17731463534389427\n",
      "Epoch:  32 Loss:  0.2584158763914051 Error:  0.1772500648648439\n",
      "Epoch:  33 Loss:  0.25706522354108846 Error:  0.1771910917794633\n",
      "Epoch:  34 Loss:  0.2557427350453988 Error:  0.17713108159110932\n",
      "Epoch:  35 Loss:  0.25442940309019146 Error:  0.17707513067536726\n",
      "Epoch:  36 Loss:  0.2531338616937934 Error:  0.17702159089242628\n",
      "Epoch:  37 Loss:  0.25188076826269756 Error:  0.17697488806561795\n",
      "Epoch:  38 Loss:  0.250651056359628 Error:  0.17693545272250374\n",
      "Epoch:  39 Loss:  0.24946413916385102 Error:  0.17690137947747808\n",
      "Epoch:  40 Loss:  0.24827394433721098 Error:  0.1768694555688047\n",
      "Epoch:  41 Loss:  0.24712908517814683 Error:  0.17684328252683856\n",
      "Epoch:  42 Loss:  0.2459925123139056 Error:  0.17681606319136248\n",
      "Epoch:  43 Loss:  0.24488659051364053 Error:  0.17679213433565494\n",
      "Epoch:  44 Loss:  0.24380354975868843 Error:  0.17677000075757146\n",
      "Epoch:  45 Loss:  0.2427450153463615 Error:  0.17675152849294468\n",
      "Epoch:  46 Loss:  0.24170318656338902 Error:  0.17673575423077909\n",
      "Epoch:  47 Loss:  0.2406806546057056 Error:  0.17671961011643894\n",
      "Epoch:  48 Loss:  0.23968967777526307 Error:  0.17670465809499433\n",
      "Epoch:  49 Loss:  0.23873577253547257 Error:  0.1766929865597251\n",
      "Epoch:  50 Loss:  0.2377770000589108 Error:  0.1766906041585043\n",
      "Epoch:  51 Loss:  0.23686743612417918 Error:  0.17669056052576282\n",
      "Epoch:  52 Loss:  0.23597211502269358 Error:  0.1766698049035615\n",
      "Epoch:  53 Loss:  0.23514521255821524 Error:  0.17666664880192923\n",
      "Epoch:  54 Loss:  0.23433315450559833 Error:  0.1766486028711239\n",
      "Epoch:  55 Loss:  0.23353108772617615 Error:  0.17665112553956266\n",
      "Epoch:  56 Loss:  0.2327745568431066 Error:  0.17663604734900468\n",
      "Epoch:  57 Loss:  0.23207929632263982 Error:  0.1766335613534836\n",
      "Epoch:  58 Loss:  0.23138671970652963 Error:  0.17662313919581338\n",
      "Epoch:  59 Loss:  0.23077630836092783 Error:  0.17660686150639357\n",
      "Epoch:  60 Loss:  0.23015766006386923 Error:  0.1766045358545052\n",
      "Epoch:  61 Loss:  0.2295652634190942 Error:  0.1765976168081432\n",
      "Epoch:  62 Loss:  0.22907829784347625 Error:  0.1765873538341351\n",
      "Epoch:  63 Loss:  0.22859912271985036 Error:  0.17657733588161584\n",
      "Epoch:  64 Loss:  0.22819482307591124 Error:  0.17657589234277873\n",
      "Epoch:  65 Loss:  0.22771202804085738 Error:  0.17656718649550113\n",
      "Epoch:  66 Loss:  0.227374915055886 Error:  0.17655865437613277\n",
      "Epoch:  67 Loss:  0.22704551587561647 Error:  0.17654788404881597\n",
      "Epoch:  68 Loss:  0.22677182526645545 Error:  0.1765437900663136\n",
      "Epoch:  69 Loss:  0.22646557865385525 Error:  0.17653361837307135\n",
      "Epoch:  70 Loss:  0.22627055832368884 Error:  0.17652964110146024\n",
      "Epoch:  71 Loss:  0.22600567215930917 Error:  0.17651817381025076\n",
      "Epoch:  72 Loss:  0.2258181014282261 Error:  0.17651344193312937\n",
      "Epoch:  73 Loss:  0.2254921150600125 Error:  0.1765053682698461\n",
      "Epoch:  74 Loss:  0.22534474088046366 Error:  0.1764964848578333\n",
      "Epoch:  75 Loss:  0.22524753266465877 Error:  0.17649009190276713\n",
      "Epoch:  76 Loss:  0.22507484551675305 Error:  0.17648233749909314\n",
      "Epoch:  77 Loss:  0.22486530708338687 Error:  0.17647843839165694\n",
      "Epoch:  78 Loss:  0.22472956990767382 Error:  0.17646820450614312\n",
      "Epoch:  79 Loss:  0.22456850191790187 Error:  0.17646351135419514\n",
      "Epoch:  80 Loss:  0.2245163292941933 Error:  0.1764478652241701\n",
      "Epoch:  81 Loss:  0.2243327752737228 Error:  0.17644364302029866\n",
      "Epoch:  82 Loss:  0.22418837454504595 Error:  0.17644338943287283\n",
      "Epoch:  83 Loss:  0.22410251423270403 Error:  0.17643173833093242\n",
      "Epoch:  84 Loss:  0.22391480796351404 Error:  0.1764248973595168\n",
      "Epoch:  85 Loss:  0.22389476875702063 Error:  0.17641610317601414\n",
      "Epoch:  86 Loss:  0.2237649064756439 Error:  0.1764155044527111\n",
      "Epoch:  87 Loss:  0.2236116322393189 Error:  0.17640558598998063\n",
      "Epoch:  88 Loss:  0.22354183930479837 Error:  0.17640287023104592\n",
      "Epoch:  89 Loss:  0.223517700762092 Error:  0.1763981501320879\n",
      "Epoch:  90 Loss:  0.22325062858844233 Error:  0.17638921362911156\n",
      "Epoch:  91 Loss:  0.22312740160676534 Error:  0.17638849596420447\n",
      "Epoch:  92 Loss:  0.22302262850864205 Error:  0.1763911220484865\n",
      "Epoch:  93 Loss:  0.22298596638762308 Error:  0.1763890730942081\n",
      "Epoch:  94 Loss:  0.22299193274118229 Error:  0.17637585701342828\n",
      "Epoch:  95 Loss:  0.22267743240216534 Error:  0.17637870072604653\n",
      "Epoch:  96 Loss:  0.2227379095233129 Error:  0.17636834727432912\n",
      "Epoch:  97 Loss:  0.22275953369583198 Error:  0.17637091946459102\n",
      "Epoch:  98 Loss:  0.22262137753520897 Error:  0.17635138091926802\n",
      "Epoch:  99 Loss:  0.22251440415125406 Error:  0.17636030145034104\n",
      "Epoch:  100 Loss:  0.22253724745290723 Error:  0.17635064032263384\n",
      "Epoch:  101 Loss:  0.22235728657531167 Error:  0.17634600927372893\n",
      "Epoch:  102 Loss:  0.22227463652630766 Error:  0.176335550086227\n",
      "Epoch:  103 Loss:  0.22212613708601742 Error:  0.1763291226889559\n",
      "Epoch:  104 Loss:  0.2220298531883491 Error:  0.1763210583054377\n",
      "Epoch:  105 Loss:  0.22195774655856057 Error:  0.17631299990022967\n",
      "Epoch:  106 Loss:  0.22191072429368597 Error:  0.1763090739350119\n",
      "Epoch:  107 Loss:  0.2218569375262289 Error:  0.1763038342584393\n",
      "Epoch:  108 Loss:  0.22186782795512033 Error:  0.1762977332590583\n",
      "Epoch:  109 Loss:  0.22176679638688435 Error:  0.17628466494069128\n",
      "Epoch:  110 Loss:  0.22161553701954687 Error:  0.17627001995455005\n",
      "Epoch:  111 Loss:  0.22155244862604997 Error:  0.1762640876862817\n",
      "Epoch:  112 Loss:  0.22149334955001307 Error:  0.17626534223913431\n",
      "Epoch:  113 Loss:  0.2214366982796949 Error:  0.17626051631516326\n",
      "Epoch:  114 Loss:  0.22129268105515462 Error:  0.1762527294323116\n",
      "Epoch:  115 Loss:  0.22136496997878938 Error:  0.176244652735259\n",
      "Epoch:  116 Loss:  0.22126551195532976 Error:  0.17623502899429755\n",
      "Epoch:  117 Loss:  0.2211704098892783 Error:  0.17622034385532676\n",
      "Epoch:  118 Loss:  0.2210890080579027 Error:  0.17622245348499208\n",
      "Epoch:  119 Loss:  0.2209898469869248 Error:  0.1762088541677612\n",
      "Epoch:  120 Loss:  0.2210043499212779 Error:  0.17620733380317688\n",
      "Epoch:  121 Loss:  0.22091143654135173 Error:  0.17619923453131123\n",
      "Epoch:  122 Loss:  0.220894051169207 Error:  0.17619192243336204\n",
      "Epoch:  123 Loss:  0.22084018106232145 Error:  0.17618411217264074\n",
      "Epoch:  124 Loss:  0.2206934075691029 Error:  0.17617424885312954\n",
      "Epoch:  125 Loss:  0.2207060553535016 Error:  0.17617208836321346\n",
      "Epoch:  126 Loss:  0.2206470356016102 Error:  0.17617597783397057\n",
      "Epoch:  127 Loss:  0.22055336886537288 Error:  0.1761562080083493\n",
      "Epoch:  128 Loss:  0.2205593350404751 Error:  0.17614045587485422\n",
      "Epoch:  129 Loss:  0.22040092052813776 Error:  0.17612751595631332\n",
      "Epoch:  130 Loss:  0.22034273229673237 Error:  0.17612238406778097\n",
      "Epoch:  131 Loss:  0.22029250538991596 Error:  0.17612387587924203\n",
      "Epoch:  132 Loss:  0.22025422002384049 Error:  0.17610379901831735\n",
      "Epoch:  133 Loss:  0.22021384510451447 Error:  0.1761008233367326\n",
      "Epoch:  134 Loss:  0.22016177919810404 Error:  0.17611017359231046\n",
      "Epoch:  135 Loss:  0.22023221689783884 Error:  0.176114220729845\n",
      "Epoch:  136 Loss:  0.22014818675146847 Error:  0.1760860573567316\n",
      "Epoch:  137 Loss:  0.22008356327068307 Error:  0.17607952993430062\n",
      "Epoch:  138 Loss:  0.21993857782757925 Error:  0.17607518718271198\n",
      "Epoch:  139 Loss:  0.22002551637723775 Error:  0.176063491377288\n",
      "Epoch:  140 Loss:  0.21997660318177617 Error:  0.17604357155854117\n",
      "Epoch:  141 Loss:  0.21997352934883027 Error:  0.1760347719320994\n",
      "Epoch:  142 Loss:  0.21978665468935482 Error:  0.17599738590017763\n",
      "Epoch:  143 Loss:  0.21971277517829826 Error:  0.17596455989126675\n",
      "Epoch:  144 Loss:  0.21958473806609652 Error:  0.17590154832351707\n",
      "Epoch:  145 Loss:  0.21951800424181772 Error:  0.17590109709494128\n",
      "Epoch:  146 Loss:  0.21948217845962434 Error:  0.17588234893576113\n",
      "Epoch:  147 Loss:  0.21945340574501518 Error:  0.17587542944325657\n",
      "Epoch:  148 Loss:  0.21945670282769347 Error:  0.1758827304768705\n",
      "Epoch:  149 Loss:  0.2193422073018765 Error:  0.1758649678465849\n",
      "Epoch:  150 Loss:  0.2193140315260002 Error:  0.17585192147843137\n",
      "Epoch:  151 Loss:  0.21932716346429493 Error:  0.17584491704038518\n",
      "Epoch:  152 Loss:  0.219181292785142 Error:  0.17585368445533478\n",
      "Epoch:  153 Loss:  0.21922731399536133 Error:  0.1758248815814892\n",
      "Epoch:  154 Loss:  0.21915190260924264 Error:  0.17581619732751103\n",
      "Epoch:  155 Loss:  0.21917423365001906 Error:  0.17581436207551443\n",
      "Epoch:  156 Loss:  0.2190838784514787 Error:  0.17579148986382398\n",
      "Epoch:  157 Loss:  0.21906537638452953 Error:  0.17578906338371916\n",
      "Epoch:  158 Loss:  0.21902664005756378 Error:  0.17577319480701833\n",
      "Epoch:  159 Loss:  0.21903267654473196 Error:  0.17576603992970405\n",
      "Epoch:  160 Loss:  0.21901640171062448 Error:  0.17578436853642948\n",
      "Epoch:  161 Loss:  0.2189536283829969 Error:  0.1757706661602694\n",
      "Epoch:  162 Loss:  0.2189312062399116 Error:  0.17575637202063008\n",
      "Epoch:  163 Loss:  0.21888985275151487 Error:  0.17574226347629182\n",
      "Epoch:  164 Loss:  0.21883366127570947 Error:  0.17575323421084238\n",
      "Epoch:  165 Loss:  0.21877423428489776 Error:  0.17574140429496765\n",
      "Epoch:  166 Loss:  0.21879501203576962 Error:  0.17575260425756078\n",
      "Epoch:  167 Loss:  0.21881871139574907 Error:  0.17572484464345578\n",
      "Epoch:  168 Loss:  0.2186789026338897 Error:  0.17573291571911223\n",
      "Epoch:  169 Loss:  0.21863245553599148 Error:  0.17570747515398585\n",
      "Epoch:  170 Loss:  0.21861536176261787 Error:  0.17569890323870196\n",
      "Epoch:  171 Loss:  0.2185605597531724 Error:  0.1757035501702817\n",
      "Epoch:  172 Loss:  0.21865625385038867 Error:  0.17569814488559427\n",
      "Epoch:  173 Loss:  0.21859013579205838 Error:  0.17567530033474196\n",
      "Epoch:  174 Loss:  0.21858040551225583 Error:  0.17567654203868913\n",
      "Epoch:  175 Loss:  0.21842180344159018 Error:  0.1756560347394315\n",
      "Epoch:  176 Loss:  0.2184458395499669 Error:  0.1756504960581214\n",
      "Epoch:  177 Loss:  0.21835108065676545 Error:  0.17566532788876288\n",
      "Epoch:  178 Loss:  0.21837578499745466 Error:  0.17567294791430058\n",
      "Epoch:  179 Loss:  0.21839983573930707 Error:  0.17564046837969455\n",
      "Epoch:  180 Loss:  0.21836016715286735 Error:  0.1756160977536333\n",
      "Epoch:  181 Loss:  0.21821151200882688 Error:  0.17561297600497744\n",
      "Epoch:  182 Loss:  0.2182616747603445 Error:  0.17559955505553834\n",
      "Epoch:  183 Loss:  0.21820124252113754 Error:  0.17558324738533912\n",
      "Epoch:  184 Loss:  0.21821226813122183 Error:  0.17556055059689962\n",
      "Epoch:  185 Loss:  0.21812051579266964 Error:  0.1755660329750198\n",
      "Epoch:  186 Loss:  0.21795112938581113 Error:  0.17539110467462482\n",
      "Epoch:  187 Loss:  0.21795247459483003 Error:  0.1753352911707884\n",
      "Epoch:  188 Loss:  0.2178582558018005 Error:  0.17527416062926104\n",
      "Epoch:  189 Loss:  0.21772186779333447 Error:  0.17524685081607566\n",
      "Epoch:  190 Loss:  0.2177236645878432 Error:  0.17524897971909917\n",
      "Epoch:  191 Loss:  0.21769898906796278 Error:  0.1752109310762611\n",
      "Epoch:  192 Loss:  0.21774892268066634 Error:  0.17520167959664396\n",
      "Epoch:  193 Loss:  0.21769274530296553 Error:  0.17517363391593546\n",
      "Epoch:  194 Loss:  0.21758522432364388 Error:  0.17518685990107988\n",
      "Epoch:  195 Loss:  0.21763162466580283 Error:  0.17517455671718735\n",
      "Epoch:  196 Loss:  0.21756021776599085 Error:  0.1751976560154361\n",
      "Epoch:  197 Loss:  0.21749624047807592 Error:  0.175148975706386\n",
      "Epoch:  198 Loss:  0.21750495346363433 Error:  0.17518912889286428\n",
      "Epoch:  199 Loss:  0.2174993034251436 Error:  0.17515484997612274\n",
      "Epoch:  200 Loss:  0.21742062698938175 Error:  0.17514559028748267\n"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "train(final_model, optimizer, lr_train_data_vectorized, hr_train_data_vectorized, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 12720)\n",
      "(112, 160, 160)\n"
     ]
    }
   ],
   "source": [
    "#Generate submission \n",
    "\n",
    "# load csvs as numpy\n",
    "test_lr_data_path = '../data/lr_test.csv'\n",
    "\n",
    "# lr_test_data = np.loadtxt(test_lr_data_path, delimiter=',')\n",
    "lr_test_data = pd.read_csv(test_lr_data_path, delimiter=',').to_numpy()\n",
    "print(lr_test_data.shape)\n",
    "lr_test_data[lr_test_data < 0] = 0\n",
    "np.nan_to_num(lr_test_data, copy=False)\n",
    "\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "\n",
    "lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "print(lr_test_data_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 (35778,)\n",
      "(4007136,)\n"
     ]
    }
   ],
   "source": [
    "final_model.eval()\n",
    "preds = []\n",
    "for lr in lr_test_data_vectorized:      \n",
    "  lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "  \n",
    "  model_outputs, _, _, _ = final_model(lr)\n",
    "  model_outputs  = unpad(model_outputs, args.padding)\n",
    "  preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "print(len(preds), preds[0].shape)\n",
    "r = np.hstack(preds)\n",
    "print(r.shape)\n",
    "meltedDF = r.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = meltedDF.shape[0]\n",
    "df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                   'Predicted': meltedDF})\n",
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
