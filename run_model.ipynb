{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "from typing import Union\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from MatrixVectorizer import *\n",
    "from dataloaders import NoisyDataset\n",
    "from model import *\n",
    "from preprocessing import *\n",
    "from test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility across multiple libraries\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8928/3687639064.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n"
     ]
    }
   ],
   "source": [
    "# load csvs as numpy\n",
    "lr_data_path = './data/lr_train.csv'\n",
    "hr_data_path = './data/hr_train.csv'\n",
    "\n",
    "lr_train_data = pd.read_csv(lr_data_path, delimiter=',').to_numpy()\n",
    "hr_train_data = pd.read_csv(hr_data_path, delimiter=',').to_numpy()\n",
    "lr_train_data[lr_train_data < 0] = 0\n",
    "np.nan_to_num(lr_train_data, copy=False)\n",
    "\n",
    "hr_train_data[hr_train_data < 0] = 0\n",
    "np.nan_to_num(hr_train_data, copy=False)\n",
    "\n",
    "# map the anti-vectorize function to each row of the lr_train_data\n",
    "lr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_train_data],\n",
    "                                        dtype=torch.float32)\n",
    "hr_train_data_vectorized = torch.tensor([MatrixVectorizer.anti_vectorize(row, 268) for row in hr_train_data],\n",
    "                                        dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = NoisyDataset(lr_train_data_vectorized, hr_train_data_vectorized, noise_level=0.5)\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_splt = 3\n",
    "epochs = 150\n",
    "lr = 0.00005 # try [0.0001, 0.0005, 0.00001, 0.00005]\n",
    "lmbda = 17 # should be around 15-20\n",
    "lamdba_topo = 0.0005 # should be around 0.0001-0.001\n",
    "lr_dim = 160\n",
    "hr_dim = 320\n",
    "hidden_dim = 320 # try smaller and larger - [160-512]\n",
    "padding = 26\n",
    "dropout = 0.2 # try [0., 0.1, 0.2, 0.3]\n",
    "\n",
    "\n",
    "args = argparse.Namespace()\n",
    "args.epochs = epochs\n",
    "args.lr = lr\n",
    "args.lmbda = lmbda\n",
    "args.lamdba_topo = lamdba_topo\n",
    "args.lr_dim = lr_dim\n",
    "args.hr_dim = hr_dim\n",
    "args.hidden_dim = hidden_dim\n",
    "args.padding = padding\n",
    "args.p = dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [0.9, 0.7, 0.6, 0.5]\n",
    "model = GSRNet(ks, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_csv(model, data_path='./data/lr_test.csv', filename='submission.csv'):\n",
    "    lr_test_data = pd.read_csv(data_path, delimiter=',').to_numpy()\n",
    "    lr_test_data[lr_test_data < 0] = 0\n",
    "    np.nan_to_num(lr_test_data, copy=False)\n",
    "    lr_test_data_vectorized = np.array([MatrixVectorizer.anti_vectorize(row, 160) for row in lr_test_data])\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for lr in lr_test_data_vectorized:      \n",
    "        lr = torch.from_numpy(lr).type(torch.FloatTensor)\n",
    "        \n",
    "        model_outputs, _, _, _ = model(lr)\n",
    "        model_outputs  = unpad(model_outputs, args.padding)\n",
    "        preds.append(MatrixVectorizer.vectorize(model_outputs.detach().numpy()))\n",
    "\n",
    "    r = np.hstack(preds)\n",
    "    meltedDF = r.flatten()\n",
    "    n = meltedDF.shape[0]\n",
    "    df = pd.DataFrame({'ID': np.arange(1, n+1),\n",
    "                    'Predicted': meltedDF})\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_data_loader, optimizer, args): \n",
    "  \n",
    "  all_epochs_loss = []\n",
    "  all_epochs_error = []\n",
    "  all_epochs_topoloss = []\n",
    "  no_epochs = args.epochs\n",
    "\n",
    "  for epoch in range(no_epochs):\n",
    "    epoch_loss = []\n",
    "    epoch_error = []\n",
    "    epoch_topo = []\n",
    "\n",
    "    model.train()\n",
    "    for lr, hr in train_data_loader: \n",
    "      lr.to(device)   \n",
    "      hr.to(device)  \n",
    "      lr = lr.reshape(160, 160)\n",
    "      hr = hr.reshape(268, 268)\n",
    "\n",
    "      model_outputs,net_outs,start_gcn_outs,layer_outs = model(lr)\n",
    "      model_outputs  = unpad(model_outputs, args.padding)\n",
    "\n",
    "      padded_hr = pad_HR_adj(hr,args.padding)\n",
    "      _, U_hr = torch.linalg.eigh(padded_hr, UPLO='U')\n",
    "\n",
    "      loss = args.lmbda * criterion(net_outs, start_gcn_outs) + criterion(model.layer.weights,U_hr) + criterion(model_outputs, hr) \n",
    "      topo = compute_topological_MAE_loss(hr, model_outputs)\n",
    "      \n",
    "      loss += args.lamdba_topo * topo\n",
    "\n",
    "      error = criterion(model_outputs, hr)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      epoch_loss.append(loss.item())\n",
    "      epoch_error.append(error.item())\n",
    "      epoch_topo.append(topo.item())\n",
    "      \n",
    "  \n",
    "    model.eval()\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "      filename = f'{epoch+1}epoch-model.sav'\n",
    "      pickle.dump(model, open(filename, 'wb'))\n",
    "      # generate_submission_csv(model, filename=f'{epoch+1}epoch-model.csv')\n",
    "    print(\"Epoch: \",epoch+1, \"Loss: \", np.mean(epoch_loss), \"Error: \", np.mean(epoch_error),\n",
    "          \"Topo: \", np.mean(epoch_topo))\n",
    "    all_epochs_loss.append(np.mean(epoch_loss))\n",
    "    all_epochs_error.append(np.mean(epoch_error))\n",
    "    all_epochs_topoloss.append(np.mean(epoch_topo))\n",
    "  df = pd.DataFrame({'Epoch': np.arange(1, no_epochs+1),\n",
    "                  'Total Loss': all_epochs_loss,\n",
    "                  'Error': all_epochs_error,\n",
    "                  'Topological loss': all_epochs_topoloss,\n",
    "                  })\n",
    "  df.to_csv('model-losses.csv', index=False)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model & Kaggle Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  0.4603491131060138 Error:  0.20874980636342558 Topo:  36.38929357357368\n",
      "Epoch:  2 Loss:  0.31078401975289077 Error:  0.18189827383992202 Topo:  20.48308480999427\n",
      "Epoch:  3 Loss:  0.30281120181797505 Error:  0.178619987325754 Topo:  19.349652804300458\n",
      "Epoch:  4 Loss:  0.29687833732473634 Error:  0.17685522817209096 Topo:  19.053390000394717\n",
      "Epoch:  5 Loss:  0.29146927898515484 Error:  0.17582334578037262 Topo:  18.929457476039133\n",
      "Epoch:  6 Loss:  0.2861098129592256 Error:  0.17474247675812887 Topo:  18.69257019522661\n",
      "Epoch:  7 Loss:  0.2810191350604246 Error:  0.17396613295206767 Topo:  18.826101759950557\n",
      "Epoch:  8 Loss:  0.27654760424605385 Error:  0.1733112237053717 Topo:  18.731812168738085\n",
      "Epoch:  9 Loss:  0.2726511842833308 Error:  0.17260693754264694 Topo:  18.763306115201846\n",
      "Epoch:  10 Loss:  0.2684864010104162 Error:  0.1717383844588331 Topo:  18.833300008031422\n",
      "Epoch:  11 Loss:  0.2648830295144441 Error:  0.17108892502185113 Topo:  18.707007956362055\n",
      "Epoch:  12 Loss:  0.26106687835947484 Error:  0.16989230538556677 Topo:  18.777526935417495\n",
      "Epoch:  13 Loss:  0.2578565539892562 Error:  0.16887823478904312 Topo:  18.635057455051445\n",
      "Epoch:  14 Loss:  0.2548188279845757 Error:  0.16799178050306743 Topo:  18.825919242676147\n",
      "Epoch:  15 Loss:  0.2527227675664925 Error:  0.16737646926305966 Topo:  18.764812869226148\n",
      "Epoch:  16 Loss:  0.24988361568508033 Error:  0.1661097261006247 Topo:  18.849437673648673\n",
      "Epoch:  17 Loss:  0.2475361679485458 Error:  0.1651727161007727 Topo:  18.697282716899576\n",
      "Epoch:  18 Loss:  0.24547922861076402 Error:  0.16408072162174178 Topo:  18.759149128805376\n",
      "Epoch:  19 Loss:  0.24334329857440765 Error:  0.16306898836604136 Topo:  18.51910416380374\n",
      "Epoch:  20 Loss:  0.24209234698446924 Error:  0.1623785005893536 Topo:  18.646397173761606\n",
      "Epoch:  21 Loss:  0.2406187576805046 Error:  0.16168651582595117 Topo:  18.71412843715645\n",
      "Epoch:  22 Loss:  0.23910570287418936 Error:  0.16078858773508473 Topo:  18.583156888356466\n",
      "Epoch:  23 Loss:  0.2379978279510658 Error:  0.16028993090469681 Topo:  18.653754011599602\n",
      "Epoch:  24 Loss:  0.23708369346435912 Error:  0.15976909597119884 Topo:  18.669702438537232\n",
      "Epoch:  25 Loss:  0.23555704228535385 Error:  0.15875134657243054 Topo:  18.48424339865496\n",
      "Epoch:  26 Loss:  0.23486819818705143 Error:  0.15842071014963938 Topo:  18.596361411545804\n",
      "Epoch:  27 Loss:  0.2339850559741437 Error:  0.15779017679348678 Topo:  18.556036949157715\n",
      "Epoch:  28 Loss:  0.23332513868808746 Error:  0.15742793697083068 Topo:  18.600647874934946\n",
      "Epoch:  29 Loss:  0.2324651502920482 Error:  0.15693894929871588 Topo:  18.587647335258072\n",
      "Epoch:  30 Loss:  0.23149509710109162 Error:  0.15629192348011953 Topo:  18.52561653731112\n",
      "Epoch:  31 Loss:  0.23060942106618137 Error:  0.15581652272247268 Topo:  18.469594778414972\n",
      "Epoch:  32 Loss:  0.2300599154062614 Error:  0.15544361688062816 Topo:  18.483393195146572\n",
      "Epoch:  33 Loss:  0.22951236545682668 Error:  0.1552310832246335 Topo:  18.43617649992069\n",
      "Epoch:  34 Loss:  0.22879825835813306 Error:  0.1547351079786609 Topo:  18.46038484287833\n",
      "Epoch:  35 Loss:  0.22830903913803444 Error:  0.15450605598395456 Topo:  18.548493870718037\n",
      "Epoch:  36 Loss:  0.22762610642852898 Error:  0.15418109529746507 Topo:  18.461722059878046\n",
      "Epoch:  37 Loss:  0.22738626492237615 Error:  0.15403728071087136 Topo:  18.633129182689917\n",
      "Epoch:  38 Loss:  0.2264486164033056 Error:  0.15349497320409305 Topo:  18.487092012416817\n",
      "Epoch:  39 Loss:  0.22612582611109683 Error:  0.1533029487211547 Topo:  18.45734651074438\n",
      "Epoch:  40 Loss:  0.22549169345530207 Error:  0.15292045018987027 Topo:  18.375521637008575\n",
      "Epoch:  41 Loss:  0.22515702283311034 Error:  0.15288399445439527 Topo:  18.477008374151357\n",
      "Epoch:  42 Loss:  0.22443916322942264 Error:  0.15254280684950822 Topo:  18.404141860093898\n",
      "Epoch:  43 Loss:  0.22446721533458389 Error:  0.152739893622741 Topo:  18.644206166981224\n",
      "Epoch:  44 Loss:  0.22342483967007276 Error:  0.15211843079078696 Topo:  18.410105613891236\n",
      "Epoch:  45 Loss:  0.22305316179098483 Error:  0.15187987881506274 Topo:  18.34752140502016\n",
      "Epoch:  46 Loss:  0.22292254659943952 Error:  0.15197751117859057 Topo:  18.4558304484019\n",
      "Epoch:  47 Loss:  0.22230264258955768 Error:  0.1516222665588299 Topo:  18.398809547195892\n",
      "Epoch:  48 Loss:  0.22197149667197358 Error:  0.151534732899623 Topo:  18.428691412874326\n",
      "Epoch:  49 Loss:  0.22173766782897675 Error:  0.1514576995622612 Topo:  18.37929747918409\n",
      "Epoch:  50 Loss:  0.22129031453660863 Error:  0.15129579791051898 Topo:  18.378492566639792\n",
      "Epoch:  51 Loss:  0.2213285541284584 Error:  0.15140222914204626 Topo:  18.48600228817877\n",
      "Epoch:  52 Loss:  0.22058805395029263 Error:  0.1510835839156619 Topo:  18.413505571331093\n",
      "Epoch:  53 Loss:  0.22008900585288774 Error:  0.15098766737176986 Topo:  18.328373680571595\n",
      "Epoch:  54 Loss:  0.21996701780907407 Error:  0.15093589190415993 Topo:  18.432379071584005\n",
      "Epoch:  55 Loss:  0.2196429729640127 Error:  0.15082513706055944 Topo:  18.424308382822367\n",
      "Epoch:  56 Loss:  0.21962256854522727 Error:  0.15088821728964766 Topo:  18.517377179539846\n",
      "Epoch:  57 Loss:  0.21918506188663894 Error:  0.15076496777777187 Topo:  18.443779197281707\n",
      "Epoch:  58 Loss:  0.21862683262296778 Error:  0.15054776782761076 Topo:  18.408926781066164\n",
      "Epoch:  59 Loss:  0.21842352046581084 Error:  0.15047102747206204 Topo:  18.420190080197273\n",
      "Epoch:  60 Loss:  0.21817148151155003 Error:  0.15048925529518528 Topo:  18.454383958599525\n",
      "Epoch:  61 Loss:  0.21770918396061767 Error:  0.15037352542677326 Topo:  18.438534216966456\n",
      "Epoch:  62 Loss:  0.2174969063012186 Error:  0.15026847340032726 Topo:  18.417529248905755\n",
      "Epoch:  63 Loss:  0.21715746953815757 Error:  0.1501266852824274 Topo:  18.375647082300244\n",
      "Epoch:  64 Loss:  0.2167121629693551 Error:  0.14994965359836282 Topo:  18.320741807629247\n",
      "Epoch:  65 Loss:  0.21667047005570578 Error:  0.15013999393778646 Topo:  18.424551461271182\n",
      "Epoch:  66 Loss:  0.21615803687872287 Error:  0.14990355895307964 Topo:  18.340402100614444\n",
      "Epoch:  67 Loss:  0.21590565145015717 Error:  0.14987539932756366 Topo:  18.354727767898652\n",
      "Epoch:  68 Loss:  0.2155051798877602 Error:  0.14960306325180087 Topo:  18.286899212591663\n",
      "Epoch:  69 Loss:  0.21571195714488 Error:  0.149863130050505 Topo:  18.39220470154357\n",
      "Epoch:  70 Loss:  0.21535029748599685 Error:  0.14971320612166455 Topo:  18.30917078006767\n",
      "Epoch:  71 Loss:  0.21509254888860052 Error:  0.14963772955411922 Topo:  18.353970493385177\n",
      "Epoch:  72 Loss:  0.21496613395071315 Error:  0.14962756258998802 Topo:  18.40124777262796\n",
      "Epoch:  73 Loss:  0.2147461535509475 Error:  0.1497156068593442 Topo:  18.412995835264287\n",
      "Epoch:  74 Loss:  0.2140575849010559 Error:  0.14928456806315632 Topo:  18.259581554435684\n",
      "Epoch:  75 Loss:  0.21416450091107878 Error:  0.14950540635043275 Topo:  18.345712758823783\n",
      "Epoch:  76 Loss:  0.2139119449668302 Error:  0.14948144158024987 Topo:  18.36558465329473\n",
      "Epoch:  77 Loss:  0.2138364008086884 Error:  0.14940593837799426 Topo:  18.313442235935234\n",
      "Epoch:  78 Loss:  0.21356955027865793 Error:  0.14945155645380476 Topo:  18.418898713802864\n",
      "Epoch:  79 Loss:  0.2135494376371007 Error:  0.1494720423828342 Topo:  18.343041711224767\n",
      "Epoch:  80 Loss:  0.2130567676828293 Error:  0.14929851260549293 Topo:  18.38281873600212\n",
      "Epoch:  81 Loss:  0.2129871541868427 Error:  0.14927139048447866 Topo:  18.309702176533772\n",
      "Epoch:  82 Loss:  0.2127008396172952 Error:  0.1492200496043274 Topo:  18.325070655274533\n",
      "Epoch:  83 Loss:  0.21246860858922947 Error:  0.149172039847531 Topo:  18.319386425132524\n",
      "Epoch:  84 Loss:  0.21266092957850702 Error:  0.14936880263204347 Topo:  18.422684949315236\n",
      "Epoch:  85 Loss:  0.21218708104002262 Error:  0.14915725053427462 Topo:  18.386508804595397\n",
      "Epoch:  86 Loss:  0.21175999948364532 Error:  0.14888279309529745 Topo:  18.26957358857115\n",
      "Epoch:  87 Loss:  0.2120448218848177 Error:  0.14923710045878757 Topo:  18.443057020267325\n",
      "Epoch:  88 Loss:  0.21177622782969904 Error:  0.14916436195730448 Topo:  18.420220854753506\n",
      "Epoch:  89 Loss:  0.2114223934219269 Error:  0.14902128221210606 Topo:  18.383270143748756\n",
      "Epoch:  90 Loss:  0.21121000949137225 Error:  0.1489323449884346 Topo:  18.298047853801066\n",
      "Epoch:  91 Loss:  0.21120190727496577 Error:  0.1490343571333828 Topo:  18.387283576462796\n",
      "Epoch:  92 Loss:  0.21118980806744742 Error:  0.1490638673216283 Topo:  18.356861731249417\n",
      "Epoch:  93 Loss:  0.21086076597967549 Error:  0.1488524278748535 Topo:  18.30737305829625\n",
      "Epoch:  94 Loss:  0.21047495260923924 Error:  0.148879386841537 Topo:  18.269349195286186\n",
      "Epoch:  95 Loss:  0.21035662909110864 Error:  0.14881117035171942 Topo:  18.245960955134407\n",
      "Epoch:  96 Loss:  0.21006781023419546 Error:  0.14857426432970755 Topo:  18.304199275856245\n",
      "Epoch:  97 Loss:  0.209817837366087 Error:  0.1485875187787467 Topo:  18.172114280883424\n",
      "Epoch:  98 Loss:  0.20987913565721342 Error:  0.1486487157955141 Topo:  18.340018849172992\n",
      "Epoch:  99 Loss:  0.20964884186933141 Error:  0.1485533603651081 Topo:  18.25217603923318\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'generate_submission_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(final_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m      5\u001b[0m final_model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 46\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, optimizer, args)\u001b[0m\n\u001b[1;32m     44\u001b[0m   filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mepoch-model.sav\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     45\u001b[0m   pickle\u001b[38;5;241m.\u001b[39mdump(model, \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m---> 46\u001b[0m   \u001b[43mgenerate_submission_csv\u001b[49m(model, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mepoch-model.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;124m\"\u001b[39m,epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(epoch_loss), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(epoch_error),\n\u001b[1;32m     48\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopo: \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mmean(epoch_topo))\n\u001b[1;32m     49\u001b[0m all_epochs_loss\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(epoch_loss))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_submission_csv' is not defined"
     ]
    }
   ],
   "source": [
    "#final train\n",
    "final_model = GSRNet(ks, args)\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=args.lr)\n",
    "\n",
    "final_model.to(device)\n",
    "\n",
    "train(final_model, train_data_loader, optimizer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'final-model.sav'\n",
    "pickle.dump(final_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_submission_csv(final_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
